{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e3e29b0",
   "metadata": {},
   "source": [
    "# Проверка загрузки MS_ResUNet из файла (.pt)\n",
    "\n",
    "Этот ноутбук делает только одно: **проверяет, что ваша модель корректно создаётся и загружается из файла** (state_dict / checkpoint) и показывает `missing/unexpected` ключи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e172c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "MODEL_PT exists: True -> C:\\Users\\Вячеслав\\Documents\\superresolution\\models\\best_x2_ms_resunet.pt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# --- НАСТРОЙКИ ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# УКАЖИТЕ ПУТЬ К ВАШЕМУ .pt (state_dict или checkpoint)\n",
    "MODEL_PT = Path(r\"C:\\Users\\Вячеслав\\Documents\\superresolution\\models\\best_x2_ms_resunet.pt\")  # <-- поменяйте на ваш путь\n",
    "\n",
    "# Если хотите строгое совпадение ключей, поставьте True.\n",
    "# На этапе диагностики удобнее False, чтобы увидеть missing/unexpected.\n",
    "STRICT = False\n",
    "\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"MODEL_PT exists:\", MODEL_PT.exists(), \"->\", MODEL_PT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62b6de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _strip_prefix(state_dict: dict, prefix: str = \"module.\") -> dict:\n",
    "    \"\"\"Убирает префикс 'module.' (часто появляется после DataParallel/DDP).\"\"\"\n",
    "    if not isinstance(state_dict, dict):\n",
    "        raise TypeError(\"state_dict must be a dict\")\n",
    "    if not any(k.startswith(prefix) for k in state_dict.keys()):\n",
    "        return state_dict\n",
    "    return {k[len(prefix):] if k.startswith(prefix) else k: v for k, v in state_dict.items()}\n",
    "\n",
    "def extract_state_dict(ckpt) -> dict:\n",
    "    \"\"\"Достаёт state_dict из разных форматов сохранения.\"\"\"\n",
    "    # 1) чистый state_dict\n",
    "    if isinstance(ckpt, dict) and all(isinstance(k, str) for k in ckpt.keys()):\n",
    "        # часто checkpoint хранит state_dict под одним из этих ключей\n",
    "        for key in (\"state_dict\", \"model\", \"model_state_dict\", \"net\", \"generator\"):\n",
    "            if key in ckpt and isinstance(ckpt[key], dict):\n",
    "                sd = ckpt[key]\n",
    "                return _strip_prefix(sd)\n",
    "        # если похоже на state_dict (ключи вида 'conv.weight' и т.п.)\n",
    "        # (эвристика: хотя бы один ключ содержит '.weight' или '.bias')\n",
    "        if any((\".weight\" in k) or (\".bias\" in k) for k in ckpt.keys()):\n",
    "            return _strip_prefix(ckpt)\n",
    "\n",
    "    raise ValueError(\n",
    "        \"Не удалось извлечь state_dict. \"\n",
    "        \"Ожидается либо state_dict напрямую, либо checkpoint с ключом вроде \"\n",
    "        \"'model' / 'state_dict' / 'model_state_dict'.\"\n",
    "    )\n",
    "\n",
    "def load_checkpoint(path: Path, map_location=\"cpu\"):\n",
    "    \"\"\"Безопасная загрузка torch checkpoint/state_dict.\"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return torch.load(path, map_location=map_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52282aa3",
   "metadata": {},
   "source": [
    "## Импорт вашей архитектуры\n",
    "\n",
    "Ниже **поменяйте импорт** на ваш файл/модуль, где определена функция `MS_ResUNet()` (или класс).\n",
    "\n",
    "Пример:\n",
    "- если у вас `models/ms_resunet.py` и внутри `def MS_ResUNet(): ...`\n",
    "  ```python\n",
    "  from models.ms_resunet import MS_ResUNet\n",
    "  ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a85a63d-5fbd-4af5-940f-32853889b5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\n"
     ]
    }
   ],
   "source": [
    "# для использования ячейки ниже нужно перейти в папку с модулем, менять, если путь изменился\n",
    "%cd C:\\Users\\Вячеслав\\Documents\\superresolution\\modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f8cd5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция-конструктор\n",
    "from ms_resunet import MS_ResUNet\n",
    "\n",
    "assert MS_ResUNet is not None, \"Поменяйте импорт выше так, чтобы MS_ResUNet был доступен.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ad0082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded keys: 363\n",
      "First keys sample:\n",
      "   conv1.weight\n",
      "   bn1.weight\n",
      "   bn1.bias\n",
      "   bn1.running_mean\n",
      "   bn1.running_var\n",
      "   bn1.num_batches_tracked\n",
      "   upCT4.weight\n",
      "   upCT4.bias\n",
      "   upCT3.weight\n",
      "   upCT3.bias\n"
     ]
    }
   ],
   "source": [
    "# 1) Загружаем checkpoint/state_dict\n",
    "ckpt = load_checkpoint(MODEL_PT, map_location=\"cpu\")\n",
    "state = extract_state_dict(ckpt)\n",
    "\n",
    "print(\"Loaded keys:\", len(state))\n",
    "first_keys = list(state.keys())[:10]\n",
    "print(\"First keys sample:\")\n",
    "for k in first_keys:\n",
    "    print(\"  \", k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43dfff32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== load_state_dict report ===\n",
      "STRICT: False\n",
      "missing keys: 0\n",
      "unexpected keys: 0\n",
      "\n",
      "params total: 26,687,841 | trainable: 26,687,841\n"
     ]
    }
   ],
   "source": [
    "# 2) Создаём модель и загружаем веса\n",
    "model = MS_ResUNet()\n",
    "model.to(DEVICE)\n",
    "\n",
    "missing, unexpected = model.load_state_dict(state, strict=STRICT)\n",
    "model.eval()\n",
    "\n",
    "# 3) Выводим диагностику\n",
    "print(\"\\n=== load_state_dict report ===\")\n",
    "print(\"STRICT:\", STRICT)\n",
    "print(\"missing keys:\", len(missing))\n",
    "print(\"unexpected keys:\", len(unexpected))\n",
    "\n",
    "if missing:\n",
    "    print(\"\\n[missing] sample (up to 30):\")\n",
    "    for k in missing[:30]:\n",
    "        print(\"  \", k)\n",
    "\n",
    "if unexpected:\n",
    "    print(\"\\n[unexpected] sample (up to 30):\")\n",
    "    for k in unexpected[:30]:\n",
    "        print(\"  \", k)\n",
    "\n",
    "# 4) Кол-во параметров\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nparams total: {n_params:,} | trainable: {n_trainable:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29d797",
   "metadata": {},
   "source": [
    "## (Опционально) Быстрый forward-test на одном изображении\n",
    "\n",
    "Если у вас модель SR `x2`, и вход — **grayscale** `[1, 1, H, W]` в диапазоне `[0,1]`,\n",
    "то следующая ячейка:\n",
    "- читает картинку,\n",
    "- прогоняет через модель,\n",
    "- сохраняет результат `sr_debug.png`.\n",
    "\n",
    "Если у вас другой формат входа (RGB/YCbCr/16-bit), просто адаптируйте предобработку.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9627303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\sr_debug.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Вячеслав\\AppData\\Local\\Temp\\ipykernel_11892\\4096709630.py:25: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  Image.fromarray(y8, mode=\"L\").save(OUT_IMG)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Укажите путь к тестовому LR изображению (обычно png/jpg)\n",
    "LR_IMG = Path(r\"C:\\Users\\Вячеслав\\Documents\\superresolution\\DeepRockSR-2D\\carbonate2D\\carbonate2D_test_LR_default_X2\\3607x2.png\")  # <-- поменяйте\n",
    "\n",
    "OUT_IMG = Path(\"sr_debug.png\")\n",
    "\n",
    "assert LR_IMG.exists(), f\"LR_IMG не найден: {LR_IMG}\"\n",
    "\n",
    "# читаем как grayscale float32 [0,1]\n",
    "img = Image.open(LR_IMG).convert(\"L\")\n",
    "arr = np.array(img, dtype=np.float32) / 255.0  # [H,W]\n",
    "x = torch.from_numpy(arr)[None, None, ...].to(DEVICE)  # [1,1,H,W]\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = model(x)\n",
    "\n",
    "# если модель возвращает список/кортеж — берём первый элемент\n",
    "if isinstance(y, (list, tuple)):\n",
    "    y = y[0]\n",
    "\n",
    "y = y.detach().float().clamp(0, 1)[0, 0].cpu().numpy()\n",
    "y8 = (y * 255.0 + 0.5).astype(np.uint8)\n",
    "Image.fromarray(y8, mode=\"L\").save(OUT_IMG)\n",
    "\n",
    "print(\"Saved:\", OUT_IMG.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc2ee8-e183-4892-b96b-a83c3180bada",
   "metadata": {},
   "source": [
    "# Преобразование в .onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ca4811-f061-4997-aef7-384b3ee3e47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\Вячеслав\\Documents\\superresolution\\modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f58f94f3-171b-4221-9f2a-fc9317e99b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Вячеслав\\AppData\\Local\\Temp\\ipykernel_24168\\4060284732.py:14: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
      "  torch.onnx.export(\n",
      "W1225 15:19:46.145000 24168 site-packages\\torch\\onnx\\_internal\\exporter\\_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 17 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `RefineNet([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `RefineNet([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 17).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 137 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 17},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.0+cu130',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"lr\"<FLOAT,[s77,1,s53,s0]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"sr\"<FLOAT,[1,1,s53,s0]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"conv1.weight\"<FLOAT,[32,1,5,5]>{Tensor(...)},\n",
       "                %\"adapt_stage1_b.0.1_conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"adapt_stage1_b.0.2_conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_b.0.1_conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_b.0.2_conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_b.0.3_conv.bias\"<FLOAT,[256]>{TorchTensor(...)},\n",
       "                %\"adapt_stage2_b.0.1_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"adapt_stage2_b.0.2_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_b.0.1_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_b.0.2_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_b.0.3_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"adapt_stage3_b.0.1_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"adapt_stage3_b.0.2_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_b.0.1_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_b.0.2_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_b.0.3_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"adapt_stage4_b.0.1_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"adapt_stage4_b.0.2_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_b.0.1_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_b.0.2_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_b.0.3_conv.bias\"<FLOAT,[128]>{TorchTensor(...)},\n",
       "                %\"clf_conv1.bias\"<FLOAT,[64]>{TorchTensor(...)},\n",
       "                %\"clf_conv2.weight\"<FLOAT,[1,64,3,3]>{TorchTensor(...)},\n",
       "                %\"clf_conv2.bias\"<FLOAT,[1]>{TorchTensor<FLOAT,[1]>(Parameter containing: tensor([0.0633], requires_grad=True), name='clf_conv2.bias')},\n",
       "                %\"layer1.0.conv1.weight\"<FLOAT,[32,32,1,1]>{Tensor(...)},\n",
       "                %\"layer1.0.conv2.weight\"<FLOAT,[32,32,3,3]>{Tensor(...)},\n",
       "                %\"layer1.0.conv3.weight\"<FLOAT,[128,32,1,1]>{Tensor(...)},\n",
       "                %\"layer1.0.downsample.0.weight\"<FLOAT,[128,32,1,1]>{Tensor(...)},\n",
       "                %\"layer1.1.conv1.weight\"<FLOAT,[32,128,1,1]>{Tensor(...)},\n",
       "                %\"layer1.1.conv2.weight\"<FLOAT,[32,32,3,3]>{Tensor(...)},\n",
       "                %\"layer1.1.conv3.weight\"<FLOAT,[128,32,1,1]>{Tensor(...)},\n",
       "                %\"layer1.2.conv1.weight\"<FLOAT,[32,128,1,1]>{Tensor(...)},\n",
       "                %\"layer1.2.conv2.weight\"<FLOAT,[32,32,3,3]>{Tensor(...)},\n",
       "                %\"layer1.2.conv3.weight\"<FLOAT,[128,32,1,1]>{Tensor(...)},\n",
       "                %\"layer2.0.conv1.weight\"<FLOAT,[64,128,1,1]>{Tensor(...)},\n",
       "                %\"layer2.0.conv2.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
       "                %\"layer2.0.conv3.weight\"<FLOAT,[256,64,1,1]>{Tensor(...)},\n",
       "                %\"layer2.0.downsample.0.weight\"<FLOAT,[256,128,1,1]>{Tensor(...)},\n",
       "                %\"layer2.1.conv1.weight\"<FLOAT,[64,256,1,1]>{Tensor(...)},\n",
       "                %\"layer2.1.conv2.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
       "                %\"layer2.1.conv3.weight\"<FLOAT,[256,64,1,1]>{Tensor(...)},\n",
       "                %\"layer2.2.conv1.weight\"<FLOAT,[64,256,1,1]>{Tensor(...)},\n",
       "                %\"layer2.2.conv2.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
       "                %\"layer2.2.conv3.weight\"<FLOAT,[256,64,1,1]>{Tensor(...)},\n",
       "                %\"layer2.3.conv1.weight\"<FLOAT,[64,256,1,1]>{Tensor(...)},\n",
       "                %\"layer2.3.conv2.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
       "                %\"layer2.3.conv3.weight\"<FLOAT,[256,64,1,1]>{Tensor(...)},\n",
       "                %\"layer3.0.conv1.weight\"<FLOAT,[128,256,1,1]>{Tensor(...)},\n",
       "                %\"layer3.0.conv2.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"layer3.0.conv3.weight\"<FLOAT,[512,128,1,1]>{Tensor(...)},\n",
       "                %\"layer3.0.downsample.0.weight\"<FLOAT,[512,256,1,1]>{Tensor(...)},\n",
       "                %\"layer3.1.conv1.weight\"<FLOAT,[128,512,1,1]>{Tensor(...)},\n",
       "                %\"layer3.1.conv2.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"layer3.1.conv3.weight\"<FLOAT,[512,128,1,1]>{Tensor(...)},\n",
       "                %\"layer3.2.conv1.weight\"<FLOAT,[128,512,1,1]>{Tensor(...)},\n",
       "                %\"layer3.2.conv2.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"layer3.2.conv3.weight\"<FLOAT,[512,128,1,1]>{Tensor(...)},\n",
       "                %\"layer4.0.conv1.weight\"<FLOAT,[256,512,1,1]>{Tensor(...)},\n",
       "                %\"layer4.0.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"layer4.0.conv3.weight\"<FLOAT,[1024,256,1,1]>{Tensor(...)},\n",
       "                %\"layer4.0.downsample.0.weight\"<FLOAT,[1024,512,1,1]>{Tensor(...)},\n",
       "                %\"layer4.1.conv1.weight\"<FLOAT,[256,1024,1,1]>{Tensor(...)},\n",
       "                %\"layer4.1.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"layer4.1.conv3.weight\"<FLOAT,[1024,256,1,1]>{Tensor(...)},\n",
       "                %\"layer4.2.conv1.weight\"<FLOAT,[256,1024,1,1]>{Tensor(...)},\n",
       "                %\"layer4.2.conv2.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"layer4.2.conv3.weight\"<FLOAT,[1024,256,1,1]>{Tensor(...)},\n",
       "                %\"p_ims1d2_outl1_dimred.weight\"<FLOAT,[256,1024,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage1_b.0.1_conv.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage1_b.0.1_conv_relu_varout_dimred.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage1_b.0.2_conv.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage1_b.0.2_conv_relu_varout_dimred.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_pool.0.1_outvar_dimred.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_pool.0.2_outvar_dimred.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_pool.0.3_outvar_dimred.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_pool.0.4_outvar_dimred.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_b.0.1_conv.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_b.0.1_conv_relu_varout_dimred.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_b.0.2_conv.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_b.0.2_conv_relu_varout_dimred.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_b.0.3_conv.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_b.0.3_conv_relu_varout_dimred.weight\"<FLOAT,[256,256,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g1_b3_joint_varout_dimred.weight\"<FLOAT,[128,256,3,3]>{TorchTensor(...)},\n",
       "                %\"up_ps4.weight\"<FLOAT,[512,128,3,3]>{TorchTensor(...)},\n",
       "                %\"p_ims1d2_outl2_dimred.weight\"<FLOAT,[128,512,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage2_b.0.1_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage2_b.0.1_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage2_b.0.2_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage2_b.0.2_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage2_b2_joint_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_pool.0.1_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_pool.0.2_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_pool.0.3_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_pool.0.4_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_b.0.1_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_b.0.1_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_b.0.2_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_b.0.2_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_b.0.3_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_b.0.3_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g2_b3_joint_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"up_ps3.weight\"<FLOAT,[512,128,3,3]>{TorchTensor(...)},\n",
       "                %\"p_ims1d2_outl3_dimred.weight\"<FLOAT,[128,256,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage3_b.0.1_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage3_b.0.1_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage3_b.0.2_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage3_b.0.2_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage3_b2_joint_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_pool.0.1_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_pool.0.2_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_pool.0.3_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_pool.0.4_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_b.0.1_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_b.0.1_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_b.0.2_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_b.0.2_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_b.0.3_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_b.0.3_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g3_b3_joint_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"up_ps2.weight\"<FLOAT,[512,128,3,3]>{TorchTensor(...)},\n",
       "                %\"p_ims1d2_outl4_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage4_b.0.1_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage4_b.0.1_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage4_b.0.2_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage4_b.0.2_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"adapt_stage4_b2_joint_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_pool.0.1_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_pool.0.2_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_pool.0.3_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_pool.0.4_outvar_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_b.0.1_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_b.0.1_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_b.0.2_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_b.0.2_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_b.0.3_conv.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"mflow_conv_g4_b.0.3_conv_relu_varout_dimred.weight\"<FLOAT,[128,128,3,3]>{TorchTensor(...)},\n",
       "                %\"clf_conv1.weight\"<FLOAT,[64,128,5,5]>{TorchTensor(...)},\n",
       "                %\"val_546\"<INT64,[1]>{Tensor<INT64,[1]>(array([2]), name='val_546')},\n",
       "                %\"val_557\"<INT64,[1]>{Tensor<INT64,[1]>(array([3]), name='val_557')},\n",
       "                %\"conv1.weight_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"layer1.0.conv1.weight_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"layer1.0.conv2.weight_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"layer1.0.conv3.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer1.0.downsample.0.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer1.1.conv1.weight_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"layer1.1.conv2.weight_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"layer1.1.conv3.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer1.2.conv1.weight_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"layer1.2.conv2.weight_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"layer1.2.conv3.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer2.0.conv1.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer2.0.conv2.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer2.0.conv3.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer2.0.downsample.0.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer2.1.conv1.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer2.1.conv2.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer2.1.conv3.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer2.2.conv1.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer2.2.conv2.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer2.2.conv3.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer2.3.conv1.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer2.3.conv2.weight_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"layer2.3.conv3.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer3.0.conv1.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer3.0.conv2.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer3.0.conv3.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer3.0.downsample.0.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer3.1.conv1.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer3.1.conv2.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer3.1.conv3.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer3.2.conv1.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer3.2.conv2.weight_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"layer3.2.conv3.weight_bias\"<FLOAT,[512]>{Tensor(...)},\n",
       "                %\"layer4.0.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer4.0.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer4.0.conv3.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"layer4.0.downsample.0.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"layer4.1.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer4.1.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer4.1.conv3.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"layer4.2.conv1.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer4.2.conv2.weight_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"layer4.2.conv3.weight_bias\"<FLOAT,[1024]>{Tensor(...)},\n",
       "                %\"val_534\"<INT64,[]>{TensorProtoTensor<INT64,[]>(array(-3), name='val_534')},\n",
       "                %\"val_535\"<INT64,[]>{TensorProtoTensor<INT64,[]>(array(4), name='val_535')},\n",
       "                %\"val_536\"<INT64,[]>{TensorProtoTensor<INT64,[]>(array(2), name='val_536')},\n",
       "                %\"val_537\"<INT64,[]>{TensorProtoTensor<INT64,[]>(array(1), name='val_537')},\n",
       "                %\"val_539\"<INT64,[1]>{Tensor<INT64,[1]>(array([-1]), name='val_539')},\n",
       "                %\"val_547\"<INT64,[1]>{Tensor<INT64,[1]>(array([1]), name='val_547')}\n",
       "            ),\n",
       "        ) {\n",
       "              0 |  # node_Shape_0\n",
       "                   %\"val_0\"<INT64,[1]> ⬅️ ::Shape(%\"lr\") {start=2, end=3}\n",
       "              1 |  # node_sym_size_int_274\n",
       "                   %\"sym_size_int_274\"<INT64,[]> ⬅️ ::Squeeze(%\"val_0\")\n",
       "              2 |  # node_Shape_1\n",
       "                   %\"val_1\"<INT64,[1]> ⬅️ ::Shape(%\"lr\") {start=3, end=4}\n",
       "              3 |  # node_sym_size_int_275\n",
       "                   %\"sym_size_int_275\"<INT64,[]> ⬅️ ::Squeeze(%\"val_1\")\n",
       "              4 |  # node_Conv_288\n",
       "                   %\"getitem\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"lr\", %\"conv1.weight\"{...}, %\"conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "              5 |  # node_relu\n",
       "                   %\"relu\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"getitem\")\n",
       "              6 |  # node_Conv_290\n",
       "                   %\"getitem_3\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu\", %\"layer1.0.conv1.weight\"{...}, %\"layer1.0.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "              7 |  # node_relu_1\n",
       "                   %\"relu_1\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"getitem_3\")\n",
       "              8 |  # node_Conv_292\n",
       "                   %\"getitem_6\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_1\", %\"layer1.0.conv2.weight\"{...}, %\"layer1.0.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "              9 |  # node_relu_2\n",
       "                   %\"relu_2\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"getitem_6\")\n",
       "             10 |  # node_Conv_294\n",
       "                   %\"getitem_9\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_2\", %\"layer1.0.conv3.weight\"{...}, %\"layer1.0.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             11 |  # node_Conv_296\n",
       "                   %\"getitem_12\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu\", %\"layer1.0.downsample.0.weight\"{...}, %\"layer1.0.downsample.0.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             12 |  # node_add_217\n",
       "                   %\"add_217\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"getitem_9\", %\"getitem_12\")\n",
       "             13 |  # node_relu_3\n",
       "                   %\"relu_3\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"add_217\")\n",
       "             14 |  # node_Conv_298\n",
       "                   %\"getitem_15\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_3\", %\"layer1.1.conv1.weight\"{...}, %\"layer1.1.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             15 |  # node_relu_4\n",
       "                   %\"relu_4\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"getitem_15\")\n",
       "             16 |  # node_Conv_300\n",
       "                   %\"getitem_18\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_4\", %\"layer1.1.conv2.weight\"{...}, %\"layer1.1.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "             17 |  # node_relu_5\n",
       "                   %\"relu_5\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"getitem_18\")\n",
       "             18 |  # node_Conv_302\n",
       "                   %\"getitem_21\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_5\", %\"layer1.1.conv3.weight\"{...}, %\"layer1.1.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             19 |  # node_add_278\n",
       "                   %\"add_278\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"getitem_21\", %\"relu_3\")\n",
       "             20 |  # node_relu_6\n",
       "                   %\"relu_6\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"add_278\")\n",
       "             21 |  # node_Conv_304\n",
       "                   %\"getitem_24\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_6\", %\"layer1.2.conv1.weight\"{...}, %\"layer1.2.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             22 |  # node_relu_7\n",
       "                   %\"relu_7\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"getitem_24\")\n",
       "             23 |  # node_Conv_306\n",
       "                   %\"getitem_27\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_7\", %\"layer1.2.conv2.weight\"{...}, %\"layer1.2.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "             24 |  # node_relu_8\n",
       "                   %\"relu_8\"<FLOAT,[1,32,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"getitem_27\")\n",
       "             25 |  # node_Conv_308\n",
       "                   %\"getitem_30\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_8\", %\"layer1.2.conv3.weight\"{...}, %\"layer1.2.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             26 |  # node_add_339\n",
       "                   %\"add_339\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"getitem_30\", %\"relu_6\")\n",
       "             27 |  # node_relu_9\n",
       "                   %\"relu_9\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"add_339\")\n",
       "             28 |  # node_Conv_310\n",
       "                   %\"getitem_33\"<FLOAT,[1,64,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_9\", %\"layer2.0.conv1.weight\"{...}, %\"layer2.0.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             29 |  # node_relu_10\n",
       "                   %\"relu_10\"<FLOAT,[1,64,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"getitem_33\")\n",
       "             30 |  # node_Conv_312\n",
       "                   %\"getitem_36\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_10\", %\"layer2.0.conv2.weight\"{...}, %\"layer2.0.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(1, 1, 1, 1)}\n",
       "             31 |  # node_relu_11\n",
       "                   %\"relu_11\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"getitem_36\")\n",
       "             32 |  # node_Conv_314\n",
       "                   %\"getitem_39\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_11\", %\"layer2.0.conv3.weight\"{...}, %\"layer2.0.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             33 |  # node_Conv_316\n",
       "                   %\"getitem_42\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_9\", %\"layer2.0.downsample.0.weight\"{...}, %\"layer2.0.downsample.0.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(0, 0, 0, 0)}\n",
       "             34 |  # node_add_408\n",
       "                   %\"add_408\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"getitem_39\", %\"getitem_42\")\n",
       "             35 |  # node_relu_12\n",
       "                   %\"relu_12\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"add_408\")\n",
       "             36 |  # node_Conv_318\n",
       "                   %\"getitem_45\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_12\", %\"layer2.1.conv1.weight\"{...}, %\"layer2.1.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             37 |  # node_relu_13\n",
       "                   %\"relu_13\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"getitem_45\")\n",
       "             38 |  # node_Conv_320\n",
       "                   %\"getitem_48\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_13\", %\"layer2.1.conv2.weight\"{...}, %\"layer2.1.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "             39 |  # node_relu_14\n",
       "                   %\"relu_14\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"getitem_48\")\n",
       "             40 |  # node_Conv_322\n",
       "                   %\"getitem_51\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_14\", %\"layer2.1.conv3.weight\"{...}, %\"layer2.1.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             41 |  # node_add_469\n",
       "                   %\"add_469\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"getitem_51\", %\"relu_12\")\n",
       "             42 |  # node_relu_15\n",
       "                   %\"relu_15\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"add_469\")\n",
       "             43 |  # node_Conv_324\n",
       "                   %\"getitem_54\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_15\", %\"layer2.2.conv1.weight\"{...}, %\"layer2.2.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             44 |  # node_relu_16\n",
       "                   %\"relu_16\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"getitem_54\")\n",
       "             45 |  # node_Conv_326\n",
       "                   %\"getitem_57\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_16\", %\"layer2.2.conv2.weight\"{...}, %\"layer2.2.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "             46 |  # node_relu_17\n",
       "                   %\"relu_17\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"getitem_57\")\n",
       "             47 |  # node_Conv_328\n",
       "                   %\"getitem_60\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_17\", %\"layer2.2.conv3.weight\"{...}, %\"layer2.2.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             48 |  # node_add_530\n",
       "                   %\"add_530\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"getitem_60\", %\"relu_15\")\n",
       "             49 |  # node_relu_18\n",
       "                   %\"relu_18\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"add_530\")\n",
       "             50 |  # node_Conv_330\n",
       "                   %\"getitem_63\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_18\", %\"layer2.3.conv1.weight\"{...}, %\"layer2.3.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             51 |  # node_relu_19\n",
       "                   %\"relu_19\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"getitem_63\")\n",
       "             52 |  # node_Conv_332\n",
       "                   %\"getitem_66\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_19\", %\"layer2.3.conv2.weight\"{...}, %\"layer2.3.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "             53 |  # node_relu_20\n",
       "                   %\"relu_20\"<FLOAT,[1,64,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"getitem_66\")\n",
       "             54 |  # node_Conv_334\n",
       "                   %\"getitem_69\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_20\", %\"layer2.3.conv3.weight\"{...}, %\"layer2.3.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             55 |  # node_add_591\n",
       "                   %\"add_591\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"getitem_69\", %\"relu_18\")\n",
       "             56 |  # node_relu_21\n",
       "                   %\"relu_21\"<FLOAT,[1,256,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"add_591\")\n",
       "             57 |  # node_Conv_336\n",
       "                   %\"getitem_72\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_21\", %\"layer3.0.conv1.weight\"{...}, %\"layer3.0.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             58 |  # node_relu_22\n",
       "                   %\"relu_22\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"getitem_72\")\n",
       "             59 |  # node_Conv_338\n",
       "                   %\"getitem_75\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_22\", %\"layer3.0.conv2.weight\"{...}, %\"layer3.0.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(1, 1, 1, 1)}\n",
       "             60 |  # node_relu_23\n",
       "                   %\"relu_23\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"getitem_75\")\n",
       "             61 |  # node_Conv_340\n",
       "                   %\"getitem_78\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_23\", %\"layer3.0.conv3.weight\"{...}, %\"layer3.0.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             62 |  # node_Conv_342\n",
       "                   %\"getitem_81\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_21\", %\"layer3.0.downsample.0.weight\"{...}, %\"layer3.0.downsample.0.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(0, 0, 0, 0)}\n",
       "             63 |  # node_add_660\n",
       "                   %\"add_660\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"getitem_78\", %\"getitem_81\")\n",
       "             64 |  # node_relu_24\n",
       "                   %\"relu_24\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"add_660\")\n",
       "             65 |  # node_Conv_344\n",
       "                   %\"getitem_84\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_24\", %\"layer3.1.conv1.weight\"{...}, %\"layer3.1.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             66 |  # node_relu_25\n",
       "                   %\"relu_25\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"getitem_84\")\n",
       "             67 |  # node_Conv_346\n",
       "                   %\"getitem_87\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_25\", %\"layer3.1.conv2.weight\"{...}, %\"layer3.1.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "             68 |  # node_relu_26\n",
       "                   %\"relu_26\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"getitem_87\")\n",
       "             69 |  # node_Conv_348\n",
       "                   %\"getitem_90\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_26\", %\"layer3.1.conv3.weight\"{...}, %\"layer3.1.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             70 |  # node_add_721\n",
       "                   %\"add_721\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"getitem_90\", %\"relu_24\")\n",
       "             71 |  # node_relu_27\n",
       "                   %\"relu_27\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"add_721\")\n",
       "             72 |  # node_Conv_350\n",
       "                   %\"getitem_93\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_27\", %\"layer3.2.conv1.weight\"{...}, %\"layer3.2.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             73 |  # node_relu_28\n",
       "                   %\"relu_28\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"getitem_93\")\n",
       "             74 |  # node_Conv_352\n",
       "                   %\"getitem_96\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_28\", %\"layer3.2.conv2.weight\"{...}, %\"layer3.2.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "             75 |  # node_relu_29\n",
       "                   %\"relu_29\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"getitem_96\")\n",
       "             76 |  # node_Conv_354\n",
       "                   %\"getitem_99\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_29\", %\"layer3.2.conv3.weight\"{...}, %\"layer3.2.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             77 |  # node_add_782\n",
       "                   %\"add_782\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"getitem_99\", %\"relu_27\")\n",
       "             78 |  # node_relu_30\n",
       "                   %\"relu_30\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"add_782\")\n",
       "             79 |  # node_Conv_356\n",
       "                   %\"getitem_102\"<FLOAT,[1,256,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_30\", %\"layer4.0.conv1.weight\"{...}, %\"layer4.0.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             80 |  # node_relu_31\n",
       "                   %\"relu_31\"<FLOAT,[1,256,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"getitem_102\")\n",
       "             81 |  # node_Conv_358\n",
       "                   %\"getitem_105\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_31\", %\"layer4.0.conv2.weight\"{...}, %\"layer4.0.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(1, 1, 1, 1)}\n",
       "             82 |  # node_relu_32\n",
       "                   %\"relu_32\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"getitem_105\")\n",
       "             83 |  # node_Conv_360\n",
       "                   %\"getitem_108\"<FLOAT,[1,1024,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_32\", %\"layer4.0.conv3.weight\"{...}, %\"layer4.0.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             84 |  # node_Conv_362\n",
       "                   %\"getitem_111\"<FLOAT,[1,1024,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_30\", %\"layer4.0.downsample.0.weight\"{...}, %\"layer4.0.downsample.0.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(2, 2), pads=(0, 0, 0, 0)}\n",
       "             85 |  # node_add_851\n",
       "                   %\"add_851\"<FLOAT,[1,1024,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"getitem_108\", %\"getitem_111\")\n",
       "             86 |  # node_relu_33\n",
       "                   %\"relu_33\"<FLOAT,[1,1024,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"add_851\")\n",
       "             87 |  # node_Conv_364\n",
       "                   %\"getitem_114\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_33\", %\"layer4.1.conv1.weight\"{...}, %\"layer4.1.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             88 |  # node_relu_34\n",
       "                   %\"relu_34\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"getitem_114\")\n",
       "             89 |  # node_Conv_366\n",
       "                   %\"getitem_117\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_34\", %\"layer4.1.conv2.weight\"{...}, %\"layer4.1.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "             90 |  # node_relu_35\n",
       "                   %\"relu_35\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"getitem_117\")\n",
       "             91 |  # node_Conv_368\n",
       "                   %\"getitem_120\"<FLOAT,[1,1024,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_35\", %\"layer4.1.conv3.weight\"{...}, %\"layer4.1.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             92 |  # node_add_912\n",
       "                   %\"add_912\"<FLOAT,[1,1024,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"getitem_120\", %\"relu_33\")\n",
       "             93 |  # node_relu_36\n",
       "                   %\"relu_36\"<FLOAT,[1,1024,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"add_912\")\n",
       "             94 |  # node_Conv_370\n",
       "                   %\"getitem_123\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_36\", %\"layer4.2.conv1.weight\"{...}, %\"layer4.2.conv1.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             95 |  # node_relu_37\n",
       "                   %\"relu_37\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"getitem_123\")\n",
       "             96 |  # node_Conv_372\n",
       "                   %\"getitem_126\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_37\", %\"layer4.2.conv2.weight\"{...}, %\"layer4.2.conv2.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "             97 |  # node_relu_38\n",
       "                   %\"relu_38\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"getitem_126\")\n",
       "             98 |  # node_Conv_374\n",
       "                   %\"getitem_129\"<FLOAT,[1,1024,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_38\", %\"layer4.2.conv3.weight\"{...}, %\"layer4.2.conv3.weight_bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(0, 0, 0, 0)}\n",
       "             99 |  # node_add_973\n",
       "                   %\"add_973\"<FLOAT,[1,1024,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"getitem_129\", %\"relu_36\")\n",
       "            100 |  # node_relu_39\n",
       "                   %\"relu_39\"<FLOAT,[1,1024,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"add_973\")\n",
       "            101 |  # node_Conv_375\n",
       "                   %\"conv2d_44\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_39\", %\"p_ims1d2_outl1_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            102 |  # node_relu_40\n",
       "                   %\"relu_40\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"conv2d_44\")\n",
       "            103 |  # node_conv2d_45\n",
       "                   %\"conv2d_45\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_40\", %\"adapt_stage1_b.0.1_conv.weight\"{...}, %\"adapt_stage1_b.0.1_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            104 |  # node_relu_41\n",
       "                   %\"relu_41\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"conv2d_45\")\n",
       "            105 |  # node_Conv_376\n",
       "                   %\"conv2d_46\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_41\", %\"adapt_stage1_b.0.1_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            106 |  # node_add_1014\n",
       "                   %\"add_1014\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"conv2d_46\", %\"conv2d_44\")\n",
       "            107 |  # node_relu_42\n",
       "                   %\"relu_42\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"add_1014\")\n",
       "            108 |  # node_conv2d_47\n",
       "                   %\"conv2d_47\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_42\", %\"adapt_stage1_b.0.2_conv.weight\"{...}, %\"adapt_stage1_b.0.2_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            109 |  # node_relu_43\n",
       "                   %\"relu_43\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"conv2d_47\")\n",
       "            110 |  # node_Conv_377\n",
       "                   %\"conv2d_48\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_43\", %\"adapt_stage1_b.0.2_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            111 |  # node_add_1043\n",
       "                   %\"add_1043\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"conv2d_48\", %\"add_1014\")\n",
       "            112 |  # node_relu_44\n",
       "                   %\"relu_44\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"add_1043\")\n",
       "            113 |  # node_max_pool2d\n",
       "                   %\"max_pool2d\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::MaxPool(%\"relu_44\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            114 |  # node_Conv_378\n",
       "                   %\"conv2d_49\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"max_pool2d\", %\"mflow_conv_g1_pool.0.1_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            115 |  # node_add_1064\n",
       "                   %\"add_1064\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"conv2d_49\", %\"relu_44\")\n",
       "            116 |  # node_max_pool2d_1\n",
       "                   %\"max_pool2d_1\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::MaxPool(%\"conv2d_49\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            117 |  # node_Conv_379\n",
       "                   %\"conv2d_50\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"max_pool2d_1\", %\"mflow_conv_g1_pool.0.2_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            118 |  # node_add_1077\n",
       "                   %\"add_1077\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"conv2d_50\", %\"add_1064\")\n",
       "            119 |  # node_max_pool2d_2\n",
       "                   %\"max_pool2d_2\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::MaxPool(%\"conv2d_50\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            120 |  # node_Conv_380\n",
       "                   %\"conv2d_51\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"max_pool2d_2\", %\"mflow_conv_g1_pool.0.3_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            121 |  # node_add_1090\n",
       "                   %\"add_1090\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"conv2d_51\", %\"add_1077\")\n",
       "            122 |  # node_max_pool2d_3\n",
       "                   %\"max_pool2d_3\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::MaxPool(%\"conv2d_51\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            123 |  # node_Conv_381\n",
       "                   %\"conv2d_52\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"max_pool2d_3\", %\"mflow_conv_g1_pool.0.4_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            124 |  # node_add_1103\n",
       "                   %\"add_1103\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"conv2d_52\", %\"add_1090\")\n",
       "            125 |  # node_relu_45\n",
       "                   %\"relu_45\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"add_1103\")\n",
       "            126 |  # node_conv2d_53\n",
       "                   %\"conv2d_53\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_45\", %\"mflow_conv_g1_b.0.1_conv.weight\"{...}, %\"mflow_conv_g1_b.0.1_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            127 |  # node_relu_46\n",
       "                   %\"relu_46\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"conv2d_53\")\n",
       "            128 |  # node_Conv_382\n",
       "                   %\"conv2d_54\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_46\", %\"mflow_conv_g1_b.0.1_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            129 |  # node_add_1132\n",
       "                   %\"add_1132\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"conv2d_54\", %\"add_1103\")\n",
       "            130 |  # node_relu_47\n",
       "                   %\"relu_47\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"add_1132\")\n",
       "            131 |  # node_conv2d_55\n",
       "                   %\"conv2d_55\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_47\", %\"mflow_conv_g1_b.0.2_conv.weight\"{...}, %\"mflow_conv_g1_b.0.2_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            132 |  # node_relu_48\n",
       "                   %\"relu_48\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"conv2d_55\")\n",
       "            133 |  # node_Conv_383\n",
       "                   %\"conv2d_56\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_48\", %\"mflow_conv_g1_b.0.2_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            134 |  # node_add_1161\n",
       "                   %\"add_1161\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"conv2d_56\", %\"add_1132\")\n",
       "            135 |  # node_relu_49\n",
       "                   %\"relu_49\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"add_1161\")\n",
       "            136 |  # node_conv2d_57\n",
       "                   %\"conv2d_57\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_49\", %\"mflow_conv_g1_b.0.3_conv.weight\"{...}, %\"mflow_conv_g1_b.0.3_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            137 |  # node_relu_50\n",
       "                   %\"relu_50\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Relu(%\"conv2d_57\")\n",
       "            138 |  # node_Conv_384\n",
       "                   %\"conv2d_58\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"relu_50\", %\"mflow_conv_g1_b.0.3_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            139 |  # node_add_1190\n",
       "                   %\"add_1190\"<FLOAT,[1,256,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Add(%\"conv2d_58\", %\"add_1161\")\n",
       "            140 |  # node_Conv_385\n",
       "                   %\"conv2d_59\"<FLOAT,[1,128,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"add_1190\", %\"mflow_conv_g1_b3_joint_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            141 |  # node_Conv_386\n",
       "                   %\"conv2d_60\"<FLOAT,[1,512,(((s53 - 3)//8)) + 1,(((s0 - 3)//8)) + 1]> ⬅️ ::Conv(%\"conv2d_59\", %\"up_ps4.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            142 |  # node_pixel_shuffle\n",
       "                   %\"pixel_shuffle\"<FLOAT,[1,128,2*(((s53 - 3)//8)) + 2,2*(((s0 - 3)//8)) + 2]> ⬅️ ::DepthToSpace(%\"conv2d_60\") {blocksize=2, mode='CRD'}\n",
       "            143 |  # node_Conv_387\n",
       "                   %\"conv2d_61\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_30\", %\"p_ims1d2_outl2_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            144 |  # node_relu_51\n",
       "                   %\"relu_51\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"conv2d_61\")\n",
       "            145 |  # node_conv2d_62\n",
       "                   %\"conv2d_62\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_51\", %\"adapt_stage2_b.0.1_conv.weight\"{...}, %\"adapt_stage2_b.0.1_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            146 |  # node_relu_52\n",
       "                   %\"relu_52\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"conv2d_62\")\n",
       "            147 |  # node_Conv_388\n",
       "                   %\"conv2d_63\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_52\", %\"adapt_stage2_b.0.1_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            148 |  # node_add_1235\n",
       "                   %\"add_1235\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"conv2d_63\", %\"conv2d_61\")\n",
       "            149 |  # node_relu_53\n",
       "                   %\"relu_53\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"add_1235\")\n",
       "            150 |  # node_conv2d_64\n",
       "                   %\"conv2d_64\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_53\", %\"adapt_stage2_b.0.2_conv.weight\"{...}, %\"adapt_stage2_b.0.2_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            151 |  # node_relu_54\n",
       "                   %\"relu_54\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"conv2d_64\")\n",
       "            152 |  # node_Conv_389\n",
       "                   %\"conv2d_65\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_54\", %\"adapt_stage2_b.0.2_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            153 |  # node_add_1264\n",
       "                   %\"add_1264\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"conv2d_65\", %\"add_1235\")\n",
       "            154 |  # node_Conv_390\n",
       "                   %\"conv2d_66\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"add_1264\", %\"adapt_stage2_b2_joint_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            155 |  # node_add_1273\n",
       "                   %\"add_1273\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"conv2d_66\", %\"pixel_shuffle\")\n",
       "            156 |  # node_relu_55\n",
       "                   %\"relu_55\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"add_1273\")\n",
       "            157 |  # node_max_pool2d_4\n",
       "                   %\"max_pool2d_4\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::MaxPool(%\"relu_55\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            158 |  # node_Conv_391\n",
       "                   %\"conv2d_67\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"max_pool2d_4\", %\"mflow_conv_g2_pool.0.1_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            159 |  # node_add_1290\n",
       "                   %\"add_1290\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"conv2d_67\", %\"relu_55\")\n",
       "            160 |  # node_max_pool2d_5\n",
       "                   %\"max_pool2d_5\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::MaxPool(%\"conv2d_67\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            161 |  # node_Conv_392\n",
       "                   %\"conv2d_68\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"max_pool2d_5\", %\"mflow_conv_g2_pool.0.2_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            162 |  # node_add_1303\n",
       "                   %\"add_1303\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"conv2d_68\", %\"add_1290\")\n",
       "            163 |  # node_max_pool2d_6\n",
       "                   %\"max_pool2d_6\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::MaxPool(%\"conv2d_68\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            164 |  # node_Conv_393\n",
       "                   %\"conv2d_69\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"max_pool2d_6\", %\"mflow_conv_g2_pool.0.3_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            165 |  # node_add_1316\n",
       "                   %\"add_1316\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"conv2d_69\", %\"add_1303\")\n",
       "            166 |  # node_max_pool2d_7\n",
       "                   %\"max_pool2d_7\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::MaxPool(%\"conv2d_69\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            167 |  # node_Conv_394\n",
       "                   %\"conv2d_70\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"max_pool2d_7\", %\"mflow_conv_g2_pool.0.4_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            168 |  # node_add_1329\n",
       "                   %\"add_1329\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"conv2d_70\", %\"add_1316\")\n",
       "            169 |  # node_relu_56\n",
       "                   %\"relu_56\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"add_1329\")\n",
       "            170 |  # node_conv2d_71\n",
       "                   %\"conv2d_71\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_56\", %\"mflow_conv_g2_b.0.1_conv.weight\"{...}, %\"mflow_conv_g2_b.0.1_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            171 |  # node_relu_57\n",
       "                   %\"relu_57\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"conv2d_71\")\n",
       "            172 |  # node_Conv_395\n",
       "                   %\"conv2d_72\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_57\", %\"mflow_conv_g2_b.0.1_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            173 |  # node_add_1358\n",
       "                   %\"add_1358\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"conv2d_72\", %\"add_1329\")\n",
       "            174 |  # node_relu_58\n",
       "                   %\"relu_58\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"add_1358\")\n",
       "            175 |  # node_conv2d_73\n",
       "                   %\"conv2d_73\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_58\", %\"mflow_conv_g2_b.0.2_conv.weight\"{...}, %\"mflow_conv_g2_b.0.2_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            176 |  # node_relu_59\n",
       "                   %\"relu_59\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"conv2d_73\")\n",
       "            177 |  # node_Conv_396\n",
       "                   %\"conv2d_74\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_59\", %\"mflow_conv_g2_b.0.2_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            178 |  # node_add_1387\n",
       "                   %\"add_1387\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"conv2d_74\", %\"add_1358\")\n",
       "            179 |  # node_relu_60\n",
       "                   %\"relu_60\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"add_1387\")\n",
       "            180 |  # node_conv2d_75\n",
       "                   %\"conv2d_75\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_60\", %\"mflow_conv_g2_b.0.3_conv.weight\"{...}, %\"mflow_conv_g2_b.0.3_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            181 |  # node_relu_61\n",
       "                   %\"relu_61\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Relu(%\"conv2d_75\")\n",
       "            182 |  # node_Conv_397\n",
       "                   %\"conv2d_76\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"relu_61\", %\"mflow_conv_g2_b.0.3_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            183 |  # node_add_1416\n",
       "                   %\"add_1416\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Add(%\"conv2d_76\", %\"add_1387\")\n",
       "            184 |  # node_Conv_398\n",
       "                   %\"conv2d_77\"<FLOAT,[1,128,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"add_1416\", %\"mflow_conv_g2_b3_joint_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            185 |  # node_Conv_399\n",
       "                   %\"conv2d_78\"<FLOAT,[1,512,(((s53 - 3)//4)) + 1,(((s0 - 3)//4)) + 1]> ⬅️ ::Conv(%\"conv2d_77\", %\"up_ps3.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            186 |  # node_pixel_shuffle_1\n",
       "                   %\"pixel_shuffle_1\"<FLOAT,[1,128,2*(((s53 - 3)//4)) + 2,2*(((s0 - 3)//4)) + 2]> ⬅️ ::DepthToSpace(%\"conv2d_78\") {blocksize=2, mode='CRD'}\n",
       "            187 |  # node_add_1433\n",
       "                   %\"add_1433\"<INT64,[]> ⬅️ ::Add(%\"val_534\"{-3}, %\"sym_size_int_274\")\n",
       "            188 |  # node_floordiv_69\n",
       "                   %\"floordiv_69\"<INT64,[]> ⬅️ ::Div(%\"add_1433\", %\"val_535\"{4})\n",
       "            189 |  # node_mul_1404\n",
       "                   %\"mul_1404\"<INT64,[]> ⬅️ ::Mul(%\"val_536\"{2}, %\"floordiv_69\")\n",
       "            190 |  # node_add_1434\n",
       "                   %\"add_1434\"<INT64,[]> ⬅️ ::Add(%\"val_536\"{2}, %\"mul_1404\")\n",
       "            191 |  # node_floordiv_70\n",
       "                   %\"floordiv_70\"<INT64,[]> ⬅️ ::Div(%\"add_1433\", %\"val_536\"{2})\n",
       "            192 |  # node_add_1435\n",
       "                   %\"add_1435\"<INT64,[]> ⬅️ ::Add(%\"val_537\"{1}, %\"floordiv_70\")\n",
       "            193 |  # node_sub_629\n",
       "                   %\"sub_629\"<INT64,[]> ⬅️ ::Sub(%\"add_1434\", %\"add_1435\")\n",
       "            194 |  # node_add_1436\n",
       "                   %\"add_1436\"<INT64,[]> ⬅️ ::Add(%\"val_534\"{-3}, %\"sym_size_int_275\")\n",
       "            195 |  # node_floordiv_71\n",
       "                   %\"floordiv_71\"<INT64,[]> ⬅️ ::Div(%\"add_1436\", %\"val_535\"{4})\n",
       "            196 |  # node_mul_1405\n",
       "                   %\"mul_1405\"<INT64,[]> ⬅️ ::Mul(%\"val_536\"{2}, %\"floordiv_71\")\n",
       "            197 |  # node_add_1437\n",
       "                   %\"add_1437\"<INT64,[]> ⬅️ ::Add(%\"val_536\"{2}, %\"mul_1405\")\n",
       "            198 |  # node_floordiv_72\n",
       "                   %\"floordiv_72\"<INT64,[]> ⬅️ ::Div(%\"add_1436\", %\"val_536\"{2})\n",
       "            199 |  # node_add_1438\n",
       "                   %\"add_1438\"<INT64,[]> ⬅️ ::Add(%\"val_537\"{1}, %\"floordiv_72\")\n",
       "            200 |  # node_sub_630\n",
       "                   %\"sub_630\"<INT64,[]> ⬅️ ::Sub(%\"add_1437\", %\"add_1438\")\n",
       "            201 |  # node_floordiv_73\n",
       "                   %\"floordiv_73\"<INT64,[]> ⬅️ ::Div(%\"sub_629\", %\"val_536\"{2})\n",
       "            202 |  # node_sub_631\n",
       "                   %\"sub_631\"<INT64,[]> ⬅️ ::Sub(%\"sub_629\", %\"floordiv_73\")\n",
       "            203 |  # node_sub_632\n",
       "                   %\"sub_632\"<INT64,[]> ⬅️ ::Sub(%\"add_1434\", %\"sub_631\")\n",
       "            204 |  # node_floordiv_74\n",
       "                   %\"floordiv_74\"<INT64,[]> ⬅️ ::Div(%\"sub_630\", %\"val_536\"{2})\n",
       "            205 |  # node_sub_633\n",
       "                   %\"sub_633\"<INT64,[]> ⬅️ ::Sub(%\"sub_630\", %\"floordiv_74\")\n",
       "            206 |  # node_sub_634\n",
       "                   %\"sub_634\"<INT64,[]> ⬅️ ::Sub(%\"add_1437\", %\"sub_633\")\n",
       "            207 |  # node_Reshape_532\n",
       "                   %\"val_540\"<INT64,[1]> ⬅️ ::Reshape(%\"floordiv_73\", %\"val_539\"{[-1]}) {allowzero=0}\n",
       "            208 |  # node_Reshape_535\n",
       "                   %\"val_543\"<INT64,[1]> ⬅️ ::Reshape(%\"sub_632\", %\"val_539\"{[-1]}) {allowzero=0}\n",
       "            209 |  # node_slice_1\n",
       "                   %\"slice_1\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,2*(((s0 - 3)//4)) + 2]> ⬅️ ::Slice(%\"pixel_shuffle_1\", %\"val_540\", %\"val_543\", %\"val_546\"{[2]}, %\"val_547\"{[1]})\n",
       "            210 |  # node_Reshape_542\n",
       "                   %\"val_550\"<INT64,[1]> ⬅️ ::Reshape(%\"floordiv_74\", %\"val_539\"{[-1]}) {allowzero=0}\n",
       "            211 |  # node_Reshape_545\n",
       "                   %\"val_553\"<INT64,[1]> ⬅️ ::Reshape(%\"sub_634\", %\"val_539\"{[-1]}) {allowzero=0}\n",
       "            212 |  # node_slice_2\n",
       "                   %\"slice_2\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Slice(%\"slice_1\", %\"val_550\", %\"val_553\", %\"val_557\"{[3]}, %\"val_547\"{[1]})\n",
       "            213 |  # node_Conv_400\n",
       "                   %\"conv2d_79\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_21\", %\"p_ims1d2_outl3_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            214 |  # node_relu_62\n",
       "                   %\"relu_62\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"conv2d_79\")\n",
       "            215 |  # node_conv2d_80\n",
       "                   %\"conv2d_80\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_62\", %\"adapt_stage3_b.0.1_conv.weight\"{...}, %\"adapt_stage3_b.0.1_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            216 |  # node_relu_63\n",
       "                   %\"relu_63\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"conv2d_80\")\n",
       "            217 |  # node_Conv_401\n",
       "                   %\"conv2d_81\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_63\", %\"adapt_stage3_b.0.1_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            218 |  # node_add_1475\n",
       "                   %\"add_1475\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"conv2d_81\", %\"conv2d_79\")\n",
       "            219 |  # node_relu_64\n",
       "                   %\"relu_64\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"add_1475\")\n",
       "            220 |  # node_conv2d_82\n",
       "                   %\"conv2d_82\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_64\", %\"adapt_stage3_b.0.2_conv.weight\"{...}, %\"adapt_stage3_b.0.2_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            221 |  # node_relu_65\n",
       "                   %\"relu_65\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"conv2d_82\")\n",
       "            222 |  # node_Conv_402\n",
       "                   %\"conv2d_83\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_65\", %\"adapt_stage3_b.0.2_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            223 |  # node_add_1504\n",
       "                   %\"add_1504\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"conv2d_83\", %\"add_1475\")\n",
       "            224 |  # node_Conv_403\n",
       "                   %\"conv2d_84\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"add_1504\", %\"adapt_stage3_b2_joint_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            225 |  # node_add_1513\n",
       "                   %\"add_1513\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"conv2d_84\", %\"slice_2\")\n",
       "            226 |  # node_relu_66\n",
       "                   %\"relu_66\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"add_1513\")\n",
       "            227 |  # node_max_pool2d_8\n",
       "                   %\"max_pool2d_8\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::MaxPool(%\"relu_66\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            228 |  # node_Conv_404\n",
       "                   %\"conv2d_85\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"max_pool2d_8\", %\"mflow_conv_g3_pool.0.1_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            229 |  # node_add_1530\n",
       "                   %\"add_1530\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"conv2d_85\", %\"relu_66\")\n",
       "            230 |  # node_max_pool2d_9\n",
       "                   %\"max_pool2d_9\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::MaxPool(%\"conv2d_85\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            231 |  # node_Conv_405\n",
       "                   %\"conv2d_86\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"max_pool2d_9\", %\"mflow_conv_g3_pool.0.2_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            232 |  # node_add_1543\n",
       "                   %\"add_1543\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"conv2d_86\", %\"add_1530\")\n",
       "            233 |  # node_max_pool2d_10\n",
       "                   %\"max_pool2d_10\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::MaxPool(%\"conv2d_86\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            234 |  # node_Conv_406\n",
       "                   %\"conv2d_87\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"max_pool2d_10\", %\"mflow_conv_g3_pool.0.3_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            235 |  # node_add_1556\n",
       "                   %\"add_1556\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"conv2d_87\", %\"add_1543\")\n",
       "            236 |  # node_max_pool2d_11\n",
       "                   %\"max_pool2d_11\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::MaxPool(%\"conv2d_87\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            237 |  # node_Conv_407\n",
       "                   %\"conv2d_88\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"max_pool2d_11\", %\"mflow_conv_g3_pool.0.4_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            238 |  # node_add_1569\n",
       "                   %\"add_1569\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"conv2d_88\", %\"add_1556\")\n",
       "            239 |  # node_relu_67\n",
       "                   %\"relu_67\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"add_1569\")\n",
       "            240 |  # node_conv2d_89\n",
       "                   %\"conv2d_89\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_67\", %\"mflow_conv_g3_b.0.1_conv.weight\"{...}, %\"mflow_conv_g3_b.0.1_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            241 |  # node_relu_68\n",
       "                   %\"relu_68\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"conv2d_89\")\n",
       "            242 |  # node_Conv_408\n",
       "                   %\"conv2d_90\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_68\", %\"mflow_conv_g3_b.0.1_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            243 |  # node_add_1598\n",
       "                   %\"add_1598\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"conv2d_90\", %\"add_1569\")\n",
       "            244 |  # node_relu_69\n",
       "                   %\"relu_69\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"add_1598\")\n",
       "            245 |  # node_conv2d_91\n",
       "                   %\"conv2d_91\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_69\", %\"mflow_conv_g3_b.0.2_conv.weight\"{...}, %\"mflow_conv_g3_b.0.2_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            246 |  # node_relu_70\n",
       "                   %\"relu_70\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"conv2d_91\")\n",
       "            247 |  # node_Conv_409\n",
       "                   %\"conv2d_92\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_70\", %\"mflow_conv_g3_b.0.2_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            248 |  # node_add_1627\n",
       "                   %\"add_1627\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"conv2d_92\", %\"add_1598\")\n",
       "            249 |  # node_relu_71\n",
       "                   %\"relu_71\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"add_1627\")\n",
       "            250 |  # node_conv2d_93\n",
       "                   %\"conv2d_93\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_71\", %\"mflow_conv_g3_b.0.3_conv.weight\"{...}, %\"mflow_conv_g3_b.0.3_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            251 |  # node_relu_72\n",
       "                   %\"relu_72\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Relu(%\"conv2d_93\")\n",
       "            252 |  # node_Conv_410\n",
       "                   %\"conv2d_94\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"relu_72\", %\"mflow_conv_g3_b.0.3_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            253 |  # node_add_1656\n",
       "                   %\"add_1656\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Add(%\"conv2d_94\", %\"add_1627\")\n",
       "            254 |  # node_Conv_411\n",
       "                   %\"conv2d_95\"<FLOAT,[1,128,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"add_1656\", %\"mflow_conv_g3_b3_joint_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            255 |  # node_Conv_412\n",
       "                   %\"conv2d_96\"<FLOAT,[1,512,(((s53 - 3)//2)) + 1,(((s0 - 3)//2)) + 1]> ⬅️ ::Conv(%\"conv2d_95\", %\"up_ps2.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            256 |  # node_pixel_shuffle_2\n",
       "                   %\"pixel_shuffle_2\"<FLOAT,[1,128,2*(((s53 - 3)//2)) + 2,2*(((s0 - 3)//2)) + 2]> ⬅️ ::DepthToSpace(%\"conv2d_96\") {blocksize=2, mode='CRD'}\n",
       "            257 |  # node_Conv_413\n",
       "                   %\"conv2d_97\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_9\", %\"p_ims1d2_outl4_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            258 |  # node_relu_73\n",
       "                   %\"relu_73\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"conv2d_97\")\n",
       "            259 |  # node_conv2d_98\n",
       "                   %\"conv2d_98\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_73\", %\"adapt_stage4_b.0.1_conv.weight\"{...}, %\"adapt_stage4_b.0.1_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            260 |  # node_relu_74\n",
       "                   %\"relu_74\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"conv2d_98\")\n",
       "            261 |  # node_Conv_414\n",
       "                   %\"conv2d_99\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_74\", %\"adapt_stage4_b.0.1_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            262 |  # node_add_1701\n",
       "                   %\"add_1701\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"conv2d_99\", %\"conv2d_97\")\n",
       "            263 |  # node_relu_75\n",
       "                   %\"relu_75\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"add_1701\")\n",
       "            264 |  # node_conv2d_100\n",
       "                   %\"conv2d_100\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_75\", %\"adapt_stage4_b.0.2_conv.weight\"{...}, %\"adapt_stage4_b.0.2_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            265 |  # node_relu_76\n",
       "                   %\"relu_76\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"conv2d_100\")\n",
       "            266 |  # node_Conv_415\n",
       "                   %\"conv2d_101\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_76\", %\"adapt_stage4_b.0.2_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            267 |  # node_add_1730\n",
       "                   %\"add_1730\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"conv2d_101\", %\"add_1701\")\n",
       "            268 |  # node_Conv_416\n",
       "                   %\"conv2d_102\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"add_1730\", %\"adapt_stage4_b2_joint_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            269 |  # node_add_1739\n",
       "                   %\"add_1739\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"conv2d_102\", %\"pixel_shuffle_2\")\n",
       "            270 |  # node_relu_77\n",
       "                   %\"relu_77\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"add_1739\")\n",
       "            271 |  # node_max_pool2d_12\n",
       "                   %\"max_pool2d_12\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::MaxPool(%\"relu_77\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            272 |  # node_Conv_417\n",
       "                   %\"conv2d_103\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"max_pool2d_12\", %\"mflow_conv_g4_pool.0.1_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            273 |  # node_add_1756\n",
       "                   %\"add_1756\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"conv2d_103\", %\"relu_77\")\n",
       "            274 |  # node_max_pool2d_13\n",
       "                   %\"max_pool2d_13\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::MaxPool(%\"conv2d_103\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            275 |  # node_Conv_418\n",
       "                   %\"conv2d_104\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"max_pool2d_13\", %\"mflow_conv_g4_pool.0.2_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            276 |  # node_add_1769\n",
       "                   %\"add_1769\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"conv2d_104\", %\"add_1756\")\n",
       "            277 |  # node_max_pool2d_14\n",
       "                   %\"max_pool2d_14\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::MaxPool(%\"conv2d_104\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            278 |  # node_Conv_419\n",
       "                   %\"conv2d_105\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"max_pool2d_14\", %\"mflow_conv_g4_pool.0.3_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            279 |  # node_add_1782\n",
       "                   %\"add_1782\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"conv2d_105\", %\"add_1769\")\n",
       "            280 |  # node_max_pool2d_15\n",
       "                   %\"max_pool2d_15\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::MaxPool(%\"conv2d_105\") {kernel_shape=(5, 5), strides=(1, 1), dilations=(1, 1), auto_pad='NOTSET', pads=(2, 2, 2, 2), ceil_mode=0, storage_order=0}\n",
       "            281 |  # node_Conv_420\n",
       "                   %\"conv2d_106\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"max_pool2d_15\", %\"mflow_conv_g4_pool.0.4_outvar_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            282 |  # node_add_1795\n",
       "                   %\"add_1795\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"conv2d_106\", %\"add_1782\")\n",
       "            283 |  # node_relu_78\n",
       "                   %\"relu_78\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"add_1795\")\n",
       "            284 |  # node_conv2d_107\n",
       "                   %\"conv2d_107\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_78\", %\"mflow_conv_g4_b.0.1_conv.weight\"{...}, %\"mflow_conv_g4_b.0.1_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            285 |  # node_relu_79\n",
       "                   %\"relu_79\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"conv2d_107\")\n",
       "            286 |  # node_Conv_421\n",
       "                   %\"conv2d_108\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_79\", %\"mflow_conv_g4_b.0.1_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            287 |  # node_add_1824\n",
       "                   %\"add_1824\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"conv2d_108\", %\"add_1795\")\n",
       "            288 |  # node_relu_80\n",
       "                   %\"relu_80\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"add_1824\")\n",
       "            289 |  # node_conv2d_109\n",
       "                   %\"conv2d_109\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_80\", %\"mflow_conv_g4_b.0.2_conv.weight\"{...}, %\"mflow_conv_g4_b.0.2_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            290 |  # node_relu_81\n",
       "                   %\"relu_81\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"conv2d_109\")\n",
       "            291 |  # node_Conv_422\n",
       "                   %\"conv2d_110\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_81\", %\"mflow_conv_g4_b.0.2_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            292 |  # node_add_1853\n",
       "                   %\"add_1853\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"conv2d_110\", %\"add_1824\")\n",
       "            293 |  # node_relu_82\n",
       "                   %\"relu_82\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"add_1853\")\n",
       "            294 |  # node_conv2d_111\n",
       "                   %\"conv2d_111\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_82\", %\"mflow_conv_g4_b.0.3_conv.weight\"{...}, %\"mflow_conv_g4_b.0.3_conv.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            295 |  # node_relu_83\n",
       "                   %\"relu_83\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Relu(%\"conv2d_111\")\n",
       "            296 |  # node_Conv_423\n",
       "                   %\"conv2d_112\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"relu_83\", %\"mflow_conv_g4_b.0.3_conv_relu_varout_dimred.weight\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(1, 1, 1, 1)}\n",
       "            297 |  # node_add_1882\n",
       "                   %\"add_1882\"<FLOAT,[1,128,s53 - 2,s0 - 2]> ⬅️ ::Add(%\"conv2d_112\", %\"add_1853\")\n",
       "            298 |  # node_conv2d_113\n",
       "                   %\"conv2d_113\"<FLOAT,[1,64,s53 - 2,s0 - 2]> ⬅️ ::Conv(%\"add_1882\", %\"clf_conv1.weight\"{...}, %\"clf_conv1.bias\"{...}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(2, 2, 2, 2)}\n",
       "            299 |  # node_conv2d_114\n",
       "                   %\"sr\"<FLOAT,[1,1,s53,s0]> ⬅️ ::Conv(%\"conv2d_113\", %\"clf_conv2.weight\"{...}, %\"clf_conv2.bias\"{[0.06330403685569763]}) {group=1, auto_pad='NOTSET', dilations=(1, 1), strides=(1, 1), pads=(2, 2, 2, 2)}\n",
       "            return %\"sr\"<FLOAT,[1,1,s53,s0]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_conv1_weight: \"f32[32, 1, 5, 5]\", p_bn1_weight: \"f32[32]\", p_bn1_bias: \"f32[32]\", p_upct4_weight: \"f32[128, 128, 4, 4]\", p_upct4_bias: \"f32[128]\", p_upct3_weight: \"f32[128, 128, 4, 4]\", p_upct3_bias: \"f32[128]\", p_upct2_weight: \"f32[128, 128, 4, 4]\", p_upct2_bias: \"f32[128]\", p_layer1_0_conv1_weight: \"f32[32, 32, 1, 1]\", p_layer1_0_bn1_weight: \"f32[32]\", p_layer1_0_bn1_bias: \"f32[32]\", p_layer1_0_conv2_weight: \"f32[32, 32, 3, 3]\", p_layer1_0_bn2_weight: \"f32[32]\", p_layer1_0_bn2_bias: \"f32[32]\", p_layer1_0_conv3_weight: \"f32[128, 32, 1, 1]\", p_layer1_0_bn3_weight: \"f32[128]\", p_layer1_0_bn3_bias: \"f32[128]\", p_layer1_0_downsample_0_weight: \"f32[128, 32, 1, 1]\", p_layer1_0_downsample_1_weight: \"f32[128]\", p_layer1_0_downsample_1_bias: \"f32[128]\", p_layer1_1_conv1_weight: \"f32[32, 128, 1, 1]\", p_layer1_1_bn1_weight: \"f32[32]\", p_layer1_1_bn1_bias: \"f32[32]\", p_layer1_1_conv2_weight: \"f32[32, 32, 3, 3]\", p_layer1_1_bn2_weight: \"f32[32]\", p_layer1_1_bn2_bias: \"f32[32]\", p_layer1_1_conv3_weight: \"f32[128, 32, 1, 1]\", p_layer1_1_bn3_weight: \"f32[128]\", p_layer1_1_bn3_bias: \"f32[128]\", p_layer1_2_conv1_weight: \"f32[32, 128, 1, 1]\", p_layer1_2_bn1_weight: \"f32[32]\", p_layer1_2_bn1_bias: \"f32[32]\", p_layer1_2_conv2_weight: \"f32[32, 32, 3, 3]\", p_layer1_2_bn2_weight: \"f32[32]\", p_layer1_2_bn2_bias: \"f32[32]\", p_layer1_2_conv3_weight: \"f32[128, 32, 1, 1]\", p_layer1_2_bn3_weight: \"f32[128]\", p_layer1_2_bn3_bias: \"f32[128]\", p_layer2_0_conv1_weight: \"f32[64, 128, 1, 1]\", p_layer2_0_bn1_weight: \"f32[64]\", p_layer2_0_bn1_bias: \"f32[64]\", p_layer2_0_conv2_weight: \"f32[64, 64, 3, 3]\", p_layer2_0_bn2_weight: \"f32[64]\", p_layer2_0_bn2_bias: \"f32[64]\", p_layer2_0_conv3_weight: \"f32[256, 64, 1, 1]\", p_layer2_0_bn3_weight: \"f32[256]\", p_layer2_0_bn3_bias: \"f32[256]\", p_layer2_0_downsample_0_weight: \"f32[256, 128, 1, 1]\", p_layer2_0_downsample_1_weight: \"f32[256]\", p_layer2_0_downsample_1_bias: \"f32[256]\", p_layer2_1_conv1_weight: \"f32[64, 256, 1, 1]\", p_layer2_1_bn1_weight: \"f32[64]\", p_layer2_1_bn1_bias: \"f32[64]\", p_layer2_1_conv2_weight: \"f32[64, 64, 3, 3]\", p_layer2_1_bn2_weight: \"f32[64]\", p_layer2_1_bn2_bias: \"f32[64]\", p_layer2_1_conv3_weight: \"f32[256, 64, 1, 1]\", p_layer2_1_bn3_weight: \"f32[256]\", p_layer2_1_bn3_bias: \"f32[256]\", p_layer2_2_conv1_weight: \"f32[64, 256, 1, 1]\", p_layer2_2_bn1_weight: \"f32[64]\", p_layer2_2_bn1_bias: \"f32[64]\", p_layer2_2_conv2_weight: \"f32[64, 64, 3, 3]\", p_layer2_2_bn2_weight: \"f32[64]\", p_layer2_2_bn2_bias: \"f32[64]\", p_layer2_2_conv3_weight: \"f32[256, 64, 1, 1]\", p_layer2_2_bn3_weight: \"f32[256]\", p_layer2_2_bn3_bias: \"f32[256]\", p_layer2_3_conv1_weight: \"f32[64, 256, 1, 1]\", p_layer2_3_bn1_weight: \"f32[64]\", p_layer2_3_bn1_bias: \"f32[64]\", p_layer2_3_conv2_weight: \"f32[64, 64, 3, 3]\", p_layer2_3_bn2_weight: \"f32[64]\", p_layer2_3_bn2_bias: \"f32[64]\", p_layer2_3_conv3_weight: \"f32[256, 64, 1, 1]\", p_layer2_3_bn3_weight: \"f32[256]\", p_layer2_3_bn3_bias: \"f32[256]\", p_layer3_0_conv1_weight: \"f32[128, 256, 1, 1]\", p_layer3_0_bn1_weight: \"f32[128]\", p_layer3_0_bn1_bias: \"f32[128]\", p_layer3_0_conv2_weight: \"f32[128, 128, 3, 3]\", p_layer3_0_bn2_weight: \"f32[128]\", p_layer3_0_bn2_bias: \"f32[128]\", p_layer3_0_conv3_weight: \"f32[512, 128, 1, 1]\", p_layer3_0_bn3_weight: \"f32[512]\", p_layer3_0_bn3_bias: \"f32[512]\", p_layer3_0_downsample_0_weight: \"f32[512, 256, 1, 1]\", p_layer3_0_downsample_1_weight: \"f32[512]\", p_layer3_0_downsample_1_bias: \"f32[512]\", p_layer3_1_conv1_weight: \"f32[128, 512, 1, 1]\", p_layer3_1_bn1_weight: \"f32[128]\", p_layer3_1_bn1_bias: \"f32[128]\", p_layer3_1_conv2_weight: \"f32[128, 128, 3, 3]\", p_layer3_1_bn2_weight: \"f32[128]\", p_layer3_1_bn2_bias: \"f32[128]\", p_layer3_1_conv3_weight: \"f32[512, 128, 1, 1]\", p_layer3_1_bn3_weight: \"f32[512]\", p_layer3_1_bn3_bias: \"f32[512]\", p_layer3_2_conv1_weight: \"f32[128, 512, 1, 1]\", p_layer3_2_bn1_weight: \"f32[128]\", p_layer3_2_bn1_bias: \"f32[128]\", p_layer3_2_conv2_weight: \"f32[128, 128, 3, 3]\", p_layer3_2_bn2_weight: \"f32[128]\", p_layer3_2_bn2_bias: \"f32[128]\", p_layer3_2_conv3_weight: \"f32[512, 128, 1, 1]\", p_layer3_2_bn3_weight: \"f32[512]\", p_layer3_2_bn3_bias: \"f32[512]\", p_layer4_0_conv1_weight: \"f32[256, 512, 1, 1]\", p_layer4_0_bn1_weight: \"f32[256]\", p_layer4_0_bn1_bias: \"f32[256]\", p_layer4_0_conv2_weight: \"f32[256, 256, 3, 3]\", p_layer4_0_bn2_weight: \"f32[256]\", p_layer4_0_bn2_bias: \"f32[256]\", p_layer4_0_conv3_weight: \"f32[1024, 256, 1, 1]\", p_layer4_0_bn3_weight: \"f32[1024]\", p_layer4_0_bn3_bias: \"f32[1024]\", p_layer4_0_downsample_0_weight: \"f32[1024, 512, 1, 1]\", p_layer4_0_downsample_1_weight: \"f32[1024]\", p_layer4_0_downsample_1_bias: \"f32[1024]\", p_layer4_1_conv1_weight: \"f32[256, 1024, 1, 1]\", p_layer4_1_bn1_weight: \"f32[256]\", p_layer4_1_bn1_bias: \"f32[256]\", p_layer4_1_conv2_weight: \"f32[256, 256, 3, 3]\", p_layer4_1_bn2_weight: \"f32[256]\", p_layer4_1_bn2_bias: \"f32[256]\", p_layer4_1_conv3_weight: \"f32[1024, 256, 1, 1]\", p_layer4_1_bn3_weight: \"f32[1024]\", p_layer4_1_bn3_bias: \"f32[1024]\", p_layer4_2_conv1_weight: \"f32[256, 1024, 1, 1]\", p_layer4_2_bn1_weight: \"f32[256]\", p_layer4_2_bn1_bias: \"f32[256]\", p_layer4_2_conv2_weight: \"f32[256, 256, 3, 3]\", p_layer4_2_bn2_weight: \"f32[256]\", p_layer4_2_bn2_bias: \"f32[256]\", p_layer4_2_conv3_weight: \"f32[1024, 256, 1, 1]\", p_layer4_2_bn3_weight: \"f32[1024]\", p_layer4_2_bn3_bias: \"f32[1024]\", p_p_ims1d2_outl1_dimred_weight: \"f32[256, 1024, 3, 3]\", p_adapt_stage1_b_0_1_conv_weight: \"f32[256, 256, 3, 3]\", p_adapt_stage1_b_0_1_conv_bias: \"f32[256]\", p_adapt_stage1_b_0_1_conv_relu_varout_dimred_weight: \"f32[256, 256, 3, 3]\", p_adapt_stage1_b_0_2_conv_weight: \"f32[256, 256, 3, 3]\", p_adapt_stage1_b_0_2_conv_bias: \"f32[256]\", p_adapt_stage1_b_0_2_conv_relu_varout_dimred_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_pool_0_1_outvar_dimred_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_pool_0_2_outvar_dimred_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_pool_0_3_outvar_dimred_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_pool_0_4_outvar_dimred_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_b_0_1_conv_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_b_0_1_conv_bias: \"f32[256]\", p_mflow_conv_g1_b_0_1_conv_relu_varout_dimred_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_b_0_2_conv_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_b_0_2_conv_bias: \"f32[256]\", p_mflow_conv_g1_b_0_2_conv_relu_varout_dimred_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_b_0_3_conv_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_b_0_3_conv_bias: \"f32[256]\", p_mflow_conv_g1_b_0_3_conv_relu_varout_dimred_weight: \"f32[256, 256, 3, 3]\", p_mflow_conv_g1_b3_joint_varout_dimred_weight: \"f32[128, 256, 3, 3]\", p_up_ps4_weight: \"f32[512, 128, 3, 3]\", p_p_ims1d2_outl2_dimred_weight: \"f32[128, 512, 3, 3]\", p_adapt_stage2_b_0_1_conv_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage2_b_0_1_conv_bias: \"f32[128]\", p_adapt_stage2_b_0_1_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage2_b_0_2_conv_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage2_b_0_2_conv_bias: \"f32[128]\", p_adapt_stage2_b_0_2_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage2_b2_joint_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_pool_0_1_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_pool_0_2_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_pool_0_3_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_pool_0_4_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_b_0_1_conv_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_b_0_1_conv_bias: \"f32[128]\", p_mflow_conv_g2_b_0_1_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_b_0_2_conv_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_b_0_2_conv_bias: \"f32[128]\", p_mflow_conv_g2_b_0_2_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_b_0_3_conv_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_b_0_3_conv_bias: \"f32[128]\", p_mflow_conv_g2_b_0_3_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g2_b3_joint_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_up_ps3_weight: \"f32[512, 128, 3, 3]\", p_p_ims1d2_outl3_dimred_weight: \"f32[128, 256, 3, 3]\", p_adapt_stage3_b_0_1_conv_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage3_b_0_1_conv_bias: \"f32[128]\", p_adapt_stage3_b_0_1_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage3_b_0_2_conv_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage3_b_0_2_conv_bias: \"f32[128]\", p_adapt_stage3_b_0_2_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage3_b2_joint_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_pool_0_1_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_pool_0_2_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_pool_0_3_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_pool_0_4_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_b_0_1_conv_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_b_0_1_conv_bias: \"f32[128]\", p_mflow_conv_g3_b_0_1_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_b_0_2_conv_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_b_0_2_conv_bias: \"f32[128]\", p_mflow_conv_g3_b_0_2_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_b_0_3_conv_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_b_0_3_conv_bias: \"f32[128]\", p_mflow_conv_g3_b_0_3_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g3_b3_joint_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_up_ps2_weight: \"f32[512, 128, 3, 3]\", p_p_ims1d2_outl4_dimred_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage4_b_0_1_conv_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage4_b_0_1_conv_bias: \"f32[128]\", p_adapt_stage4_b_0_1_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage4_b_0_2_conv_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage4_b_0_2_conv_bias: \"f32[128]\", p_adapt_stage4_b_0_2_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_adapt_stage4_b2_joint_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g4_pool_0_1_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g4_pool_0_2_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g4_pool_0_3_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g4_pool_0_4_outvar_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g4_b_0_1_conv_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g4_b_0_1_conv_bias: \"f32[128]\", p_mflow_conv_g4_b_0_1_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g4_b_0_2_conv_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g4_b_0_2_conv_bias: \"f32[128]\", p_mflow_conv_g4_b_0_2_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g4_b_0_3_conv_weight: \"f32[128, 128, 3, 3]\", p_mflow_conv_g4_b_0_3_conv_bias: \"f32[128]\", p_mflow_conv_g4_b_0_3_conv_relu_varout_dimred_weight: \"f32[128, 128, 3, 3]\", p_clf_conv1_weight: \"f32[64, 128, 5, 5]\", p_clf_conv1_bias: \"f32[64]\", p_clf_conv2_weight: \"f32[1, 64, 3, 3]\", p_clf_conv2_bias: \"f32[1]\", b_bn1_running_mean: \"f32[32]\", b_bn1_running_var: \"f32[32]\", b_bn1_num_batches_tracked: \"i64[]\", b_layer1_0_bn1_running_mean: \"f32[32]\", b_layer1_0_bn1_running_var: \"f32[32]\", b_layer1_0_bn1_num_batches_tracked: \"i64[]\", b_layer1_0_bn2_running_mean: \"f32[32]\", b_layer1_0_bn2_running_var: \"f32[32]\", b_layer1_0_bn2_num_batches_tracked: \"i64[]\", b_layer1_0_bn3_running_mean: \"f32[128]\", b_layer1_0_bn3_running_var: \"f32[128]\", b_layer1_0_bn3_num_batches_tracked: \"i64[]\", b_layer1_0_downsample_1_running_mean: \"f32[128]\", b_layer1_0_downsample_1_running_var: \"f32[128]\", b_layer1_0_downsample_1_num_batches_tracked: \"i64[]\", b_layer1_1_bn1_running_mean: \"f32[32]\", b_layer1_1_bn1_running_var: \"f32[32]\", b_layer1_1_bn1_num_batches_tracked: \"i64[]\", b_layer1_1_bn2_running_mean: \"f32[32]\", b_layer1_1_bn2_running_var: \"f32[32]\", b_layer1_1_bn2_num_batches_tracked: \"i64[]\", b_layer1_1_bn3_running_mean: \"f32[128]\", b_layer1_1_bn3_running_var: \"f32[128]\", b_layer1_1_bn3_num_batches_tracked: \"i64[]\", b_layer1_2_bn1_running_mean: \"f32[32]\", b_layer1_2_bn1_running_var: \"f32[32]\", b_layer1_2_bn1_num_batches_tracked: \"i64[]\", b_layer1_2_bn2_running_mean: \"f32[32]\", b_layer1_2_bn2_running_var: \"f32[32]\", b_layer1_2_bn2_num_batches_tracked: \"i64[]\", b_layer1_2_bn3_running_mean: \"f32[128]\", b_layer1_2_bn3_running_var: \"f32[128]\", b_layer1_2_bn3_num_batches_tracked: \"i64[]\", b_layer2_0_bn1_running_mean: \"f32[64]\", b_layer2_0_bn1_running_var: \"f32[64]\", b_layer2_0_bn1_num_batches_tracked: \"i64[]\", b_layer2_0_bn2_running_mean: \"f32[64]\", b_layer2_0_bn2_running_var: \"f32[64]\", b_layer2_0_bn2_num_batches_tracked: \"i64[]\", b_layer2_0_bn3_running_mean: \"f32[256]\", b_layer2_0_bn3_running_var: \"f32[256]\", b_layer2_0_bn3_num_batches_tracked: \"i64[]\", b_layer2_0_downsample_1_running_mean: \"f32[256]\", b_layer2_0_downsample_1_running_var: \"f32[256]\", b_layer2_0_downsample_1_num_batches_tracked: \"i64[]\", b_layer2_1_bn1_running_mean: \"f32[64]\", b_layer2_1_bn1_running_var: \"f32[64]\", b_layer2_1_bn1_num_batches_tracked: \"i64[]\", b_layer2_1_bn2_running_mean: \"f32[64]\", b_layer2_1_bn2_running_var: \"f32[64]\", b_layer2_1_bn2_num_batches_tracked: \"i64[]\", b_layer2_1_bn3_running_mean: \"f32[256]\", b_layer2_1_bn3_running_var: \"f32[256]\", b_layer2_1_bn3_num_batches_tracked: \"i64[]\", b_layer2_2_bn1_running_mean: \"f32[64]\", b_layer2_2_bn1_running_var: \"f32[64]\", b_layer2_2_bn1_num_batches_tracked: \"i64[]\", b_layer2_2_bn2_running_mean: \"f32[64]\", b_layer2_2_bn2_running_var: \"f32[64]\", b_layer2_2_bn2_num_batches_tracked: \"i64[]\", b_layer2_2_bn3_running_mean: \"f32[256]\", b_layer2_2_bn3_running_var: \"f32[256]\", b_layer2_2_bn3_num_batches_tracked: \"i64[]\", b_layer2_3_bn1_running_mean: \"f32[64]\", b_layer2_3_bn1_running_var: \"f32[64]\", b_layer2_3_bn1_num_batches_tracked: \"i64[]\", b_layer2_3_bn2_running_mean: \"f32[64]\", b_layer2_3_bn2_running_var: \"f32[64]\", b_layer2_3_bn2_num_batches_tracked: \"i64[]\", b_layer2_3_bn3_running_mean: \"f32[256]\", b_layer2_3_bn3_running_var: \"f32[256]\", b_layer2_3_bn3_num_batches_tracked: \"i64[]\", b_layer3_0_bn1_running_mean: \"f32[128]\", b_layer3_0_bn1_running_var: \"f32[128]\", b_layer3_0_bn1_num_batches_tracked: \"i64[]\", b_layer3_0_bn2_running_mean: \"f32[128]\", b_layer3_0_bn2_running_var: \"f32[128]\", b_layer3_0_bn2_num_batches_tracked: \"i64[]\", b_layer3_0_bn3_running_mean: \"f32[512]\", b_layer3_0_bn3_running_var: \"f32[512]\", b_layer3_0_bn3_num_batches_tracked: \"i64[]\", b_layer3_0_downsample_1_running_mean: \"f32[512]\", b_layer3_0_downsample_1_running_var: \"f32[512]\", b_layer3_0_downsample_1_num_batches_tracked: \"i64[]\", b_layer3_1_bn1_running_mean: \"f32[128]\", b_layer3_1_bn1_running_var: \"f32[128]\", b_layer3_1_bn1_num_batches_tracked: \"i64[]\", b_layer3_1_bn2_running_mean: \"f32[128]\", b_layer3_1_bn2_running_var: \"f32[128]\", b_layer3_1_bn2_num_batches_tracked: \"i64[]\", b_layer3_1_bn3_running_mean: \"f32[512]\", b_layer3_1_bn3_running_var: \"f32[512]\", b_layer3_1_bn3_num_batches_tracked: \"i64[]\", b_layer3_2_bn1_running_mean: \"f32[128]\", b_layer3_2_bn1_running_var: \"f32[128]\", b_layer3_2_bn1_num_batches_tracked: \"i64[]\", b_layer3_2_bn2_running_mean: \"f32[128]\", b_layer3_2_bn2_running_var: \"f32[128]\", b_layer3_2_bn2_num_batches_tracked: \"i64[]\", b_layer3_2_bn3_running_mean: \"f32[512]\", b_layer3_2_bn3_running_var: \"f32[512]\", b_layer3_2_bn3_num_batches_tracked: \"i64[]\", b_layer4_0_bn1_running_mean: \"f32[256]\", b_layer4_0_bn1_running_var: \"f32[256]\", b_layer4_0_bn1_num_batches_tracked: \"i64[]\", b_layer4_0_bn2_running_mean: \"f32[256]\", b_layer4_0_bn2_running_var: \"f32[256]\", b_layer4_0_bn2_num_batches_tracked: \"i64[]\", b_layer4_0_bn3_running_mean: \"f32[1024]\", b_layer4_0_bn3_running_var: \"f32[1024]\", b_layer4_0_bn3_num_batches_tracked: \"i64[]\", b_layer4_0_downsample_1_running_mean: \"f32[1024]\", b_layer4_0_downsample_1_running_var: \"f32[1024]\", b_layer4_0_downsample_1_num_batches_tracked: \"i64[]\", b_layer4_1_bn1_running_mean: \"f32[256]\", b_layer4_1_bn1_running_var: \"f32[256]\", b_layer4_1_bn1_num_batches_tracked: \"i64[]\", b_layer4_1_bn2_running_mean: \"f32[256]\", b_layer4_1_bn2_running_var: \"f32[256]\", b_layer4_1_bn2_num_batches_tracked: \"i64[]\", b_layer4_1_bn3_running_mean: \"f32[1024]\", b_layer4_1_bn3_running_var: \"f32[1024]\", b_layer4_1_bn3_num_batches_tracked: \"i64[]\", b_layer4_2_bn1_running_mean: \"f32[256]\", b_layer4_2_bn1_running_var: \"f32[256]\", b_layer4_2_bn1_num_batches_tracked: \"i64[]\", b_layer4_2_bn2_running_mean: \"f32[256]\", b_layer4_2_bn2_running_var: \"f32[256]\", b_layer4_2_bn2_num_batches_tracked: \"i64[]\", b_layer4_2_bn3_running_mean: \"f32[1024]\", b_layer4_2_bn3_running_var: \"f32[1024]\", b_layer4_2_bn3_num_batches_tracked: \"i64[]\", x: \"f32[s77, 1, s53, s0]\"):\n",
       "                     # \n",
       "                    sym_size_int_274: \"Sym(s53)\" = torch.ops.aten.sym_size.int(x, 2)\n",
       "                    sym_size_int_275: \"Sym(s0)\" = torch.ops.aten.sym_size.int(x, 3)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d: \"f32[s77, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(x, p_conv1_weight, None, [1, 1], [1, 1]);  x = p_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d, p_bn1_weight, p_bn1_bias, b_bn1_running_mean, b_bn1_running_var, 0.1, 1e-05);  conv2d = p_bn1_weight = p_bn1_bias = b_bn1_running_mean = b_bn1_running_var = None\n",
       "                    getitem: \"f32[1, 32, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training[0];  _native_batch_norm_legit_no_training = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(getitem);  getitem = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_1: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu, p_layer1_0_conv1_weight);  p_layer1_0_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_1 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_1, p_layer1_0_bn1_weight, p_layer1_0_bn1_bias, b_layer1_0_bn1_running_mean, b_layer1_0_bn1_running_var, 0.1, 1e-05);  conv2d_1 = p_layer1_0_bn1_weight = p_layer1_0_bn1_bias = b_layer1_0_bn1_running_mean = b_layer1_0_bn1_running_var = None\n",
       "                    getitem_3: \"f32[1, 32, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_1[0];  _native_batch_norm_legit_no_training_1 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_1: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(getitem_3);  getitem_3 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_2: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_1, p_layer1_0_conv2_weight, None, [1, 1], [1, 1]);  relu_1 = p_layer1_0_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_2 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_2, p_layer1_0_bn2_weight, p_layer1_0_bn2_bias, b_layer1_0_bn2_running_mean, b_layer1_0_bn2_running_var, 0.1, 1e-05);  conv2d_2 = p_layer1_0_bn2_weight = p_layer1_0_bn2_bias = b_layer1_0_bn2_running_mean = b_layer1_0_bn2_running_var = None\n",
       "                    getitem_6: \"f32[1, 32, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_2[0];  _native_batch_norm_legit_no_training_2 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_2: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(getitem_6);  getitem_6 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_3: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_2, p_layer1_0_conv3_weight);  relu_2 = p_layer1_0_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_3 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_3, p_layer1_0_bn3_weight, p_layer1_0_bn3_bias, b_layer1_0_bn3_running_mean, b_layer1_0_bn3_running_var, 0.1, 1e-05);  conv2d_3 = p_layer1_0_bn3_weight = p_layer1_0_bn3_bias = b_layer1_0_bn3_running_mean = b_layer1_0_bn3_running_var = None\n",
       "                    getitem_9: \"f32[1, 128, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_3[0];  _native_batch_norm_legit_no_training_3 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_4: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu, p_layer1_0_downsample_0_weight);  relu = p_layer1_0_downsample_0_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_4 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_4, p_layer1_0_downsample_1_weight, p_layer1_0_downsample_1_bias, b_layer1_0_downsample_1_running_mean, b_layer1_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_4 = p_layer1_0_downsample_1_weight = p_layer1_0_downsample_1_bias = b_layer1_0_downsample_1_running_mean = b_layer1_0_downsample_1_running_var = None\n",
       "                    getitem_12: \"f32[1, 128, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_4[0];  _native_batch_norm_legit_no_training_4 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_217: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(getitem_9, getitem_12);  getitem_9 = getitem_12 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_3: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(add_217);  add_217 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_5: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_3, p_layer1_1_conv1_weight);  p_layer1_1_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_5 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_5, p_layer1_1_bn1_weight, p_layer1_1_bn1_bias, b_layer1_1_bn1_running_mean, b_layer1_1_bn1_running_var, 0.1, 1e-05);  conv2d_5 = p_layer1_1_bn1_weight = p_layer1_1_bn1_bias = b_layer1_1_bn1_running_mean = b_layer1_1_bn1_running_var = None\n",
       "                    getitem_15: \"f32[1, 32, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_5[0];  _native_batch_norm_legit_no_training_5 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_4: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(getitem_15);  getitem_15 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_6: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_4, p_layer1_1_conv2_weight, None, [1, 1], [1, 1]);  relu_4 = p_layer1_1_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_6 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_6, p_layer1_1_bn2_weight, p_layer1_1_bn2_bias, b_layer1_1_bn2_running_mean, b_layer1_1_bn2_running_var, 0.1, 1e-05);  conv2d_6 = p_layer1_1_bn2_weight = p_layer1_1_bn2_bias = b_layer1_1_bn2_running_mean = b_layer1_1_bn2_running_var = None\n",
       "                    getitem_18: \"f32[1, 32, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_6[0];  _native_batch_norm_legit_no_training_6 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_5: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(getitem_18);  getitem_18 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_7: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_5, p_layer1_1_conv3_weight);  relu_5 = p_layer1_1_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_7 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_7, p_layer1_1_bn3_weight, p_layer1_1_bn3_bias, b_layer1_1_bn3_running_mean, b_layer1_1_bn3_running_var, 0.1, 1e-05);  conv2d_7 = p_layer1_1_bn3_weight = p_layer1_1_bn3_bias = b_layer1_1_bn3_running_mean = b_layer1_1_bn3_running_var = None\n",
       "                    getitem_21: \"f32[1, 128, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_7[0];  _native_batch_norm_legit_no_training_7 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_278: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(getitem_21, relu_3);  getitem_21 = relu_3 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_6: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(add_278);  add_278 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_8: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_6, p_layer1_2_conv1_weight);  p_layer1_2_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_8 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_8, p_layer1_2_bn1_weight, p_layer1_2_bn1_bias, b_layer1_2_bn1_running_mean, b_layer1_2_bn1_running_var, 0.1, 1e-05);  conv2d_8 = p_layer1_2_bn1_weight = p_layer1_2_bn1_bias = b_layer1_2_bn1_running_mean = b_layer1_2_bn1_running_var = None\n",
       "                    getitem_24: \"f32[1, 32, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_8[0];  _native_batch_norm_legit_no_training_8 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_7: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(getitem_24);  getitem_24 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_9: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_7, p_layer1_2_conv2_weight, None, [1, 1], [1, 1]);  relu_7 = p_layer1_2_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_9 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_9, p_layer1_2_bn2_weight, p_layer1_2_bn2_bias, b_layer1_2_bn2_running_mean, b_layer1_2_bn2_running_var, 0.1, 1e-05);  conv2d_9 = p_layer1_2_bn2_weight = p_layer1_2_bn2_bias = b_layer1_2_bn2_running_mean = b_layer1_2_bn2_running_var = None\n",
       "                    getitem_27: \"f32[1, 32, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_9[0];  _native_batch_norm_legit_no_training_9 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_8: \"f32[1, 32, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(getitem_27);  getitem_27 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_10: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_8, p_layer1_2_conv3_weight);  relu_8 = p_layer1_2_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_10 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_10, p_layer1_2_bn3_weight, p_layer1_2_bn3_bias, b_layer1_2_bn3_running_mean, b_layer1_2_bn3_running_var, 0.1, 1e-05);  conv2d_10 = p_layer1_2_bn3_weight = p_layer1_2_bn3_bias = b_layer1_2_bn3_running_mean = b_layer1_2_bn3_running_var = None\n",
       "                    getitem_30: \"f32[1, 128, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_10[0];  _native_batch_norm_legit_no_training_10 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_339: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(getitem_30, relu_6);  getitem_30 = relu_6 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_9: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(add_339);  add_339 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_11: \"f32[1, 64, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_9, p_layer2_0_conv1_weight);  p_layer2_0_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_11 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_11, p_layer2_0_bn1_weight, p_layer2_0_bn1_bias, b_layer2_0_bn1_running_mean, b_layer2_0_bn1_running_var, 0.1, 1e-05);  conv2d_11 = p_layer2_0_bn1_weight = p_layer2_0_bn1_bias = b_layer2_0_bn1_running_mean = b_layer2_0_bn1_running_var = None\n",
       "                    getitem_33: \"f32[1, 64, s53 - 2, s0 - 2]\" = _native_batch_norm_legit_no_training_11[0];  _native_batch_norm_legit_no_training_11 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_10: \"f32[1, 64, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(getitem_33);  getitem_33 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_12: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_10, p_layer2_0_conv2_weight, None, [2, 2], [1, 1]);  relu_10 = p_layer2_0_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_12 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_12, p_layer2_0_bn2_weight, p_layer2_0_bn2_bias, b_layer2_0_bn2_running_mean, b_layer2_0_bn2_running_var, 0.1, 1e-05);  conv2d_12 = p_layer2_0_bn2_weight = p_layer2_0_bn2_bias = b_layer2_0_bn2_running_mean = b_layer2_0_bn2_running_var = None\n",
       "                    getitem_36: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_12[0];  _native_batch_norm_legit_no_training_12 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_11: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(getitem_36);  getitem_36 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_13: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_11, p_layer2_0_conv3_weight);  relu_11 = p_layer2_0_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_13 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_13, p_layer2_0_bn3_weight, p_layer2_0_bn3_bias, b_layer2_0_bn3_running_mean, b_layer2_0_bn3_running_var, 0.1, 1e-05);  conv2d_13 = p_layer2_0_bn3_weight = p_layer2_0_bn3_bias = b_layer2_0_bn3_running_mean = b_layer2_0_bn3_running_var = None\n",
       "                    getitem_39: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_13[0];  _native_batch_norm_legit_no_training_13 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_14: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_9, p_layer2_0_downsample_0_weight, None, [2, 2]);  p_layer2_0_downsample_0_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_14 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_14, p_layer2_0_downsample_1_weight, p_layer2_0_downsample_1_bias, b_layer2_0_downsample_1_running_mean, b_layer2_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_14 = p_layer2_0_downsample_1_weight = p_layer2_0_downsample_1_bias = b_layer2_0_downsample_1_running_mean = b_layer2_0_downsample_1_running_var = None\n",
       "                    getitem_42: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_14[0];  _native_batch_norm_legit_no_training_14 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_408: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(getitem_39, getitem_42);  getitem_39 = getitem_42 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_12: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(add_408);  add_408 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_15: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_12, p_layer2_1_conv1_weight);  p_layer2_1_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_15 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_15, p_layer2_1_bn1_weight, p_layer2_1_bn1_bias, b_layer2_1_bn1_running_mean, b_layer2_1_bn1_running_var, 0.1, 1e-05);  conv2d_15 = p_layer2_1_bn1_weight = p_layer2_1_bn1_bias = b_layer2_1_bn1_running_mean = b_layer2_1_bn1_running_var = None\n",
       "                    getitem_45: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_15[0];  _native_batch_norm_legit_no_training_15 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_13: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(getitem_45);  getitem_45 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_16: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_13, p_layer2_1_conv2_weight, None, [1, 1], [1, 1]);  relu_13 = p_layer2_1_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_16 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_16, p_layer2_1_bn2_weight, p_layer2_1_bn2_bias, b_layer2_1_bn2_running_mean, b_layer2_1_bn2_running_var, 0.1, 1e-05);  conv2d_16 = p_layer2_1_bn2_weight = p_layer2_1_bn2_bias = b_layer2_1_bn2_running_mean = b_layer2_1_bn2_running_var = None\n",
       "                    getitem_48: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_16[0];  _native_batch_norm_legit_no_training_16 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_14: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(getitem_48);  getitem_48 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_17: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_14, p_layer2_1_conv3_weight);  relu_14 = p_layer2_1_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_17 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_17, p_layer2_1_bn3_weight, p_layer2_1_bn3_bias, b_layer2_1_bn3_running_mean, b_layer2_1_bn3_running_var, 0.1, 1e-05);  conv2d_17 = p_layer2_1_bn3_weight = p_layer2_1_bn3_bias = b_layer2_1_bn3_running_mean = b_layer2_1_bn3_running_var = None\n",
       "                    getitem_51: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_17[0];  _native_batch_norm_legit_no_training_17 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_469: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(getitem_51, relu_12);  getitem_51 = relu_12 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_15: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(add_469);  add_469 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_18: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_15, p_layer2_2_conv1_weight);  p_layer2_2_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_18 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_18, p_layer2_2_bn1_weight, p_layer2_2_bn1_bias, b_layer2_2_bn1_running_mean, b_layer2_2_bn1_running_var, 0.1, 1e-05);  conv2d_18 = p_layer2_2_bn1_weight = p_layer2_2_bn1_bias = b_layer2_2_bn1_running_mean = b_layer2_2_bn1_running_var = None\n",
       "                    getitem_54: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_18[0];  _native_batch_norm_legit_no_training_18 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_16: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(getitem_54);  getitem_54 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_19: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_16, p_layer2_2_conv2_weight, None, [1, 1], [1, 1]);  relu_16 = p_layer2_2_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_19 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_19, p_layer2_2_bn2_weight, p_layer2_2_bn2_bias, b_layer2_2_bn2_running_mean, b_layer2_2_bn2_running_var, 0.1, 1e-05);  conv2d_19 = p_layer2_2_bn2_weight = p_layer2_2_bn2_bias = b_layer2_2_bn2_running_mean = b_layer2_2_bn2_running_var = None\n",
       "                    getitem_57: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_19[0];  _native_batch_norm_legit_no_training_19 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_17: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(getitem_57);  getitem_57 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_20: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_17, p_layer2_2_conv3_weight);  relu_17 = p_layer2_2_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_20 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_20, p_layer2_2_bn3_weight, p_layer2_2_bn3_bias, b_layer2_2_bn3_running_mean, b_layer2_2_bn3_running_var, 0.1, 1e-05);  conv2d_20 = p_layer2_2_bn3_weight = p_layer2_2_bn3_bias = b_layer2_2_bn3_running_mean = b_layer2_2_bn3_running_var = None\n",
       "                    getitem_60: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_20[0];  _native_batch_norm_legit_no_training_20 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_530: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(getitem_60, relu_15);  getitem_60 = relu_15 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_18: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(add_530);  add_530 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_21: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_18, p_layer2_3_conv1_weight);  p_layer2_3_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_21 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_21, p_layer2_3_bn1_weight, p_layer2_3_bn1_bias, b_layer2_3_bn1_running_mean, b_layer2_3_bn1_running_var, 0.1, 1e-05);  conv2d_21 = p_layer2_3_bn1_weight = p_layer2_3_bn1_bias = b_layer2_3_bn1_running_mean = b_layer2_3_bn1_running_var = None\n",
       "                    getitem_63: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_21[0];  _native_batch_norm_legit_no_training_21 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_19: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(getitem_63);  getitem_63 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_22: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_19, p_layer2_3_conv2_weight, None, [1, 1], [1, 1]);  relu_19 = p_layer2_3_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_22 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_22, p_layer2_3_bn2_weight, p_layer2_3_bn2_bias, b_layer2_3_bn2_running_mean, b_layer2_3_bn2_running_var, 0.1, 1e-05);  conv2d_22 = p_layer2_3_bn2_weight = p_layer2_3_bn2_bias = b_layer2_3_bn2_running_mean = b_layer2_3_bn2_running_var = None\n",
       "                    getitem_66: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_22[0];  _native_batch_norm_legit_no_training_22 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_20: \"f32[1, 64, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(getitem_66);  getitem_66 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_23: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_20, p_layer2_3_conv3_weight);  relu_20 = p_layer2_3_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_23 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_23, p_layer2_3_bn3_weight, p_layer2_3_bn3_bias, b_layer2_3_bn3_running_mean, b_layer2_3_bn3_running_var, 0.1, 1e-05);  conv2d_23 = p_layer2_3_bn3_weight = p_layer2_3_bn3_bias = b_layer2_3_bn3_running_mean = b_layer2_3_bn3_running_var = None\n",
       "                    getitem_69: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_23[0];  _native_batch_norm_legit_no_training_23 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_591: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(getitem_69, relu_18);  getitem_69 = relu_18 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_21: \"f32[1, 256, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(add_591);  add_591 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_24: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_21, p_layer3_0_conv1_weight);  p_layer3_0_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_24 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_24, p_layer3_0_bn1_weight, p_layer3_0_bn1_bias, b_layer3_0_bn1_running_mean, b_layer3_0_bn1_running_var, 0.1, 1e-05);  conv2d_24 = p_layer3_0_bn1_weight = p_layer3_0_bn1_bias = b_layer3_0_bn1_running_mean = b_layer3_0_bn1_running_var = None\n",
       "                    getitem_72: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = _native_batch_norm_legit_no_training_24[0];  _native_batch_norm_legit_no_training_24 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_22: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(getitem_72);  getitem_72 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_25: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_22, p_layer3_0_conv2_weight, None, [2, 2], [1, 1]);  relu_22 = p_layer3_0_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_25 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_25, p_layer3_0_bn2_weight, p_layer3_0_bn2_bias, b_layer3_0_bn2_running_mean, b_layer3_0_bn2_running_var, 0.1, 1e-05);  conv2d_25 = p_layer3_0_bn2_weight = p_layer3_0_bn2_bias = b_layer3_0_bn2_running_mean = b_layer3_0_bn2_running_var = None\n",
       "                    getitem_75: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = _native_batch_norm_legit_no_training_25[0];  _native_batch_norm_legit_no_training_25 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_23: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(getitem_75);  getitem_75 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_26: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_23, p_layer3_0_conv3_weight);  relu_23 = p_layer3_0_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_26 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_26, p_layer3_0_bn3_weight, p_layer3_0_bn3_bias, b_layer3_0_bn3_running_mean, b_layer3_0_bn3_running_var, 0.1, 1e-05);  conv2d_26 = p_layer3_0_bn3_weight = p_layer3_0_bn3_bias = b_layer3_0_bn3_running_mean = b_layer3_0_bn3_running_var = None\n",
       "                    getitem_78: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = _native_batch_norm_legit_no_training_26[0];  _native_batch_norm_legit_no_training_26 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_27: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_21, p_layer3_0_downsample_0_weight, None, [2, 2]);  p_layer3_0_downsample_0_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_27 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_27, p_layer3_0_downsample_1_weight, p_layer3_0_downsample_1_bias, b_layer3_0_downsample_1_running_mean, b_layer3_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_27 = p_layer3_0_downsample_1_weight = p_layer3_0_downsample_1_bias = b_layer3_0_downsample_1_running_mean = b_layer3_0_downsample_1_running_var = None\n",
       "                    getitem_81: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = _native_batch_norm_legit_no_training_27[0];  _native_batch_norm_legit_no_training_27 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_660: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(getitem_78, getitem_81);  getitem_78 = getitem_81 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_24: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(add_660);  add_660 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_28: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_24, p_layer3_1_conv1_weight);  p_layer3_1_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_28 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_28, p_layer3_1_bn1_weight, p_layer3_1_bn1_bias, b_layer3_1_bn1_running_mean, b_layer3_1_bn1_running_var, 0.1, 1e-05);  conv2d_28 = p_layer3_1_bn1_weight = p_layer3_1_bn1_bias = b_layer3_1_bn1_running_mean = b_layer3_1_bn1_running_var = None\n",
       "                    getitem_84: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = _native_batch_norm_legit_no_training_28[0];  _native_batch_norm_legit_no_training_28 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_25: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(getitem_84);  getitem_84 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_29: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_25, p_layer3_1_conv2_weight, None, [1, 1], [1, 1]);  relu_25 = p_layer3_1_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_29 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_29, p_layer3_1_bn2_weight, p_layer3_1_bn2_bias, b_layer3_1_bn2_running_mean, b_layer3_1_bn2_running_var, 0.1, 1e-05);  conv2d_29 = p_layer3_1_bn2_weight = p_layer3_1_bn2_bias = b_layer3_1_bn2_running_mean = b_layer3_1_bn2_running_var = None\n",
       "                    getitem_87: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = _native_batch_norm_legit_no_training_29[0];  _native_batch_norm_legit_no_training_29 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_26: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(getitem_87);  getitem_87 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_30: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_26, p_layer3_1_conv3_weight);  relu_26 = p_layer3_1_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_30 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_30, p_layer3_1_bn3_weight, p_layer3_1_bn3_bias, b_layer3_1_bn3_running_mean, b_layer3_1_bn3_running_var, 0.1, 1e-05);  conv2d_30 = p_layer3_1_bn3_weight = p_layer3_1_bn3_bias = b_layer3_1_bn3_running_mean = b_layer3_1_bn3_running_var = None\n",
       "                    getitem_90: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = _native_batch_norm_legit_no_training_30[0];  _native_batch_norm_legit_no_training_30 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_721: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(getitem_90, relu_24);  getitem_90 = relu_24 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_27: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(add_721);  add_721 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_31: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_27, p_layer3_2_conv1_weight);  p_layer3_2_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_31 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_31, p_layer3_2_bn1_weight, p_layer3_2_bn1_bias, b_layer3_2_bn1_running_mean, b_layer3_2_bn1_running_var, 0.1, 1e-05);  conv2d_31 = p_layer3_2_bn1_weight = p_layer3_2_bn1_bias = b_layer3_2_bn1_running_mean = b_layer3_2_bn1_running_var = None\n",
       "                    getitem_93: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = _native_batch_norm_legit_no_training_31[0];  _native_batch_norm_legit_no_training_31 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_28: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(getitem_93);  getitem_93 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_32: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_28, p_layer3_2_conv2_weight, None, [1, 1], [1, 1]);  relu_28 = p_layer3_2_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_32 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_32, p_layer3_2_bn2_weight, p_layer3_2_bn2_bias, b_layer3_2_bn2_running_mean, b_layer3_2_bn2_running_var, 0.1, 1e-05);  conv2d_32 = p_layer3_2_bn2_weight = p_layer3_2_bn2_bias = b_layer3_2_bn2_running_mean = b_layer3_2_bn2_running_var = None\n",
       "                    getitem_96: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = _native_batch_norm_legit_no_training_32[0];  _native_batch_norm_legit_no_training_32 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_29: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(getitem_96);  getitem_96 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_33: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_29, p_layer3_2_conv3_weight);  relu_29 = p_layer3_2_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_33 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_33, p_layer3_2_bn3_weight, p_layer3_2_bn3_bias, b_layer3_2_bn3_running_mean, b_layer3_2_bn3_running_var, 0.1, 1e-05);  conv2d_33 = p_layer3_2_bn3_weight = p_layer3_2_bn3_bias = b_layer3_2_bn3_running_mean = b_layer3_2_bn3_running_var = None\n",
       "                    getitem_99: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = _native_batch_norm_legit_no_training_33[0];  _native_batch_norm_legit_no_training_33 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_782: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(getitem_99, relu_27);  getitem_99 = relu_27 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_30: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(add_782);  add_782 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_34: \"f32[1, 256, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_30, p_layer4_0_conv1_weight);  p_layer4_0_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_34 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_34, p_layer4_0_bn1_weight, p_layer4_0_bn1_bias, b_layer4_0_bn1_running_mean, b_layer4_0_bn1_running_var, 0.1, 1e-05);  conv2d_34 = p_layer4_0_bn1_weight = p_layer4_0_bn1_bias = b_layer4_0_bn1_running_mean = b_layer4_0_bn1_running_var = None\n",
       "                    getitem_102: \"f32[1, 256, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = _native_batch_norm_legit_no_training_34[0];  _native_batch_norm_legit_no_training_34 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_31: \"f32[1, 256, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(getitem_102);  getitem_102 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_35: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_31, p_layer4_0_conv2_weight, None, [2, 2], [1, 1]);  relu_31 = p_layer4_0_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_35 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_35, p_layer4_0_bn2_weight, p_layer4_0_bn2_bias, b_layer4_0_bn2_running_mean, b_layer4_0_bn2_running_var, 0.1, 1e-05);  conv2d_35 = p_layer4_0_bn2_weight = p_layer4_0_bn2_bias = b_layer4_0_bn2_running_mean = b_layer4_0_bn2_running_var = None\n",
       "                    getitem_105: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = _native_batch_norm_legit_no_training_35[0];  _native_batch_norm_legit_no_training_35 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_32: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(getitem_105);  getitem_105 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_36: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_32, p_layer4_0_conv3_weight);  relu_32 = p_layer4_0_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_36 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_36, p_layer4_0_bn3_weight, p_layer4_0_bn3_bias, b_layer4_0_bn3_running_mean, b_layer4_0_bn3_running_var, 0.1, 1e-05);  conv2d_36 = p_layer4_0_bn3_weight = p_layer4_0_bn3_bias = b_layer4_0_bn3_running_mean = b_layer4_0_bn3_running_var = None\n",
       "                    getitem_108: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = _native_batch_norm_legit_no_training_36[0];  _native_batch_norm_legit_no_training_36 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_37: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_30, p_layer4_0_downsample_0_weight, None, [2, 2]);  p_layer4_0_downsample_0_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_37 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_37, p_layer4_0_downsample_1_weight, p_layer4_0_downsample_1_bias, b_layer4_0_downsample_1_running_mean, b_layer4_0_downsample_1_running_var, 0.1, 1e-05);  conv2d_37 = p_layer4_0_downsample_1_weight = p_layer4_0_downsample_1_bias = b_layer4_0_downsample_1_running_mean = b_layer4_0_downsample_1_running_var = None\n",
       "                    getitem_111: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = _native_batch_norm_legit_no_training_37[0];  _native_batch_norm_legit_no_training_37 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_851: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(getitem_108, getitem_111);  getitem_108 = getitem_111 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_33: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(add_851);  add_851 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_38: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_33, p_layer4_1_conv1_weight);  p_layer4_1_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_38 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_38, p_layer4_1_bn1_weight, p_layer4_1_bn1_bias, b_layer4_1_bn1_running_mean, b_layer4_1_bn1_running_var, 0.1, 1e-05);  conv2d_38 = p_layer4_1_bn1_weight = p_layer4_1_bn1_bias = b_layer4_1_bn1_running_mean = b_layer4_1_bn1_running_var = None\n",
       "                    getitem_114: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = _native_batch_norm_legit_no_training_38[0];  _native_batch_norm_legit_no_training_38 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_34: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(getitem_114);  getitem_114 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_39: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_34, p_layer4_1_conv2_weight, None, [1, 1], [1, 1]);  relu_34 = p_layer4_1_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_39 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_39, p_layer4_1_bn2_weight, p_layer4_1_bn2_bias, b_layer4_1_bn2_running_mean, b_layer4_1_bn2_running_var, 0.1, 1e-05);  conv2d_39 = p_layer4_1_bn2_weight = p_layer4_1_bn2_bias = b_layer4_1_bn2_running_mean = b_layer4_1_bn2_running_var = None\n",
       "                    getitem_117: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = _native_batch_norm_legit_no_training_39[0];  _native_batch_norm_legit_no_training_39 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_35: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(getitem_117);  getitem_117 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_40: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_35, p_layer4_1_conv3_weight);  relu_35 = p_layer4_1_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_40 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_40, p_layer4_1_bn3_weight, p_layer4_1_bn3_bias, b_layer4_1_bn3_running_mean, b_layer4_1_bn3_running_var, 0.1, 1e-05);  conv2d_40 = p_layer4_1_bn3_weight = p_layer4_1_bn3_bias = b_layer4_1_bn3_running_mean = b_layer4_1_bn3_running_var = None\n",
       "                    getitem_120: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = _native_batch_norm_legit_no_training_40[0];  _native_batch_norm_legit_no_training_40 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_912: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(getitem_120, relu_33);  getitem_120 = relu_33 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_36: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(add_912);  add_912 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_41: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_36, p_layer4_2_conv1_weight);  p_layer4_2_conv1_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_41 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_41, p_layer4_2_bn1_weight, p_layer4_2_bn1_bias, b_layer4_2_bn1_running_mean, b_layer4_2_bn1_running_var, 0.1, 1e-05);  conv2d_41 = p_layer4_2_bn1_weight = p_layer4_2_bn1_bias = b_layer4_2_bn1_running_mean = b_layer4_2_bn1_running_var = None\n",
       "                    getitem_123: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = _native_batch_norm_legit_no_training_41[0];  _native_batch_norm_legit_no_training_41 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_37: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(getitem_123);  getitem_123 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_42: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_37, p_layer4_2_conv2_weight, None, [1, 1], [1, 1]);  relu_37 = p_layer4_2_conv2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_42 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_42, p_layer4_2_bn2_weight, p_layer4_2_bn2_bias, b_layer4_2_bn2_running_mean, b_layer4_2_bn2_running_var, 0.1, 1e-05);  conv2d_42 = p_layer4_2_bn2_weight = p_layer4_2_bn2_bias = b_layer4_2_bn2_running_mean = b_layer4_2_bn2_running_var = None\n",
       "                    getitem_126: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = _native_batch_norm_legit_no_training_42[0];  _native_batch_norm_legit_no_training_42 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_38: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(getitem_126);  getitem_126 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_43: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_38, p_layer4_2_conv3_weight);  relu_38 = p_layer4_2_conv3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_43 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_43, p_layer4_2_bn3_weight, p_layer4_2_bn3_bias, b_layer4_2_bn3_running_mean, b_layer4_2_bn3_running_var, 0.1, 1e-05);  conv2d_43 = p_layer4_2_bn3_weight = p_layer4_2_bn3_bias = b_layer4_2_bn3_running_mean = b_layer4_2_bn3_running_var = None\n",
       "                    getitem_129: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = _native_batch_norm_legit_no_training_43[0];  _native_batch_norm_legit_no_training_43 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:90 in forward, code: out += residual\n",
       "                    add_973: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(getitem_129, relu_36);  getitem_129 = relu_36 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_39: \"f32[1, 1024, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(add_973);  add_973 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_44: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_39, p_p_ims1d2_outl1_dimred_weight, None, [1, 1], [1, 1]);  relu_39 = p_p_ims1d2_outl1_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_40: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(conv2d_44)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_45: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_40, p_adapt_stage1_b_0_1_conv_weight, p_adapt_stage1_b_0_1_conv_bias, [1, 1], [1, 1]);  relu_40 = p_adapt_stage1_b_0_1_conv_weight = p_adapt_stage1_b_0_1_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_41: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(conv2d_45);  conv2d_45 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_46: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_41, p_adapt_stage1_b_0_1_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_41 = p_adapt_stage1_b_0_1_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1014: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_46, conv2d_44);  conv2d_46 = conv2d_44 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_42: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(add_1014)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_47: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_42, p_adapt_stage1_b_0_2_conv_weight, p_adapt_stage1_b_0_2_conv_bias, [1, 1], [1, 1]);  relu_42 = p_adapt_stage1_b_0_2_conv_weight = p_adapt_stage1_b_0_2_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_43: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(conv2d_47);  conv2d_47 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_48: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_43, p_adapt_stage1_b_0_2_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_43 = p_adapt_stage1_b_0_2_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1043: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_48, add_1014);  conv2d_48 = add_1014 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_44: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(add_1043);  add_1043 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.max_pool2d.default(relu_44, [5, 5], [1, 1], [2, 2])\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_49: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d, p_mflow_conv_g1_pool_0_1_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d = p_mflow_conv_g1_pool_0_1_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1064: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_49, relu_44);  relu_44 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_1: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.max_pool2d.default(conv2d_49, [5, 5], [1, 1], [2, 2]);  conv2d_49 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_50: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_1, p_mflow_conv_g1_pool_0_2_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_1 = p_mflow_conv_g1_pool_0_2_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1077: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_50, add_1064);  add_1064 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_2: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.max_pool2d.default(conv2d_50, [5, 5], [1, 1], [2, 2]);  conv2d_50 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_51: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_2, p_mflow_conv_g1_pool_0_3_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_2 = p_mflow_conv_g1_pool_0_3_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1090: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_51, add_1077);  add_1077 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_3: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.max_pool2d.default(conv2d_51, [5, 5], [1, 1], [2, 2]);  conv2d_51 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_52: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_3, p_mflow_conv_g1_pool_0_4_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_3 = p_mflow_conv_g1_pool_0_4_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1103: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_52, add_1090);  conv2d_52 = add_1090 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_45: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(add_1103)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_53: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_45, p_mflow_conv_g1_b_0_1_conv_weight, p_mflow_conv_g1_b_0_1_conv_bias, [1, 1], [1, 1]);  relu_45 = p_mflow_conv_g1_b_0_1_conv_weight = p_mflow_conv_g1_b_0_1_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_46: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(conv2d_53);  conv2d_53 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_54: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_46, p_mflow_conv_g1_b_0_1_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_46 = p_mflow_conv_g1_b_0_1_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1132: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_54, add_1103);  conv2d_54 = add_1103 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_47: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(add_1132)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_55: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_47, p_mflow_conv_g1_b_0_2_conv_weight, p_mflow_conv_g1_b_0_2_conv_bias, [1, 1], [1, 1]);  relu_47 = p_mflow_conv_g1_b_0_2_conv_weight = p_mflow_conv_g1_b_0_2_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_48: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(conv2d_55);  conv2d_55 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_56: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_48, p_mflow_conv_g1_b_0_2_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_48 = p_mflow_conv_g1_b_0_2_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1161: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_56, add_1132);  conv2d_56 = add_1132 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_49: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(add_1161)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_57: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_49, p_mflow_conv_g1_b_0_3_conv_weight, p_mflow_conv_g1_b_0_3_conv_bias, [1, 1], [1, 1]);  relu_49 = p_mflow_conv_g1_b_0_3_conv_weight = p_mflow_conv_g1_b_0_3_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_50: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.relu.default(conv2d_57);  conv2d_57 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_58: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(relu_50, p_mflow_conv_g1_b_0_3_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_50 = p_mflow_conv_g1_b_0_3_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1190: \"f32[1, 256, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_58, add_1161);  conv2d_58 = add_1161 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_59: \"f32[1, 128, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(add_1190, p_mflow_conv_g1_b3_joint_varout_dimred_weight, None, [1, 1], [1, 1]);  add_1190 = p_mflow_conv_g1_b3_joint_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_60: \"f32[1, 512, (((s53 - 3)//8)) + 1, (((s0 - 3)//8)) + 1]\" = torch.ops.aten.conv2d.default(conv2d_59, p_up_ps4_weight, None, [1, 1], [1, 1]);  conv2d_59 = p_up_ps4_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pixelshuffle.py:62 in forward, code: return F.pixel_shuffle(input, self.upscale_factor)\n",
       "                    pixel_shuffle: \"f32[1, 128, 2*(((s53 - 3)//8)) + 2, 2*(((s0 - 3)//8)) + 2]\" = torch.ops.aten.pixel_shuffle.default(conv2d_60, 2);  conv2d_60 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_61: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_30, p_p_ims1d2_outl2_dimred_weight, None, [1, 1], [1, 1]);  relu_30 = p_p_ims1d2_outl2_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_51: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(conv2d_61)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_62: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_51, p_adapt_stage2_b_0_1_conv_weight, p_adapt_stage2_b_0_1_conv_bias, [1, 1], [1, 1]);  relu_51 = p_adapt_stage2_b_0_1_conv_weight = p_adapt_stage2_b_0_1_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_52: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(conv2d_62);  conv2d_62 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_63: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_52, p_adapt_stage2_b_0_1_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_52 = p_adapt_stage2_b_0_1_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1235: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_63, conv2d_61);  conv2d_63 = conv2d_61 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_53: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(add_1235)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_64: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_53, p_adapt_stage2_b_0_2_conv_weight, p_adapt_stage2_b_0_2_conv_bias, [1, 1], [1, 1]);  relu_53 = p_adapt_stage2_b_0_2_conv_weight = p_adapt_stage2_b_0_2_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_54: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(conv2d_64);  conv2d_64 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_65: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_54, p_adapt_stage2_b_0_2_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_54 = p_adapt_stage2_b_0_2_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1264: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_65, add_1235);  conv2d_65 = add_1235 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_66: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(add_1264, p_adapt_stage2_b2_joint_varout_dimred_weight, None, [1, 1], [1, 1]);  add_1264 = p_adapt_stage2_b2_joint_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:222 in forward, code: x3 = x3 + x4\n",
       "                    add_1273: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_66, pixel_shuffle);  conv2d_66 = pixel_shuffle = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:223 in forward, code: x3 = F.relu(x3)\n",
       "                    relu_55: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(add_1273);  add_1273 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_4: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.max_pool2d.default(relu_55, [5, 5], [1, 1], [2, 2])\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_67: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_4, p_mflow_conv_g2_pool_0_1_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_4 = p_mflow_conv_g2_pool_0_1_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1290: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_67, relu_55);  relu_55 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_5: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.max_pool2d.default(conv2d_67, [5, 5], [1, 1], [2, 2]);  conv2d_67 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_68: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_5, p_mflow_conv_g2_pool_0_2_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_5 = p_mflow_conv_g2_pool_0_2_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1303: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_68, add_1290);  add_1290 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_6: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.max_pool2d.default(conv2d_68, [5, 5], [1, 1], [2, 2]);  conv2d_68 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_69: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_6, p_mflow_conv_g2_pool_0_3_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_6 = p_mflow_conv_g2_pool_0_3_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1316: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_69, add_1303);  add_1303 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_7: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.max_pool2d.default(conv2d_69, [5, 5], [1, 1], [2, 2]);  conv2d_69 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_70: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_7, p_mflow_conv_g2_pool_0_4_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_7 = p_mflow_conv_g2_pool_0_4_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1329: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_70, add_1316);  conv2d_70 = add_1316 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_56: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(add_1329)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_71: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_56, p_mflow_conv_g2_b_0_1_conv_weight, p_mflow_conv_g2_b_0_1_conv_bias, [1, 1], [1, 1]);  relu_56 = p_mflow_conv_g2_b_0_1_conv_weight = p_mflow_conv_g2_b_0_1_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_57: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(conv2d_71);  conv2d_71 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_72: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_57, p_mflow_conv_g2_b_0_1_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_57 = p_mflow_conv_g2_b_0_1_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1358: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_72, add_1329);  conv2d_72 = add_1329 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_58: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(add_1358)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_73: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_58, p_mflow_conv_g2_b_0_2_conv_weight, p_mflow_conv_g2_b_0_2_conv_bias, [1, 1], [1, 1]);  relu_58 = p_mflow_conv_g2_b_0_2_conv_weight = p_mflow_conv_g2_b_0_2_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_59: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(conv2d_73);  conv2d_73 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_74: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_59, p_mflow_conv_g2_b_0_2_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_59 = p_mflow_conv_g2_b_0_2_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1387: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_74, add_1358);  conv2d_74 = add_1358 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_60: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(add_1387)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_75: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_60, p_mflow_conv_g2_b_0_3_conv_weight, p_mflow_conv_g2_b_0_3_conv_bias, [1, 1], [1, 1]);  relu_60 = p_mflow_conv_g2_b_0_3_conv_weight = p_mflow_conv_g2_b_0_3_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_61: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.relu.default(conv2d_75);  conv2d_75 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_76: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(relu_61, p_mflow_conv_g2_b_0_3_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_61 = p_mflow_conv_g2_b_0_3_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1416: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_76, add_1387);  conv2d_76 = add_1387 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_77: \"f32[1, 128, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(add_1416, p_mflow_conv_g2_b3_joint_varout_dimred_weight, None, [1, 1], [1, 1]);  add_1416 = p_mflow_conv_g2_b3_joint_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_78: \"f32[1, 512, (((s53 - 3)//4)) + 1, (((s0 - 3)//4)) + 1]\" = torch.ops.aten.conv2d.default(conv2d_77, p_up_ps3_weight, None, [1, 1], [1, 1]);  conv2d_77 = p_up_ps3_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pixelshuffle.py:62 in forward, code: return F.pixel_shuffle(input, self.upscale_factor)\n",
       "                    pixel_shuffle_1: \"f32[1, 128, 2*(((s53 - 3)//4)) + 2, 2*(((s0 - 3)//4)) + 2]\" = torch.ops.aten.pixel_shuffle.default(conv2d_78, 2);  conv2d_78 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:232 in forward, code: x3 = self._crop_like(x3, l2) # ConvTranspose, PixelShuffle\n",
       "                    add_1433: \"Sym(s53 - 3)\" = -3 + sym_size_int_274;  sym_size_int_274 = None\n",
       "                    floordiv_69: \"Sym(((s53 - 3)//4))\" = add_1433 // 4\n",
       "                    mul_1404: \"Sym(2*(((s53 - 3)//4)))\" = 2 * floordiv_69;  floordiv_69 = None\n",
       "                    add_1434: \"Sym(2*(((s53 - 3)//4)) + 2)\" = 2 + mul_1404;  mul_1404 = None\n",
       "                    floordiv_70: \"Sym(((s53 - 3)//2))\" = add_1433 // 2;  add_1433 = None\n",
       "                    add_1435: \"Sym((((s53 - 3)//2)) + 1)\" = 1 + floordiv_70;  floordiv_70 = None\n",
       "                    sub_629: \"Sym(-(((s53 - 3)//2)) + 2*(((s53 - 3)//4)) + 1)\" = add_1434 - add_1435;  add_1435 = None\n",
       "                    add_1436: \"Sym(s0 - 3)\" = -3 + sym_size_int_275;  sym_size_int_275 = None\n",
       "                    floordiv_71: \"Sym(((s0 - 3)//4))\" = add_1436 // 4\n",
       "                    mul_1405: \"Sym(2*(((s0 - 3)//4)))\" = 2 * floordiv_71;  floordiv_71 = None\n",
       "                    add_1437: \"Sym(2*(((s0 - 3)//4)) + 2)\" = 2 + mul_1405;  mul_1405 = None\n",
       "                    floordiv_72: \"Sym(((s0 - 3)//2))\" = add_1436 // 2;  add_1436 = None\n",
       "                    add_1438: \"Sym((((s0 - 3)//2)) + 1)\" = 1 + floordiv_72;  floordiv_72 = None\n",
       "                    sub_630: \"Sym(-(((s0 - 3)//2)) + 2*(((s0 - 3)//4)) + 1)\" = add_1437 - add_1438;  add_1438 = None\n",
       "                    floordiv_73: \"Sym((((1 - (((s53 - 3)//2)))//2)) + (((s53 - 3)//4)))\" = sub_629 // 2\n",
       "                    sub_631: \"Sym(-(((1 - (((s53 - 3)//2)))//2)) - (((s53 - 3)//2)) + (((s53 - 3)//4)) + 1)\" = sub_629 - floordiv_73;  sub_629 = None\n",
       "                    sub_632: \"Sym((((1 - (((s53 - 3)//2)))//2)) + (((s53 - 3)//2)) + (((s53 - 3)//4)) + 1)\" = add_1434 - sub_631;  add_1434 = sub_631 = None\n",
       "                    floordiv_74: \"Sym((((1 - (((s0 - 3)//2)))//2)) + (((s0 - 3)//4)))\" = sub_630 // 2\n",
       "                    sub_633: \"Sym(-(((1 - (((s0 - 3)//2)))//2)) - (((s0 - 3)//2)) + (((s0 - 3)//4)) + 1)\" = sub_630 - floordiv_74;  sub_630 = None\n",
       "                    sub_634: \"Sym((((1 - (((s0 - 3)//2)))//2)) + (((s0 - 3)//2)) + (((s0 - 3)//4)) + 1)\" = add_1437 - sub_633;  add_1437 = sub_633 = None\n",
       "                    slice_1: \"f32[1, 128, (((s53 - 3)//2)) + 1, 2*(((s0 - 3)//4)) + 2]\" = torch.ops.aten.slice.Tensor(pixel_shuffle_1, 2, floordiv_73, sub_632);  pixel_shuffle_1 = floordiv_73 = sub_632 = None\n",
       "                    slice_2: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.slice.Tensor(slice_1, 3, floordiv_74, sub_634);  slice_1 = floordiv_74 = sub_634 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_79: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_21, p_p_ims1d2_outl3_dimred_weight, None, [1, 1], [1, 1]);  relu_21 = p_p_ims1d2_outl3_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_62: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(conv2d_79)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_80: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_62, p_adapt_stage3_b_0_1_conv_weight, p_adapt_stage3_b_0_1_conv_bias, [1, 1], [1, 1]);  relu_62 = p_adapt_stage3_b_0_1_conv_weight = p_adapt_stage3_b_0_1_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_63: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(conv2d_80);  conv2d_80 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_81: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_63, p_adapt_stage3_b_0_1_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_63 = p_adapt_stage3_b_0_1_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1475: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_81, conv2d_79);  conv2d_81 = conv2d_79 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_64: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(add_1475)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_82: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_64, p_adapt_stage3_b_0_2_conv_weight, p_adapt_stage3_b_0_2_conv_bias, [1, 1], [1, 1]);  relu_64 = p_adapt_stage3_b_0_2_conv_weight = p_adapt_stage3_b_0_2_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_65: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(conv2d_82);  conv2d_82 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_83: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_65, p_adapt_stage3_b_0_2_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_65 = p_adapt_stage3_b_0_2_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1504: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_83, add_1475);  conv2d_83 = add_1475 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_84: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(add_1504, p_adapt_stage3_b2_joint_varout_dimred_weight, None, [1, 1], [1, 1]);  add_1504 = p_adapt_stage3_b2_joint_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:237 in forward, code: x2 = x2 + x3\n",
       "                    add_1513: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_84, slice_2);  conv2d_84 = slice_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:238 in forward, code: x2 = F.relu(x2)\n",
       "                    relu_66: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(add_1513);  add_1513 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_8: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.max_pool2d.default(relu_66, [5, 5], [1, 1], [2, 2])\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_85: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_8, p_mflow_conv_g3_pool_0_1_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_8 = p_mflow_conv_g3_pool_0_1_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1530: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_85, relu_66);  relu_66 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_9: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.max_pool2d.default(conv2d_85, [5, 5], [1, 1], [2, 2]);  conv2d_85 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_86: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_9, p_mflow_conv_g3_pool_0_2_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_9 = p_mflow_conv_g3_pool_0_2_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1543: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_86, add_1530);  add_1530 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_10: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.max_pool2d.default(conv2d_86, [5, 5], [1, 1], [2, 2]);  conv2d_86 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_87: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_10, p_mflow_conv_g3_pool_0_3_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_10 = p_mflow_conv_g3_pool_0_3_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1556: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_87, add_1543);  add_1543 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_11: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.max_pool2d.default(conv2d_87, [5, 5], [1, 1], [2, 2]);  conv2d_87 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_88: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(max_pool2d_11, p_mflow_conv_g3_pool_0_4_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_11 = p_mflow_conv_g3_pool_0_4_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1569: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_88, add_1556);  conv2d_88 = add_1556 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_67: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(add_1569)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_89: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_67, p_mflow_conv_g3_b_0_1_conv_weight, p_mflow_conv_g3_b_0_1_conv_bias, [1, 1], [1, 1]);  relu_67 = p_mflow_conv_g3_b_0_1_conv_weight = p_mflow_conv_g3_b_0_1_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_68: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(conv2d_89);  conv2d_89 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_90: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_68, p_mflow_conv_g3_b_0_1_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_68 = p_mflow_conv_g3_b_0_1_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1598: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_90, add_1569);  conv2d_90 = add_1569 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_69: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(add_1598)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_91: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_69, p_mflow_conv_g3_b_0_2_conv_weight, p_mflow_conv_g3_b_0_2_conv_bias, [1, 1], [1, 1]);  relu_69 = p_mflow_conv_g3_b_0_2_conv_weight = p_mflow_conv_g3_b_0_2_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_70: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(conv2d_91);  conv2d_91 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_92: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_70, p_mflow_conv_g3_b_0_2_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_70 = p_mflow_conv_g3_b_0_2_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1627: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_92, add_1598);  conv2d_92 = add_1598 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_71: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(add_1627)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_93: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_71, p_mflow_conv_g3_b_0_3_conv_weight, p_mflow_conv_g3_b_0_3_conv_bias, [1, 1], [1, 1]);  relu_71 = p_mflow_conv_g3_b_0_3_conv_weight = p_mflow_conv_g3_b_0_3_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_72: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.relu.default(conv2d_93);  conv2d_93 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_94: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(relu_72, p_mflow_conv_g3_b_0_3_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_72 = p_mflow_conv_g3_b_0_3_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1656: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.add.Tensor(conv2d_94, add_1627);  conv2d_94 = add_1627 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_95: \"f32[1, 128, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(add_1656, p_mflow_conv_g3_b3_joint_varout_dimred_weight, None, [1, 1], [1, 1]);  add_1656 = p_mflow_conv_g3_b3_joint_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_96: \"f32[1, 512, (((s53 - 3)//2)) + 1, (((s0 - 3)//2)) + 1]\" = torch.ops.aten.conv2d.default(conv2d_95, p_up_ps2_weight, None, [1, 1], [1, 1]);  conv2d_95 = p_up_ps2_weight = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pixelshuffle.py:62 in forward, code: return F.pixel_shuffle(input, self.upscale_factor)\n",
       "                    pixel_shuffle_2: \"f32[1, 128, 2*(((s53 - 3)//2)) + 2, 2*(((s0 - 3)//2)) + 2]\" = torch.ops.aten.pixel_shuffle.default(conv2d_96, 2);  conv2d_96 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_97: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_9, p_p_ims1d2_outl4_dimred_weight, None, [1, 1], [1, 1]);  relu_9 = p_p_ims1d2_outl4_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_73: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(conv2d_97)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_98: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_73, p_adapt_stage4_b_0_1_conv_weight, p_adapt_stage4_b_0_1_conv_bias, [1, 1], [1, 1]);  relu_73 = p_adapt_stage4_b_0_1_conv_weight = p_adapt_stage4_b_0_1_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_74: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(conv2d_98);  conv2d_98 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_99: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_74, p_adapt_stage4_b_0_1_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_74 = p_adapt_stage4_b_0_1_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1701: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(conv2d_99, conv2d_97);  conv2d_99 = conv2d_97 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_75: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(add_1701)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_100: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_75, p_adapt_stage4_b_0_2_conv_weight, p_adapt_stage4_b_0_2_conv_bias, [1, 1], [1, 1]);  relu_75 = p_adapt_stage4_b_0_2_conv_weight = p_adapt_stage4_b_0_2_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_76: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(conv2d_100);  conv2d_100 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_101: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_76, p_adapt_stage4_b_0_2_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_76 = p_adapt_stage4_b_0_2_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1730: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(conv2d_101, add_1701);  conv2d_101 = add_1701 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_102: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(add_1730, p_adapt_stage4_b2_joint_varout_dimred_weight, None, [1, 1], [1, 1]);  add_1730 = p_adapt_stage4_b2_joint_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:252 in forward, code: x1 = x1 + x2\n",
       "                    add_1739: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(conv2d_102, pixel_shuffle_2);  conv2d_102 = pixel_shuffle_2 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:253 in forward, code: x1 = F.relu(x1)\n",
       "                    relu_77: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(add_1739);  add_1739 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_12: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.max_pool2d.default(relu_77, [5, 5], [1, 1], [2, 2])\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_103: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(max_pool2d_12, p_mflow_conv_g4_pool_0_1_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_12 = p_mflow_conv_g4_pool_0_1_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1756: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(conv2d_103, relu_77);  relu_77 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_13: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.max_pool2d.default(conv2d_103, [5, 5], [1, 1], [2, 2]);  conv2d_103 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_104: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(max_pool2d_13, p_mflow_conv_g4_pool_0_2_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_13 = p_mflow_conv_g4_pool_0_2_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1769: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(conv2d_104, add_1756);  add_1756 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_14: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.max_pool2d.default(conv2d_104, [5, 5], [1, 1], [2, 2]);  conv2d_104 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_105: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(max_pool2d_14, p_mflow_conv_g4_pool_0_3_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_14 = p_mflow_conv_g4_pool_0_3_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1782: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(conv2d_105, add_1769);  add_1769 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_15: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.max_pool2d.default(conv2d_105, [5, 5], [1, 1], [2, 2]);  conv2d_105 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_106: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(max_pool2d_15, p_mflow_conv_g4_pool_0_4_outvar_dimred_weight, None, [1, 1], [1, 1]);  max_pool2d_15 = p_mflow_conv_g4_pool_0_4_outvar_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:29 in forward, code: x = top + x\n",
       "                    add_1795: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(conv2d_106, add_1782);  conv2d_106 = add_1782 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_78: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(add_1795)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_107: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_78, p_mflow_conv_g4_b_0_1_conv_weight, p_mflow_conv_g4_b_0_1_conv_bias, [1, 1], [1, 1]);  relu_78 = p_mflow_conv_g4_b_0_1_conv_weight = p_mflow_conv_g4_b_0_1_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_79: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(conv2d_107);  conv2d_107 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_108: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_79, p_mflow_conv_g4_b_0_1_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_79 = p_mflow_conv_g4_b_0_1_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1824: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(conv2d_108, add_1795);  conv2d_108 = add_1795 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_80: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(add_1824)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_109: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_80, p_mflow_conv_g4_b_0_2_conv_weight, p_mflow_conv_g4_b_0_2_conv_bias, [1, 1], [1, 1]);  relu_80 = p_mflow_conv_g4_b_0_2_conv_weight = p_mflow_conv_g4_b_0_2_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_81: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(conv2d_109);  conv2d_109 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_110: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_81, p_mflow_conv_g4_b_0_2_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_81 = p_mflow_conv_g4_b_0_2_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1853: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(conv2d_110, add_1824);  conv2d_110 = add_1824 = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_82: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(add_1853)\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_111: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_82, p_mflow_conv_g4_b_0_3_conv_weight, p_mflow_conv_g4_b_0_3_conv_bias, [1, 1], [1, 1]);  relu_82 = p_mflow_conv_g4_b_0_3_conv_weight = p_mflow_conv_g4_b_0_3_conv_bias = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:52 in forward, code: x = F.relu(x)\n",
       "                    relu_83: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.relu.default(conv2d_111);  conv2d_111 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_112: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(relu_83, p_mflow_conv_g4_b_0_3_conv_relu_varout_dimred_weight, None, [1, 1], [1, 1]);  relu_83 = p_mflow_conv_g4_b_0_3_conv_relu_varout_dimred_weight = None\n",
       "            \n",
       "                     # File: C:\\Users\\Вячеслав\\Documents\\superresolution\\modules\\ms_resunet.py:54 in forward, code: x += residual\n",
       "                    add_1882: \"f32[1, 128, s53 - 2, s0 - 2]\" = torch.ops.aten.add.Tensor(conv2d_112, add_1853);  conv2d_112 = add_1853 = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_113: \"f32[1, 64, s53 - 2, s0 - 2]\" = torch.ops.aten.conv2d.default(add_1882, p_clf_conv1_weight, p_clf_conv1_bias, [1, 1], [2, 2]);  add_1882 = p_clf_conv1_weight = p_clf_conv1_bias = None\n",
       "            \n",
       "                     # File: C:\\anaconda\\envs\\superresolution\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_114: \"f32[1, 1, s53, s0]\" = torch.ops.aten.conv2d.default(conv2d_113, p_clf_conv2_weight, p_clf_conv2_bias, [1, 1], [2, 2]);  conv2d_113 = p_clf_conv2_weight = p_clf_conv2_bias = None\n",
       "                    return (conv2d_114,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_conv1_weight: PARAMETER target='conv1.weight'\n",
       "            p_bn1_weight: PARAMETER target='bn1.weight'\n",
       "            p_bn1_bias: PARAMETER target='bn1.bias'\n",
       "            p_upct4_weight: PARAMETER target='upCT4.weight'\n",
       "            p_upct4_bias: PARAMETER target='upCT4.bias'\n",
       "            p_upct3_weight: PARAMETER target='upCT3.weight'\n",
       "            p_upct3_bias: PARAMETER target='upCT3.bias'\n",
       "            p_upct2_weight: PARAMETER target='upCT2.weight'\n",
       "            p_upct2_bias: PARAMETER target='upCT2.bias'\n",
       "            p_layer1_0_conv1_weight: PARAMETER target='layer1.0.conv1.weight'\n",
       "            p_layer1_0_bn1_weight: PARAMETER target='layer1.0.bn1.weight'\n",
       "            p_layer1_0_bn1_bias: PARAMETER target='layer1.0.bn1.bias'\n",
       "            p_layer1_0_conv2_weight: PARAMETER target='layer1.0.conv2.weight'\n",
       "            p_layer1_0_bn2_weight: PARAMETER target='layer1.0.bn2.weight'\n",
       "            p_layer1_0_bn2_bias: PARAMETER target='layer1.0.bn2.bias'\n",
       "            p_layer1_0_conv3_weight: PARAMETER target='layer1.0.conv3.weight'\n",
       "            p_layer1_0_bn3_weight: PARAMETER target='layer1.0.bn3.weight'\n",
       "            p_layer1_0_bn3_bias: PARAMETER target='layer1.0.bn3.bias'\n",
       "            p_layer1_0_downsample_0_weight: PARAMETER target='layer1.0.downsample.0.weight'\n",
       "            p_layer1_0_downsample_1_weight: PARAMETER target='layer1.0.downsample.1.weight'\n",
       "            p_layer1_0_downsample_1_bias: PARAMETER target='layer1.0.downsample.1.bias'\n",
       "            p_layer1_1_conv1_weight: PARAMETER target='layer1.1.conv1.weight'\n",
       "            p_layer1_1_bn1_weight: PARAMETER target='layer1.1.bn1.weight'\n",
       "            p_layer1_1_bn1_bias: PARAMETER target='layer1.1.bn1.bias'\n",
       "            p_layer1_1_conv2_weight: PARAMETER target='layer1.1.conv2.weight'\n",
       "            p_layer1_1_bn2_weight: PARAMETER target='layer1.1.bn2.weight'\n",
       "            p_layer1_1_bn2_bias: PARAMETER target='layer1.1.bn2.bias'\n",
       "            p_layer1_1_conv3_weight: PARAMETER target='layer1.1.conv3.weight'\n",
       "            p_layer1_1_bn3_weight: PARAMETER target='layer1.1.bn3.weight'\n",
       "            p_layer1_1_bn3_bias: PARAMETER target='layer1.1.bn3.bias'\n",
       "            p_layer1_2_conv1_weight: PARAMETER target='layer1.2.conv1.weight'\n",
       "            p_layer1_2_bn1_weight: PARAMETER target='layer1.2.bn1.weight'\n",
       "            p_layer1_2_bn1_bias: PARAMETER target='layer1.2.bn1.bias'\n",
       "            p_layer1_2_conv2_weight: PARAMETER target='layer1.2.conv2.weight'\n",
       "            p_layer1_2_bn2_weight: PARAMETER target='layer1.2.bn2.weight'\n",
       "            p_layer1_2_bn2_bias: PARAMETER target='layer1.2.bn2.bias'\n",
       "            p_layer1_2_conv3_weight: PARAMETER target='layer1.2.conv3.weight'\n",
       "            p_layer1_2_bn3_weight: PARAMETER target='layer1.2.bn3.weight'\n",
       "            p_layer1_2_bn3_bias: PARAMETER target='layer1.2.bn3.bias'\n",
       "            p_layer2_0_conv1_weight: PARAMETER target='layer2.0.conv1.weight'\n",
       "            p_layer2_0_bn1_weight: PARAMETER target='layer2.0.bn1.weight'\n",
       "            p_layer2_0_bn1_bias: PARAMETER target='layer2.0.bn1.bias'\n",
       "            p_layer2_0_conv2_weight: PARAMETER target='layer2.0.conv2.weight'\n",
       "            p_layer2_0_bn2_weight: PARAMETER target='layer2.0.bn2.weight'\n",
       "            p_layer2_0_bn2_bias: PARAMETER target='layer2.0.bn2.bias'\n",
       "            p_layer2_0_conv3_weight: PARAMETER target='layer2.0.conv3.weight'\n",
       "            p_layer2_0_bn3_weight: PARAMETER target='layer2.0.bn3.weight'\n",
       "            p_layer2_0_bn3_bias: PARAMETER target='layer2.0.bn3.bias'\n",
       "            p_layer2_0_downsample_0_weight: PARAMETER target='layer2.0.downsample.0.weight'\n",
       "            p_layer2_0_downsample_1_weight: PARAMETER target='layer2.0.downsample.1.weight'\n",
       "            p_layer2_0_downsample_1_bias: PARAMETER target='layer2.0.downsample.1.bias'\n",
       "            p_layer2_1_conv1_weight: PARAMETER target='layer2.1.conv1.weight'\n",
       "            p_layer2_1_bn1_weight: PARAMETER target='layer2.1.bn1.weight'\n",
       "            p_layer2_1_bn1_bias: PARAMETER target='layer2.1.bn1.bias'\n",
       "            p_layer2_1_conv2_weight: PARAMETER target='layer2.1.conv2.weight'\n",
       "            p_layer2_1_bn2_weight: PARAMETER target='layer2.1.bn2.weight'\n",
       "            p_layer2_1_bn2_bias: PARAMETER target='layer2.1.bn2.bias'\n",
       "            p_layer2_1_conv3_weight: PARAMETER target='layer2.1.conv3.weight'\n",
       "            p_layer2_1_bn3_weight: PARAMETER target='layer2.1.bn3.weight'\n",
       "            p_layer2_1_bn3_bias: PARAMETER target='layer2.1.bn3.bias'\n",
       "            p_layer2_2_conv1_weight: PARAMETER target='layer2.2.conv1.weight'\n",
       "            p_layer2_2_bn1_weight: PARAMETER target='layer2.2.bn1.weight'\n",
       "            p_layer2_2_bn1_bias: PARAMETER target='layer2.2.bn1.bias'\n",
       "            p_layer2_2_conv2_weight: PARAMETER target='layer2.2.conv2.weight'\n",
       "            p_layer2_2_bn2_weight: PARAMETER target='layer2.2.bn2.weight'\n",
       "            p_layer2_2_bn2_bias: PARAMETER target='layer2.2.bn2.bias'\n",
       "            p_layer2_2_conv3_weight: PARAMETER target='layer2.2.conv3.weight'\n",
       "            p_layer2_2_bn3_weight: PARAMETER target='layer2.2.bn3.weight'\n",
       "            p_layer2_2_bn3_bias: PARAMETER target='layer2.2.bn3.bias'\n",
       "            p_layer2_3_conv1_weight: PARAMETER target='layer2.3.conv1.weight'\n",
       "            p_layer2_3_bn1_weight: PARAMETER target='layer2.3.bn1.weight'\n",
       "            p_layer2_3_bn1_bias: PARAMETER target='layer2.3.bn1.bias'\n",
       "            p_layer2_3_conv2_weight: PARAMETER target='layer2.3.conv2.weight'\n",
       "            p_layer2_3_bn2_weight: PARAMETER target='layer2.3.bn2.weight'\n",
       "            p_layer2_3_bn2_bias: PARAMETER target='layer2.3.bn2.bias'\n",
       "            p_layer2_3_conv3_weight: PARAMETER target='layer2.3.conv3.weight'\n",
       "            p_layer2_3_bn3_weight: PARAMETER target='layer2.3.bn3.weight'\n",
       "            p_layer2_3_bn3_bias: PARAMETER target='layer2.3.bn3.bias'\n",
       "            p_layer3_0_conv1_weight: PARAMETER target='layer3.0.conv1.weight'\n",
       "            p_layer3_0_bn1_weight: PARAMETER target='layer3.0.bn1.weight'\n",
       "            p_layer3_0_bn1_bias: PARAMETER target='layer3.0.bn1.bias'\n",
       "            p_layer3_0_conv2_weight: PARAMETER target='layer3.0.conv2.weight'\n",
       "            p_layer3_0_bn2_weight: PARAMETER target='layer3.0.bn2.weight'\n",
       "            p_layer3_0_bn2_bias: PARAMETER target='layer3.0.bn2.bias'\n",
       "            p_layer3_0_conv3_weight: PARAMETER target='layer3.0.conv3.weight'\n",
       "            p_layer3_0_bn3_weight: PARAMETER target='layer3.0.bn3.weight'\n",
       "            p_layer3_0_bn3_bias: PARAMETER target='layer3.0.bn3.bias'\n",
       "            p_layer3_0_downsample_0_weight: PARAMETER target='layer3.0.downsample.0.weight'\n",
       "            p_layer3_0_downsample_1_weight: PARAMETER target='layer3.0.downsample.1.weight'\n",
       "            p_layer3_0_downsample_1_bias: PARAMETER target='layer3.0.downsample.1.bias'\n",
       "            p_layer3_1_conv1_weight: PARAMETER target='layer3.1.conv1.weight'\n",
       "            p_layer3_1_bn1_weight: PARAMETER target='layer3.1.bn1.weight'\n",
       "            p_layer3_1_bn1_bias: PARAMETER target='layer3.1.bn1.bias'\n",
       "            p_layer3_1_conv2_weight: PARAMETER target='layer3.1.conv2.weight'\n",
       "            p_layer3_1_bn2_weight: PARAMETER target='layer3.1.bn2.weight'\n",
       "            p_layer3_1_bn2_bias: PARAMETER target='layer3.1.bn2.bias'\n",
       "            p_layer3_1_conv3_weight: PARAMETER target='layer3.1.conv3.weight'\n",
       "            p_layer3_1_bn3_weight: PARAMETER target='layer3.1.bn3.weight'\n",
       "            p_layer3_1_bn3_bias: PARAMETER target='layer3.1.bn3.bias'\n",
       "            p_layer3_2_conv1_weight: PARAMETER target='layer3.2.conv1.weight'\n",
       "            p_layer3_2_bn1_weight: PARAMETER target='layer3.2.bn1.weight'\n",
       "            p_layer3_2_bn1_bias: PARAMETER target='layer3.2.bn1.bias'\n",
       "            p_layer3_2_conv2_weight: PARAMETER target='layer3.2.conv2.weight'\n",
       "            p_layer3_2_bn2_weight: PARAMETER target='layer3.2.bn2.weight'\n",
       "            p_layer3_2_bn2_bias: PARAMETER target='layer3.2.bn2.bias'\n",
       "            p_layer3_2_conv3_weight: PARAMETER target='layer3.2.conv3.weight'\n",
       "            p_layer3_2_bn3_weight: PARAMETER target='layer3.2.bn3.weight'\n",
       "            p_layer3_2_bn3_bias: PARAMETER target='layer3.2.bn3.bias'\n",
       "            p_layer4_0_conv1_weight: PARAMETER target='layer4.0.conv1.weight'\n",
       "            p_layer4_0_bn1_weight: PARAMETER target='layer4.0.bn1.weight'\n",
       "            p_layer4_0_bn1_bias: PARAMETER target='layer4.0.bn1.bias'\n",
       "            p_layer4_0_conv2_weight: PARAMETER target='layer4.0.conv2.weight'\n",
       "            p_layer4_0_bn2_weight: PARAMETER target='layer4.0.bn2.weight'\n",
       "            p_layer4_0_bn2_bias: PARAMETER target='layer4.0.bn2.bias'\n",
       "            p_layer4_0_conv3_weight: PARAMETER target='layer4.0.conv3.weight'\n",
       "            p_layer4_0_bn3_weight: PARAMETER target='layer4.0.bn3.weight'\n",
       "            p_layer4_0_bn3_bias: PARAMETER target='layer4.0.bn3.bias'\n",
       "            p_layer4_0_downsample_0_weight: PARAMETER target='layer4.0.downsample.0.weight'\n",
       "            p_layer4_0_downsample_1_weight: PARAMETER target='layer4.0.downsample.1.weight'\n",
       "            p_layer4_0_downsample_1_bias: PARAMETER target='layer4.0.downsample.1.bias'\n",
       "            p_layer4_1_conv1_weight: PARAMETER target='layer4.1.conv1.weight'\n",
       "            p_layer4_1_bn1_weight: PARAMETER target='layer4.1.bn1.weight'\n",
       "            p_layer4_1_bn1_bias: PARAMETER target='layer4.1.bn1.bias'\n",
       "            p_layer4_1_conv2_weight: PARAMETER target='layer4.1.conv2.weight'\n",
       "            p_layer4_1_bn2_weight: PARAMETER target='layer4.1.bn2.weight'\n",
       "            p_layer4_1_bn2_bias: PARAMETER target='layer4.1.bn2.bias'\n",
       "            p_layer4_1_conv3_weight: PARAMETER target='layer4.1.conv3.weight'\n",
       "            p_layer4_1_bn3_weight: PARAMETER target='layer4.1.bn3.weight'\n",
       "            p_layer4_1_bn3_bias: PARAMETER target='layer4.1.bn3.bias'\n",
       "            p_layer4_2_conv1_weight: PARAMETER target='layer4.2.conv1.weight'\n",
       "            p_layer4_2_bn1_weight: PARAMETER target='layer4.2.bn1.weight'\n",
       "            p_layer4_2_bn1_bias: PARAMETER target='layer4.2.bn1.bias'\n",
       "            p_layer4_2_conv2_weight: PARAMETER target='layer4.2.conv2.weight'\n",
       "            p_layer4_2_bn2_weight: PARAMETER target='layer4.2.bn2.weight'\n",
       "            p_layer4_2_bn2_bias: PARAMETER target='layer4.2.bn2.bias'\n",
       "            p_layer4_2_conv3_weight: PARAMETER target='layer4.2.conv3.weight'\n",
       "            p_layer4_2_bn3_weight: PARAMETER target='layer4.2.bn3.weight'\n",
       "            p_layer4_2_bn3_bias: PARAMETER target='layer4.2.bn3.bias'\n",
       "            p_p_ims1d2_outl1_dimred_weight: PARAMETER target='p_ims1d2_outl1_dimred.weight'\n",
       "            p_adapt_stage1_b_0_1_conv_weight: PARAMETER target='adapt_stage1_b.0.1_conv.weight'\n",
       "            p_adapt_stage1_b_0_1_conv_bias: PARAMETER target='adapt_stage1_b.0.1_conv.bias'\n",
       "            p_adapt_stage1_b_0_1_conv_relu_varout_dimred_weight: PARAMETER target='adapt_stage1_b.0.1_conv_relu_varout_dimred.weight'\n",
       "            p_adapt_stage1_b_0_2_conv_weight: PARAMETER target='adapt_stage1_b.0.2_conv.weight'\n",
       "            p_adapt_stage1_b_0_2_conv_bias: PARAMETER target='adapt_stage1_b.0.2_conv.bias'\n",
       "            p_adapt_stage1_b_0_2_conv_relu_varout_dimred_weight: PARAMETER target='adapt_stage1_b.0.2_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g1_pool_0_1_outvar_dimred_weight: PARAMETER target='mflow_conv_g1_pool.0.1_outvar_dimred.weight'\n",
       "            p_mflow_conv_g1_pool_0_2_outvar_dimred_weight: PARAMETER target='mflow_conv_g1_pool.0.2_outvar_dimred.weight'\n",
       "            p_mflow_conv_g1_pool_0_3_outvar_dimred_weight: PARAMETER target='mflow_conv_g1_pool.0.3_outvar_dimred.weight'\n",
       "            p_mflow_conv_g1_pool_0_4_outvar_dimred_weight: PARAMETER target='mflow_conv_g1_pool.0.4_outvar_dimred.weight'\n",
       "            p_mflow_conv_g1_b_0_1_conv_weight: PARAMETER target='mflow_conv_g1_b.0.1_conv.weight'\n",
       "            p_mflow_conv_g1_b_0_1_conv_bias: PARAMETER target='mflow_conv_g1_b.0.1_conv.bias'\n",
       "            p_mflow_conv_g1_b_0_1_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g1_b.0.1_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g1_b_0_2_conv_weight: PARAMETER target='mflow_conv_g1_b.0.2_conv.weight'\n",
       "            p_mflow_conv_g1_b_0_2_conv_bias: PARAMETER target='mflow_conv_g1_b.0.2_conv.bias'\n",
       "            p_mflow_conv_g1_b_0_2_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g1_b.0.2_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g1_b_0_3_conv_weight: PARAMETER target='mflow_conv_g1_b.0.3_conv.weight'\n",
       "            p_mflow_conv_g1_b_0_3_conv_bias: PARAMETER target='mflow_conv_g1_b.0.3_conv.bias'\n",
       "            p_mflow_conv_g1_b_0_3_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g1_b.0.3_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g1_b3_joint_varout_dimred_weight: PARAMETER target='mflow_conv_g1_b3_joint_varout_dimred.weight'\n",
       "            p_up_ps4_weight: PARAMETER target='up_ps4.weight'\n",
       "            p_p_ims1d2_outl2_dimred_weight: PARAMETER target='p_ims1d2_outl2_dimred.weight'\n",
       "            p_adapt_stage2_b_0_1_conv_weight: PARAMETER target='adapt_stage2_b.0.1_conv.weight'\n",
       "            p_adapt_stage2_b_0_1_conv_bias: PARAMETER target='adapt_stage2_b.0.1_conv.bias'\n",
       "            p_adapt_stage2_b_0_1_conv_relu_varout_dimred_weight: PARAMETER target='adapt_stage2_b.0.1_conv_relu_varout_dimred.weight'\n",
       "            p_adapt_stage2_b_0_2_conv_weight: PARAMETER target='adapt_stage2_b.0.2_conv.weight'\n",
       "            p_adapt_stage2_b_0_2_conv_bias: PARAMETER target='adapt_stage2_b.0.2_conv.bias'\n",
       "            p_adapt_stage2_b_0_2_conv_relu_varout_dimred_weight: PARAMETER target='adapt_stage2_b.0.2_conv_relu_varout_dimred.weight'\n",
       "            p_adapt_stage2_b2_joint_varout_dimred_weight: PARAMETER target='adapt_stage2_b2_joint_varout_dimred.weight'\n",
       "            p_mflow_conv_g2_pool_0_1_outvar_dimred_weight: PARAMETER target='mflow_conv_g2_pool.0.1_outvar_dimred.weight'\n",
       "            p_mflow_conv_g2_pool_0_2_outvar_dimred_weight: PARAMETER target='mflow_conv_g2_pool.0.2_outvar_dimred.weight'\n",
       "            p_mflow_conv_g2_pool_0_3_outvar_dimred_weight: PARAMETER target='mflow_conv_g2_pool.0.3_outvar_dimred.weight'\n",
       "            p_mflow_conv_g2_pool_0_4_outvar_dimred_weight: PARAMETER target='mflow_conv_g2_pool.0.4_outvar_dimred.weight'\n",
       "            p_mflow_conv_g2_b_0_1_conv_weight: PARAMETER target='mflow_conv_g2_b.0.1_conv.weight'\n",
       "            p_mflow_conv_g2_b_0_1_conv_bias: PARAMETER target='mflow_conv_g2_b.0.1_conv.bias'\n",
       "            p_mflow_conv_g2_b_0_1_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g2_b.0.1_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g2_b_0_2_conv_weight: PARAMETER target='mflow_conv_g2_b.0.2_conv.weight'\n",
       "            p_mflow_conv_g2_b_0_2_conv_bias: PARAMETER target='mflow_conv_g2_b.0.2_conv.bias'\n",
       "            p_mflow_conv_g2_b_0_2_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g2_b.0.2_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g2_b_0_3_conv_weight: PARAMETER target='mflow_conv_g2_b.0.3_conv.weight'\n",
       "            p_mflow_conv_g2_b_0_3_conv_bias: PARAMETER target='mflow_conv_g2_b.0.3_conv.bias'\n",
       "            p_mflow_conv_g2_b_0_3_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g2_b.0.3_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g2_b3_joint_varout_dimred_weight: PARAMETER target='mflow_conv_g2_b3_joint_varout_dimred.weight'\n",
       "            p_up_ps3_weight: PARAMETER target='up_ps3.weight'\n",
       "            p_p_ims1d2_outl3_dimred_weight: PARAMETER target='p_ims1d2_outl3_dimred.weight'\n",
       "            p_adapt_stage3_b_0_1_conv_weight: PARAMETER target='adapt_stage3_b.0.1_conv.weight'\n",
       "            p_adapt_stage3_b_0_1_conv_bias: PARAMETER target='adapt_stage3_b.0.1_conv.bias'\n",
       "            p_adapt_stage3_b_0_1_conv_relu_varout_dimred_weight: PARAMETER target='adapt_stage3_b.0.1_conv_relu_varout_dimred.weight'\n",
       "            p_adapt_stage3_b_0_2_conv_weight: PARAMETER target='adapt_stage3_b.0.2_conv.weight'\n",
       "            p_adapt_stage3_b_0_2_conv_bias: PARAMETER target='adapt_stage3_b.0.2_conv.bias'\n",
       "            p_adapt_stage3_b_0_2_conv_relu_varout_dimred_weight: PARAMETER target='adapt_stage3_b.0.2_conv_relu_varout_dimred.weight'\n",
       "            p_adapt_stage3_b2_joint_varout_dimred_weight: PARAMETER target='adapt_stage3_b2_joint_varout_dimred.weight'\n",
       "            p_mflow_conv_g3_pool_0_1_outvar_dimred_weight: PARAMETER target='mflow_conv_g3_pool.0.1_outvar_dimred.weight'\n",
       "            p_mflow_conv_g3_pool_0_2_outvar_dimred_weight: PARAMETER target='mflow_conv_g3_pool.0.2_outvar_dimred.weight'\n",
       "            p_mflow_conv_g3_pool_0_3_outvar_dimred_weight: PARAMETER target='mflow_conv_g3_pool.0.3_outvar_dimred.weight'\n",
       "            p_mflow_conv_g3_pool_0_4_outvar_dimred_weight: PARAMETER target='mflow_conv_g3_pool.0.4_outvar_dimred.weight'\n",
       "            p_mflow_conv_g3_b_0_1_conv_weight: PARAMETER target='mflow_conv_g3_b.0.1_conv.weight'\n",
       "            p_mflow_conv_g3_b_0_1_conv_bias: PARAMETER target='mflow_conv_g3_b.0.1_conv.bias'\n",
       "            p_mflow_conv_g3_b_0_1_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g3_b.0.1_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g3_b_0_2_conv_weight: PARAMETER target='mflow_conv_g3_b.0.2_conv.weight'\n",
       "            p_mflow_conv_g3_b_0_2_conv_bias: PARAMETER target='mflow_conv_g3_b.0.2_conv.bias'\n",
       "            p_mflow_conv_g3_b_0_2_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g3_b.0.2_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g3_b_0_3_conv_weight: PARAMETER target='mflow_conv_g3_b.0.3_conv.weight'\n",
       "            p_mflow_conv_g3_b_0_3_conv_bias: PARAMETER target='mflow_conv_g3_b.0.3_conv.bias'\n",
       "            p_mflow_conv_g3_b_0_3_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g3_b.0.3_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g3_b3_joint_varout_dimred_weight: PARAMETER target='mflow_conv_g3_b3_joint_varout_dimred.weight'\n",
       "            p_up_ps2_weight: PARAMETER target='up_ps2.weight'\n",
       "            p_p_ims1d2_outl4_dimred_weight: PARAMETER target='p_ims1d2_outl4_dimred.weight'\n",
       "            p_adapt_stage4_b_0_1_conv_weight: PARAMETER target='adapt_stage4_b.0.1_conv.weight'\n",
       "            p_adapt_stage4_b_0_1_conv_bias: PARAMETER target='adapt_stage4_b.0.1_conv.bias'\n",
       "            p_adapt_stage4_b_0_1_conv_relu_varout_dimred_weight: PARAMETER target='adapt_stage4_b.0.1_conv_relu_varout_dimred.weight'\n",
       "            p_adapt_stage4_b_0_2_conv_weight: PARAMETER target='adapt_stage4_b.0.2_conv.weight'\n",
       "            p_adapt_stage4_b_0_2_conv_bias: PARAMETER target='adapt_stage4_b.0.2_conv.bias'\n",
       "            p_adapt_stage4_b_0_2_conv_relu_varout_dimred_weight: PARAMETER target='adapt_stage4_b.0.2_conv_relu_varout_dimred.weight'\n",
       "            p_adapt_stage4_b2_joint_varout_dimred_weight: PARAMETER target='adapt_stage4_b2_joint_varout_dimred.weight'\n",
       "            p_mflow_conv_g4_pool_0_1_outvar_dimred_weight: PARAMETER target='mflow_conv_g4_pool.0.1_outvar_dimred.weight'\n",
       "            p_mflow_conv_g4_pool_0_2_outvar_dimred_weight: PARAMETER target='mflow_conv_g4_pool.0.2_outvar_dimred.weight'\n",
       "            p_mflow_conv_g4_pool_0_3_outvar_dimred_weight: PARAMETER target='mflow_conv_g4_pool.0.3_outvar_dimred.weight'\n",
       "            p_mflow_conv_g4_pool_0_4_outvar_dimred_weight: PARAMETER target='mflow_conv_g4_pool.0.4_outvar_dimred.weight'\n",
       "            p_mflow_conv_g4_b_0_1_conv_weight: PARAMETER target='mflow_conv_g4_b.0.1_conv.weight'\n",
       "            p_mflow_conv_g4_b_0_1_conv_bias: PARAMETER target='mflow_conv_g4_b.0.1_conv.bias'\n",
       "            p_mflow_conv_g4_b_0_1_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g4_b.0.1_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g4_b_0_2_conv_weight: PARAMETER target='mflow_conv_g4_b.0.2_conv.weight'\n",
       "            p_mflow_conv_g4_b_0_2_conv_bias: PARAMETER target='mflow_conv_g4_b.0.2_conv.bias'\n",
       "            p_mflow_conv_g4_b_0_2_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g4_b.0.2_conv_relu_varout_dimred.weight'\n",
       "            p_mflow_conv_g4_b_0_3_conv_weight: PARAMETER target='mflow_conv_g4_b.0.3_conv.weight'\n",
       "            p_mflow_conv_g4_b_0_3_conv_bias: PARAMETER target='mflow_conv_g4_b.0.3_conv.bias'\n",
       "            p_mflow_conv_g4_b_0_3_conv_relu_varout_dimred_weight: PARAMETER target='mflow_conv_g4_b.0.3_conv_relu_varout_dimred.weight'\n",
       "            p_clf_conv1_weight: PARAMETER target='clf_conv1.weight'\n",
       "            p_clf_conv1_bias: PARAMETER target='clf_conv1.bias'\n",
       "            p_clf_conv2_weight: PARAMETER target='clf_conv2.weight'\n",
       "            p_clf_conv2_bias: PARAMETER target='clf_conv2.bias'\n",
       "            b_bn1_running_mean: BUFFER target='bn1.running_mean' persistent=True\n",
       "            b_bn1_running_var: BUFFER target='bn1.running_var' persistent=True\n",
       "            b_bn1_num_batches_tracked: BUFFER target='bn1.num_batches_tracked' persistent=True\n",
       "            b_layer1_0_bn1_running_mean: BUFFER target='layer1.0.bn1.running_mean' persistent=True\n",
       "            b_layer1_0_bn1_running_var: BUFFER target='layer1.0.bn1.running_var' persistent=True\n",
       "            b_layer1_0_bn1_num_batches_tracked: BUFFER target='layer1.0.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer1_0_bn2_running_mean: BUFFER target='layer1.0.bn2.running_mean' persistent=True\n",
       "            b_layer1_0_bn2_running_var: BUFFER target='layer1.0.bn2.running_var' persistent=True\n",
       "            b_layer1_0_bn2_num_batches_tracked: BUFFER target='layer1.0.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer1_0_bn3_running_mean: BUFFER target='layer1.0.bn3.running_mean' persistent=True\n",
       "            b_layer1_0_bn3_running_var: BUFFER target='layer1.0.bn3.running_var' persistent=True\n",
       "            b_layer1_0_bn3_num_batches_tracked: BUFFER target='layer1.0.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer1_0_downsample_1_running_mean: BUFFER target='layer1.0.downsample.1.running_mean' persistent=True\n",
       "            b_layer1_0_downsample_1_running_var: BUFFER target='layer1.0.downsample.1.running_var' persistent=True\n",
       "            b_layer1_0_downsample_1_num_batches_tracked: BUFFER target='layer1.0.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_layer1_1_bn1_running_mean: BUFFER target='layer1.1.bn1.running_mean' persistent=True\n",
       "            b_layer1_1_bn1_running_var: BUFFER target='layer1.1.bn1.running_var' persistent=True\n",
       "            b_layer1_1_bn1_num_batches_tracked: BUFFER target='layer1.1.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer1_1_bn2_running_mean: BUFFER target='layer1.1.bn2.running_mean' persistent=True\n",
       "            b_layer1_1_bn2_running_var: BUFFER target='layer1.1.bn2.running_var' persistent=True\n",
       "            b_layer1_1_bn2_num_batches_tracked: BUFFER target='layer1.1.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer1_1_bn3_running_mean: BUFFER target='layer1.1.bn3.running_mean' persistent=True\n",
       "            b_layer1_1_bn3_running_var: BUFFER target='layer1.1.bn3.running_var' persistent=True\n",
       "            b_layer1_1_bn3_num_batches_tracked: BUFFER target='layer1.1.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer1_2_bn1_running_mean: BUFFER target='layer1.2.bn1.running_mean' persistent=True\n",
       "            b_layer1_2_bn1_running_var: BUFFER target='layer1.2.bn1.running_var' persistent=True\n",
       "            b_layer1_2_bn1_num_batches_tracked: BUFFER target='layer1.2.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer1_2_bn2_running_mean: BUFFER target='layer1.2.bn2.running_mean' persistent=True\n",
       "            b_layer1_2_bn2_running_var: BUFFER target='layer1.2.bn2.running_var' persistent=True\n",
       "            b_layer1_2_bn2_num_batches_tracked: BUFFER target='layer1.2.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer1_2_bn3_running_mean: BUFFER target='layer1.2.bn3.running_mean' persistent=True\n",
       "            b_layer1_2_bn3_running_var: BUFFER target='layer1.2.bn3.running_var' persistent=True\n",
       "            b_layer1_2_bn3_num_batches_tracked: BUFFER target='layer1.2.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer2_0_bn1_running_mean: BUFFER target='layer2.0.bn1.running_mean' persistent=True\n",
       "            b_layer2_0_bn1_running_var: BUFFER target='layer2.0.bn1.running_var' persistent=True\n",
       "            b_layer2_0_bn1_num_batches_tracked: BUFFER target='layer2.0.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer2_0_bn2_running_mean: BUFFER target='layer2.0.bn2.running_mean' persistent=True\n",
       "            b_layer2_0_bn2_running_var: BUFFER target='layer2.0.bn2.running_var' persistent=True\n",
       "            b_layer2_0_bn2_num_batches_tracked: BUFFER target='layer2.0.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer2_0_bn3_running_mean: BUFFER target='layer2.0.bn3.running_mean' persistent=True\n",
       "            b_layer2_0_bn3_running_var: BUFFER target='layer2.0.bn3.running_var' persistent=True\n",
       "            b_layer2_0_bn3_num_batches_tracked: BUFFER target='layer2.0.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer2_0_downsample_1_running_mean: BUFFER target='layer2.0.downsample.1.running_mean' persistent=True\n",
       "            b_layer2_0_downsample_1_running_var: BUFFER target='layer2.0.downsample.1.running_var' persistent=True\n",
       "            b_layer2_0_downsample_1_num_batches_tracked: BUFFER target='layer2.0.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_layer2_1_bn1_running_mean: BUFFER target='layer2.1.bn1.running_mean' persistent=True\n",
       "            b_layer2_1_bn1_running_var: BUFFER target='layer2.1.bn1.running_var' persistent=True\n",
       "            b_layer2_1_bn1_num_batches_tracked: BUFFER target='layer2.1.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer2_1_bn2_running_mean: BUFFER target='layer2.1.bn2.running_mean' persistent=True\n",
       "            b_layer2_1_bn2_running_var: BUFFER target='layer2.1.bn2.running_var' persistent=True\n",
       "            b_layer2_1_bn2_num_batches_tracked: BUFFER target='layer2.1.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer2_1_bn3_running_mean: BUFFER target='layer2.1.bn3.running_mean' persistent=True\n",
       "            b_layer2_1_bn3_running_var: BUFFER target='layer2.1.bn3.running_var' persistent=True\n",
       "            b_layer2_1_bn3_num_batches_tracked: BUFFER target='layer2.1.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer2_2_bn1_running_mean: BUFFER target='layer2.2.bn1.running_mean' persistent=True\n",
       "            b_layer2_2_bn1_running_var: BUFFER target='layer2.2.bn1.running_var' persistent=True\n",
       "            b_layer2_2_bn1_num_batches_tracked: BUFFER target='layer2.2.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer2_2_bn2_running_mean: BUFFER target='layer2.2.bn2.running_mean' persistent=True\n",
       "            b_layer2_2_bn2_running_var: BUFFER target='layer2.2.bn2.running_var' persistent=True\n",
       "            b_layer2_2_bn2_num_batches_tracked: BUFFER target='layer2.2.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer2_2_bn3_running_mean: BUFFER target='layer2.2.bn3.running_mean' persistent=True\n",
       "            b_layer2_2_bn3_running_var: BUFFER target='layer2.2.bn3.running_var' persistent=True\n",
       "            b_layer2_2_bn3_num_batches_tracked: BUFFER target='layer2.2.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer2_3_bn1_running_mean: BUFFER target='layer2.3.bn1.running_mean' persistent=True\n",
       "            b_layer2_3_bn1_running_var: BUFFER target='layer2.3.bn1.running_var' persistent=True\n",
       "            b_layer2_3_bn1_num_batches_tracked: BUFFER target='layer2.3.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer2_3_bn2_running_mean: BUFFER target='layer2.3.bn2.running_mean' persistent=True\n",
       "            b_layer2_3_bn2_running_var: BUFFER target='layer2.3.bn2.running_var' persistent=True\n",
       "            b_layer2_3_bn2_num_batches_tracked: BUFFER target='layer2.3.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer2_3_bn3_running_mean: BUFFER target='layer2.3.bn3.running_mean' persistent=True\n",
       "            b_layer2_3_bn3_running_var: BUFFER target='layer2.3.bn3.running_var' persistent=True\n",
       "            b_layer2_3_bn3_num_batches_tracked: BUFFER target='layer2.3.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer3_0_bn1_running_mean: BUFFER target='layer3.0.bn1.running_mean' persistent=True\n",
       "            b_layer3_0_bn1_running_var: BUFFER target='layer3.0.bn1.running_var' persistent=True\n",
       "            b_layer3_0_bn1_num_batches_tracked: BUFFER target='layer3.0.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer3_0_bn2_running_mean: BUFFER target='layer3.0.bn2.running_mean' persistent=True\n",
       "            b_layer3_0_bn2_running_var: BUFFER target='layer3.0.bn2.running_var' persistent=True\n",
       "            b_layer3_0_bn2_num_batches_tracked: BUFFER target='layer3.0.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer3_0_bn3_running_mean: BUFFER target='layer3.0.bn3.running_mean' persistent=True\n",
       "            b_layer3_0_bn3_running_var: BUFFER target='layer3.0.bn3.running_var' persistent=True\n",
       "            b_layer3_0_bn3_num_batches_tracked: BUFFER target='layer3.0.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer3_0_downsample_1_running_mean: BUFFER target='layer3.0.downsample.1.running_mean' persistent=True\n",
       "            b_layer3_0_downsample_1_running_var: BUFFER target='layer3.0.downsample.1.running_var' persistent=True\n",
       "            b_layer3_0_downsample_1_num_batches_tracked: BUFFER target='layer3.0.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_layer3_1_bn1_running_mean: BUFFER target='layer3.1.bn1.running_mean' persistent=True\n",
       "            b_layer3_1_bn1_running_var: BUFFER target='layer3.1.bn1.running_var' persistent=True\n",
       "            b_layer3_1_bn1_num_batches_tracked: BUFFER target='layer3.1.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer3_1_bn2_running_mean: BUFFER target='layer3.1.bn2.running_mean' persistent=True\n",
       "            b_layer3_1_bn2_running_var: BUFFER target='layer3.1.bn2.running_var' persistent=True\n",
       "            b_layer3_1_bn2_num_batches_tracked: BUFFER target='layer3.1.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer3_1_bn3_running_mean: BUFFER target='layer3.1.bn3.running_mean' persistent=True\n",
       "            b_layer3_1_bn3_running_var: BUFFER target='layer3.1.bn3.running_var' persistent=True\n",
       "            b_layer3_1_bn3_num_batches_tracked: BUFFER target='layer3.1.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer3_2_bn1_running_mean: BUFFER target='layer3.2.bn1.running_mean' persistent=True\n",
       "            b_layer3_2_bn1_running_var: BUFFER target='layer3.2.bn1.running_var' persistent=True\n",
       "            b_layer3_2_bn1_num_batches_tracked: BUFFER target='layer3.2.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer3_2_bn2_running_mean: BUFFER target='layer3.2.bn2.running_mean' persistent=True\n",
       "            b_layer3_2_bn2_running_var: BUFFER target='layer3.2.bn2.running_var' persistent=True\n",
       "            b_layer3_2_bn2_num_batches_tracked: BUFFER target='layer3.2.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer3_2_bn3_running_mean: BUFFER target='layer3.2.bn3.running_mean' persistent=True\n",
       "            b_layer3_2_bn3_running_var: BUFFER target='layer3.2.bn3.running_var' persistent=True\n",
       "            b_layer3_2_bn3_num_batches_tracked: BUFFER target='layer3.2.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer4_0_bn1_running_mean: BUFFER target='layer4.0.bn1.running_mean' persistent=True\n",
       "            b_layer4_0_bn1_running_var: BUFFER target='layer4.0.bn1.running_var' persistent=True\n",
       "            b_layer4_0_bn1_num_batches_tracked: BUFFER target='layer4.0.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer4_0_bn2_running_mean: BUFFER target='layer4.0.bn2.running_mean' persistent=True\n",
       "            b_layer4_0_bn2_running_var: BUFFER target='layer4.0.bn2.running_var' persistent=True\n",
       "            b_layer4_0_bn2_num_batches_tracked: BUFFER target='layer4.0.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer4_0_bn3_running_mean: BUFFER target='layer4.0.bn3.running_mean' persistent=True\n",
       "            b_layer4_0_bn3_running_var: BUFFER target='layer4.0.bn3.running_var' persistent=True\n",
       "            b_layer4_0_bn3_num_batches_tracked: BUFFER target='layer4.0.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer4_0_downsample_1_running_mean: BUFFER target='layer4.0.downsample.1.running_mean' persistent=True\n",
       "            b_layer4_0_downsample_1_running_var: BUFFER target='layer4.0.downsample.1.running_var' persistent=True\n",
       "            b_layer4_0_downsample_1_num_batches_tracked: BUFFER target='layer4.0.downsample.1.num_batches_tracked' persistent=True\n",
       "            b_layer4_1_bn1_running_mean: BUFFER target='layer4.1.bn1.running_mean' persistent=True\n",
       "            b_layer4_1_bn1_running_var: BUFFER target='layer4.1.bn1.running_var' persistent=True\n",
       "            b_layer4_1_bn1_num_batches_tracked: BUFFER target='layer4.1.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer4_1_bn2_running_mean: BUFFER target='layer4.1.bn2.running_mean' persistent=True\n",
       "            b_layer4_1_bn2_running_var: BUFFER target='layer4.1.bn2.running_var' persistent=True\n",
       "            b_layer4_1_bn2_num_batches_tracked: BUFFER target='layer4.1.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer4_1_bn3_running_mean: BUFFER target='layer4.1.bn3.running_mean' persistent=True\n",
       "            b_layer4_1_bn3_running_var: BUFFER target='layer4.1.bn3.running_var' persistent=True\n",
       "            b_layer4_1_bn3_num_batches_tracked: BUFFER target='layer4.1.bn3.num_batches_tracked' persistent=True\n",
       "            b_layer4_2_bn1_running_mean: BUFFER target='layer4.2.bn1.running_mean' persistent=True\n",
       "            b_layer4_2_bn1_running_var: BUFFER target='layer4.2.bn1.running_var' persistent=True\n",
       "            b_layer4_2_bn1_num_batches_tracked: BUFFER target='layer4.2.bn1.num_batches_tracked' persistent=True\n",
       "            b_layer4_2_bn2_running_mean: BUFFER target='layer4.2.bn2.running_mean' persistent=True\n",
       "            b_layer4_2_bn2_running_var: BUFFER target='layer4.2.bn2.running_var' persistent=True\n",
       "            b_layer4_2_bn2_num_batches_tracked: BUFFER target='layer4.2.bn2.num_batches_tracked' persistent=True\n",
       "            b_layer4_2_bn3_running_mean: BUFFER target='layer4.2.bn3.running_mean' persistent=True\n",
       "            b_layer4_2_bn3_running_var: BUFFER target='layer4.2.bn3.running_var' persistent=True\n",
       "            b_layer4_2_bn3_num_batches_tracked: BUFFER target='layer4.2.bn3.num_batches_tracked' persistent=True\n",
       "            x: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            conv2d_114: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {s77: VR[0, int_oo], s53: VR[3, int_oo], s0: VR[3, int_oo]}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from ms_resunet import MS_ResUNet, Bottleneck\n",
    "\n",
    "# 1) создать модель\n",
    "model = MS_ResUNet()  # твоя функция/класс\n",
    "ckpt = torch.load(\"C:/Users/Вячеслав/Documents/superresolution/models/best_x2_ms_resunet.pt\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model\"])  # или ckpt[\"model\"], зависит от формата\n",
    "model.eval()\n",
    "\n",
    "# 2) dummy input (пример: 1x1x256x256)\n",
    "x = torch.randn(1, 1, 256, 256, dtype=torch.float32)\n",
    "\n",
    "# 3) экспорт\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    (x,),\n",
    "    \"sr_model.onnx\",\n",
    "    dynamo=True,                 # новый рекомендуемый экспорт\n",
    "    opset_version=17,            # часто нормальный базовый выбор\n",
    "    input_names=[\"lr\"],\n",
    "    output_names=[\"sr\"],\n",
    "    dynamic_axes={\"lr\": {0: \"N\", 2: \"H\", 3: \"W\"},\n",
    "                  \"sr\": {0: \"N\", 2: \"H_out\", 3: \"W_out\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25352c57-44ec-4cb7-8efc-c1158dceee61",
   "metadata": {},
   "source": [
    "### checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5432f094-155c-441d-b943-8c2e75051797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opset imports: [('', 17)]\n",
      "IR version: 10\n"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "\n",
    "m = onnx.load(\"sr_model.onnx\")\n",
    "onnx.checker.check_model(m)\n",
    "print(\"Opset imports:\", [(op.domain, op.version) for op in m.opset_import])\n",
    "print(\"IR version:\", m.ir_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7018bbe-3f6a-49c8-8511-96831f498491",
   "metadata": {},
   "source": [
    "### Сравнение выходов pytorch и onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe2a283d-f581-4708-a28e-4dbcf3af51b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max abs diff: 2.682209e-06\n",
      "mean abs diff: 4.5286515e-07\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import onnxruntime as ort\n",
    "\n",
    "# torch output\n",
    "model.eval()\n",
    "x = torch.randn(1, 1, 256, 256, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    y_torch = model(x).cpu().numpy()\n",
    "\n",
    "# onnxruntime output\n",
    "sess = ort.InferenceSession(\"sr_model.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "inp_name = sess.get_inputs()[0].name\n",
    "y_onnx = sess.run(None, {inp_name: x.cpu().numpy()})[0]\n",
    "\n",
    "diff = np.max(np.abs(y_torch - y_onnx))\n",
    "print(\"max abs diff:\", diff)\n",
    "print(\"mean abs diff:\", np.mean(np.abs(y_torch - y_onnx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339e83b9-22b9-4f29-bc4b-4c1a5570cc90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
