{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42cd6193-0187-4212-921e-614bc4ff8f8b",
   "metadata": {},
   "source": [
    "# Эксперименты с CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29cf4d4-c93c-4834-a0e0-d07f6d7301c6",
   "metadata": {},
   "source": [
    "## U-Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef714df9-d111-467e-9bfd-addefc6ca713",
   "metadata": {},
   "source": [
    "![Детали тренировок CNN](Example_architecture_of_U-Net_for_producing_k_256-by-256_image_masks_for_a_256-by-256_RGB_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c72bbf1-e140-4a81-bcaf-98f07d309e35",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ae2e4-09c8-4ef1-9a87-9a3a11d21489",
   "metadata": {},
   "source": [
    "Для разных CNN отличаются рекомендуемая предобработка. Так, скажем, для U-Net в статье **Exploring microstructureand petrophysical properties of microporous volcanic rocks through 3D multiscale and super‑resolution imaging**, Buono et al. (2023) говорится про следующую цепочку обработок: HR  -> Bicubic downsampling -> LR -> Bicubic upsampling -> LR того же разрешения, что и HR. Данный метод дает наивысшее качество среди всех CNN в испытаниях ($SSIM$ = 0.8, $PSNR$ = 28.58). В обучении других НС использовался внутренний архитектурный upsampling. Однако, при детальном рассмотрении выяснилось, что U-Net обучалась на 24300 патчах, в то время как остальные CNN - лишь на 3888:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e643d1-e591-4b6d-a175-9703c1a22b85",
   "metadata": {},
   "source": [
    "![Детали тренировок CNN](buono_cnn_training.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27444e14-cb95-4d69-82cc-61eda8d2d76f",
   "metadata": {},
   "source": [
    "Это означает, что у U-Net было изначально больше данных, чем у других моделей. В нашем исследовании будем использовать одинаковое количество training epochs и patches для всех архитектур."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf7bf6c-baf3-41bd-97b5-8a5469c167ab",
   "metadata": {},
   "source": [
    "Обучение baseline будет проходить на данных из shuffled2D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c623a-5c3b-4b9c-9c45-a5ac31d4c1e0",
   "metadata": {},
   "source": [
    "**План обработки для U-Net**:\n",
    "1. Перевод из 'RGB' в 'L' (серый);\n",
    "2. Upscaling LR до размера HR;\n",
    "3. Аугментации:\n",
    "   * Нормализация: либо в диапазон [0, 1], либо по (mean, std). Возьмем по (mean, std). Обязательный этап;\n",
    "   * `RandomHorizontalFlip`, `RandomVerticalFlip`: не нарушает статистику пор, но добавляет информации;\n",
    "   * `RandomCrop`: полезно для экономии памяти;\n",
    "   * `RandomGaussianBlur`: размывает изображение, имитирует настоящие ошибки при КТ;\n",
    "4. Этап обучения, предсказание;\n",
    "5. Обратная нормализация для интерпретации результатов."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300c94a-b884-4f75-a118-c7fda32a4359",
   "metadata": {},
   "source": [
    "Импорты библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d3ab1a2-f056-44af-a043-288aac7fff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional, Callable\n",
    "import random\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchmetrics.image.ssim import StructuralSimilarityIndexMeasure\n",
    "import torch\n",
    "from pytorch_msssim import ssim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import wandb\n",
    "import math\n",
    "from math import log10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd24b310-ca1e-4423-9763-9d0284d3336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "try:\n",
    "    from pydantic._internal._generate_schema import UnsupportedFieldAttributeWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=UnsupportedFieldAttributeWarning)\n",
    "except Exception:\n",
    "    from pydantic.warnings import PydanticUserWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=PydanticUserWarning, module=\"pydantic._internal._generate_schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb57b1f-17d8-436b-8831-f63094b21864",
   "metadata": {},
   "source": [
    "Зафиксируем один seed для воспроизводимости:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0cef67-7dc5-4e80-9831-43292eebd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8c93a6-0a62-4ca8-8913-e04f9fce03f3",
   "metadata": {},
   "source": [
    "Мы хотим создать датасет из пар HR и LR. Для этого:\n",
    "1. Обратимся к директориям с изображениями (_get_dirs);\n",
    "2. Обрежем имена изображений из LR-директорий (_strip_lr_suffix. Сейчас они формата 0001x2 или 0001x4, приведем их к 0001.) Это нужно для сопоставления пар HR и LR по имени;\n",
    "3. Создаем датасет (Shuffled2DPaired). На выходе получим словарь вида {'hr': ..., 'lr': ..., 'stem': x2 или x4}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af315941-666a-451a-ac8a-f7c18196d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_dirs(root: str, split: str, scale: str) -> Tuple[Path, Path]:\n",
    "    root = Path(root)\n",
    "    hr_dir = root / \"shuffled2D\" / f\"shuffled2D_{split}_HR\"\n",
    "    lr_dir = root / \"shuffled2D\" / f\"shuffled2D_{split}_LR_default_{scale}\"\n",
    "    if not lr_dir.exists():\n",
    "        fb = root / \"shuffled2D\" / f\"shuffled2D_{split}_LR_default_X2\"\n",
    "        if fb.exists():\n",
    "            lr_dir = fb\n",
    "    if not (hr_dir.exists() and lr_dir.exists()):\n",
    "        raise FileNotFoundError(f\"Не найдены HR/LR директории для split={split}, scale={scale}\")\n",
    "    return hr_dir, lr_dir\n",
    "    \n",
    "def _strip_lr_suffix(stem: str, scale: str) -> str:\n",
    "    # scale: \"X2\"/\"X4\" → убрать ровно такой хвост; поддержим опционально '_' или '-'\n",
    "    suf = scale.lower()\n",
    "    if not suf.startswith('x'):\n",
    "        suf = 'x' + suf\n",
    "    return re.sub(fr'([_-]?){re.escape(suf)}$', '', stem, flags=re.IGNORECASE)\n",
    "\n",
    "class Shuffled2DPaired(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root: str,\n",
    "        split: str = \"train\",\n",
    "        scale: str = \"X2\",\n",
    "        exts: Tuple[str, ...] = (\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\"),\n",
    "        transform_pair: Optional[Callable] = None,  # <- весь пайплайн тут\n",
    "    ):\n",
    "        self.hr_dir, self.lr_dir = _get_dirs(root, split, scale)\n",
    "        self.exts = exts\n",
    "        self.transform_pair = transform_pair\n",
    "\n",
    "        hr_files = sorted([p for p in self.hr_dir.iterdir() if p.suffix.lower() in exts])\n",
    "        if not hr_files:\n",
    "            raise RuntimeError(f\"Нет HR-файлов в {self.hr_dir}\")\n",
    "        hr_map = {p.stem: p for p in hr_files}\n",
    "\n",
    "        lr_files = sorted([p for p in self.lr_dir.iterdir() if p.suffix.lower() in exts])\n",
    "        pairs = []\n",
    "        for p in lr_files:\n",
    "            hr_stem = _strip_lr_suffix(p.stem, scale)\n",
    "            hr = hr_map.get(hr_stem)\n",
    "            if hr is not None:\n",
    "                pairs.append((p, hr))\n",
    "        if not pairs:\n",
    "            raise RuntimeError(\"Не найдено пар LR↔HR по совпадающим именам файлов.\")\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def _open_raw(self, p: Path) -> Image.Image:\n",
    "        with Image.open(p) as img:\n",
    "            return img.copy()\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        lr_path, hr_path = self.pairs[idx]\n",
    "        lr = self._open_raw(lr_path)\n",
    "        hr = self._open_raw(hr_path)\n",
    "\n",
    "        if self.transform_pair is not None:\n",
    "            lr, hr = self.transform_pair(lr, hr)\n",
    "\n",
    "        return {\"lr\": lr, \"hr\": hr, \"stem\": lr_path.stem}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c3a2c4-ab22-4ba3-ac31-7e87b740f465",
   "metadata": {},
   "source": [
    "Единоразово считаем среднее и стандартное отклонение по всем shuffled HR, предварительно переведя в grayscale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19db939c-2397-43c4-9fa7-d5c7ce0b607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_via_hist_from_ds(ds) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Считает mean/std для HR через 256-биновую гистограмму, предварительно конвертируя в 'L'.\n",
    "    Результат в [0, 1].\n",
    "    Требования:\n",
    "      - у датасета есть поля: hr_dir (Path) и exts (кортеж расширений)\n",
    "    \"\"\"\n",
    "    hr_dir: Path = ds.hr_dir\n",
    "    exts = set(e.lower() for e in getattr(ds, \"exts\", (\".png\", \".jpg\", \".jpeg\", \".tif\", \".tiff\")))\n",
    "\n",
    "    hist = np.zeros(256, dtype=np.int64)\n",
    "    total_pixels = 0\n",
    "\n",
    "    for p in hr_dir.iterdir():\n",
    "        if p.suffix.lower() not in exts:\n",
    "            continue\n",
    "        # Конвертация в 'L' здесь гарантирует нужный формат (8-бит, один канал)\n",
    "        with Image.open(p) as img:\n",
    "            img = img.convert(\"L\")\n",
    "            arr = np.asarray(img, dtype=np.uint8)  # 0..255\n",
    "\n",
    "        h = np.bincount(arr.ravel(), minlength=256)  # int64\n",
    "        hist += h\n",
    "        total_pixels += arr.size\n",
    "\n",
    "    if total_pixels == 0:\n",
    "        raise RuntimeError(f\"В {hr_dir} нет подходящих HR-изображений (расширения: {sorted(exts)})\")\n",
    "\n",
    "    bins = np.arange(256, dtype=np.float64)\n",
    "    mean  = (bins * hist).sum() / total_pixels\n",
    "    mean2 = (bins**2 * hist).sum() / total_pixels\n",
    "    std   = np.sqrt(max(0.0, mean2 - mean * mean))\n",
    "\n",
    "    # в [0,1]\n",
    "    return np.array([mean / 255.0]), np.array([std / 255.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1d9a242-dfc6-4115-b8fb-faee3b210697",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path(\"DeepRockSR-2D/shuffled2D\")  # корень, где лежат папки shuffled2D_*\n",
    "hr_dir = root / \"shuffled2D_train_HR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af41f0ef-bf3c-45d5-94ea-3ce3d46c26f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR mean: [0.45161797]\n",
      "HR std : [0.20893379]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ds_stats = Shuffled2DPaired(root, split=\"train\", scale=\"X2\",\n",
    "                            patch_size=None, augment=False, grayscale=True)\n",
    "\n",
    "mean_hr, std_hr = mean_std_via_hist_from_ds(ds_stats)\n",
    "print(\"HR mean:\", mean_hr)\n",
    "print(\"HR std :\", std_hr)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84fffffc-6d77-4e38-9693-066a02169cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_mean = (0.45161797,)\n",
    "hr_std = (0.20893379,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1c53e9-c9a5-483a-8e06-6073c73c394b",
   "metadata": {},
   "source": [
    "Аугментации и трансформации в виде классов. Обычный v2.Compose не позволяет по умолчанию применять рандомизированные трасформации с одним и тем же исходом, что важно для парных датасетов, поэтому были построены кастомные классы для каждого преобразования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e59fa43-b83e-480a-b159-47f143605c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairCompose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    def __call__(self, lr, hr):\n",
    "        for t in self.transforms:\n",
    "            lr, hr = t(lr, hr)\n",
    "        return lr, hr\n",
    "\n",
    "class PairGrayscale:\n",
    "    def __init__(self, num_output_channels: int = 1):\n",
    "        self.t = T.Grayscale(num_output_channels)\n",
    "    def __call__(self, lr, hr):\n",
    "        return self.t(lr), self.t(hr)\n",
    "\n",
    "class PairUpscaleLRtoHR:\n",
    "    \"\"\"Апскейлит LR до точного размера HR (bicubic)\"\"\"\n",
    "    def __call__(self, lr, hr):\n",
    "        if lr.size != hr.size:\n",
    "            lr = F.resize(lr, size=hr.size[::-1], interpolation=InterpolationMode.BICUBIC, antialias=True)\n",
    "        return lr, hr\n",
    "\n",
    "class PairRandomCrop:\n",
    "    \"\"\"Согласованный кроп после апскейла: одна и та же box к LR_up и HR.\"\"\"\n",
    "    def __init__(self, patch_size: Optional[int]):\n",
    "        self.ps = patch_size\n",
    "    def __call__(self, lr, hr):\n",
    "        if self.ps is None:\n",
    "            return lr, hr\n",
    "        h, w = hr.size[1], hr.size[0]\n",
    "        th = tw = self.ps\n",
    "        i, j, h, w = T.RandomCrop.get_params(hr, output_size=(th, tw))\n",
    "        return F.crop(lr, i, j, h, w), F.crop(hr, i, j, h, w)\n",
    "\n",
    "class PairFlips:\n",
    "    \"\"\"Согласованные флипы.\"\"\"\n",
    "    def __init__(self, p_flip=0.5, p_vflip=0.5):\n",
    "        self.pf, self.pv = p_flip, p_vflip\n",
    "    def __call__(self, lr, hr):\n",
    "        if torch.rand(()) < self.pf:\n",
    "            lr, hr = F.hflip(lr), F.hflip(hr)\n",
    "        if torch.rand(()) < self.pv:\n",
    "            lr, hr = F.vflip(lr), F.vflip(hr)\n",
    "        return lr, hr\n",
    "\n",
    "class PairToTensor01:\n",
    "    \"\"\"ToImage -> float32 [0,1] для обеих.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.to_img = T.ToImage()\n",
    "        self.to_f32 = T.ToDtype(torch.float32, scale=True)\n",
    "    def __call__(self, lr, hr):\n",
    "        lr = self.to_f32(self.to_img(lr))\n",
    "        hr = self.to_f32(self.to_img(hr))\n",
    "        return lr, hr\n",
    "        \n",
    "class PairGaussianBlur:\n",
    "    \"\"\"\n",
    "    Согласованный GaussianBlur: один и тот же sigma/случайность для LR и HR.\n",
    "    Работает на тензорах после ToTensor/ToDtype(scale=True).\n",
    "    \"\"\"\n",
    "    def __init__(self, kernel_size, sigma=(0.1, 2.0), p=0.5):\n",
    "        if isinstance(kernel_size, int):\n",
    "            # kernel должен быть нечётным\n",
    "            if kernel_size % 2 == 0:\n",
    "                raise ValueError(\"kernel_size должен быть нечётным\")\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "        self.p = float(p)\n",
    "\n",
    "    def __call__(self, lr, hr):\n",
    "        # одна монетка на пару\n",
    "        if torch.rand(()) >= self.p:\n",
    "            return lr, hr\n",
    "        # одна sigma на пару\n",
    "        if isinstance(self.sigma, (tuple, list)):\n",
    "            s_low, s_high = self.sigma\n",
    "            sigma = float(torch.empty(1).uniform_(s_low, s_high))\n",
    "        else:\n",
    "            sigma = float(self.sigma)\n",
    "        lr = F.gaussian_blur(lr, kernel_size=self.kernel_size, sigma=sigma)\n",
    "        hr = F.gaussian_blur(hr, kernel_size=self.kernel_size, sigma=sigma)\n",
    "        return lr, hr\n",
    "        \n",
    "class PairNormalize:\n",
    "    \"\"\"Применяет одинаковый Normalize к обоим тензорам (каналы те же, параметры детерминированы).\"\"\"\n",
    "    def __init__(self, mean, std):\n",
    "        self.norm = T.Normalize(mean=mean, std=std)\n",
    "    def __call__(self, lr, hr):\n",
    "        return self.norm(lr), self.norm(hr)\n",
    "        \n",
    "def build_pair_transform(\n",
    "    patch_size: Optional[int],\n",
    "    do_flips: bool = True,\n",
    "    do_blur: bool = True,\n",
    "    blur_kernel: int = 3,\n",
    "    blur_sigma: tuple[float, float] = (0.1, 1.5),\n",
    "    mean: tuple[float, ...] = (0.45161797,),\n",
    "    std: tuple[float, ...]  = (0.20893379,)\n",
    ") -> PairCompose:\n",
    "    stages = [\n",
    "        PairGrayscale(num_output_channels=1),   # 1) grayscale\n",
    "        PairUpscaleLRtoHR(),                    # 2) upscale LR -> HR size\n",
    "        PairRandomCrop(patch_size),             # 3) согласованный кроп\n",
    "    ]\n",
    "    if do_flips:\n",
    "        stages.append(PairFlips())              # 4) согласованные флипы\n",
    "    stages.append(PairToTensor01())             # 5) к тензорам [0,1]\n",
    "    if do_blur:\n",
    "        stages.append(PairGaussianBlur(kernel_size=blur_kernel, sigma=blur_sigma, p=0.5))  # блюр\n",
    "    stages.append(PairNormalize(mean=mean, std=std)) # нормализация\n",
    "    return PairCompose(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dd402d-ded8-432c-91f1-2593a5fc99ea",
   "metadata": {},
   "source": [
    "Собираем трансформы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73129a3d-e97a-4ceb-9d86-871aee48b409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Комментариями описан порядок строго встроенных преобразований\n",
    "train_tf = build_pair_transform(\n",
    "    patch_size=256,\n",
    "    # stages из функции,\n",
    "    do_flips=True,\n",
    "    # автоматически T.ToImage(),\n",
    "    # автоматически T.ToDtype(torch.float32, scale=True),\n",
    "    do_blur=True,\n",
    "    # автоматически T.Normalize(mean=mean, std=std))\n",
    ")\n",
    "\n",
    "eval_test_tf = build_pair_transform(\n",
    "    # те же обязательные stages, одинаковы для train, val, test\n",
    "    patch_size=None, \n",
    "    do_flips=False,\n",
    "    # автоматически T.ToImage(),\n",
    "    # автоматически T.ToDtype(torch.float32, scale=True),\n",
    "    do_blur=False,\n",
    "    # автоматически T.Normalize(mean=mean, std=std))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dcb37d-e307-4e43-9e1b-1e5f8b346d7f",
   "metadata": {},
   "source": [
    "Применяем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e9c4d3-a573-452c-a920-ecdfe35efa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_x2 = Shuffled2DPaired(root=\"DeepRockSR-2D\", split=\"train\", scale=\"X2\", transform_pair=train_tf)\n",
    "valid_ds_x2 = Shuffled2DPaired(root=\"DeepRockSR-2D\", split=\"valid\", scale=\"X2\", transform_pair=eval_test_tf)\n",
    "test_ds_x2  = Shuffled2DPaired(root=\"DeepRockSR-2D\", split=\"test\",  scale=\"X2\", transform_pair=eval_test_tf)\n",
    "\n",
    "train_ds_x4 = Shuffled2DPaired(root=\"DeepRockSR-2D\", split=\"train\", scale=\"X4\", transform_pair=train_tf)\n",
    "valid_ds_x4 = Shuffled2DPaired(root=\"DeepRockSR-2D\", split=\"valid\", scale=\"X4\", transform_pair=eval_test_tf)\n",
    "test_ds_x4  = Shuffled2DPaired(root=\"DeepRockSR-2D\", split=\"test\",  scale=\"X4\", transform_pair=eval_test_tf)\n",
    "\n",
    "sample = train_ds_x2[0]\n",
    "# sample[\"lr\"], sample[\"hr\"] — тензоры [1,H,W] в [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd241cbe-2a94-4404-a031-f4e43dbb49a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': Image([[[-1.4671, -1.4644, -1.4597,  ..., -1.4058, -1.4172, -1.4242],\n",
       "         [-1.4671, -1.4634, -1.4570,  ..., -1.4020, -1.4144, -1.4222],\n",
       "         [-1.4671, -1.4621, -1.4533,  ..., -1.4020, -1.4158, -1.4245],\n",
       "         ...,\n",
       "         [-1.8900, -1.8923, -1.8964,  ..., -1.4185, -1.4080, -1.4020],\n",
       "         [-1.8850, -1.8887, -1.8951,  ..., -1.4159, -1.3970, -1.3870],\n",
       "         [-1.8888, -1.8914, -1.8961,  ..., -1.4131, -1.3932, -1.3832]]], ),\n",
       " 'hr': Image([[[-1.4576, -1.4551, -1.4556,  ..., -1.4116, -1.4292, -1.4390],\n",
       "         [-1.4674, -1.4630, -1.4583,  ..., -1.4134, -1.4333, -1.4419],\n",
       "         [-1.4823, -1.4745, -1.4597,  ..., -1.4207, -1.4411, -1.4468],\n",
       "         ...,\n",
       "         [-1.8973, -1.8964, -1.8923,  ..., -1.4130, -1.4043, -1.4037],\n",
       "         [-1.8827, -1.8860, -1.8928,  ..., -1.4185, -1.4021, -1.3952],\n",
       "         [-1.8800, -1.8850, -1.8961,  ..., -1.4231, -1.4034, -1.3932]]], ),\n",
       " 'stem': '0001x2'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d07e0d-c0d1-4aff-8664-fbddbe5489fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_x2 = DataLoader(train_ds_x2, batch_size=16, shuffle=True,\n",
    "                          num_workers=0, pin_memory=True, drop_last=True, persistent_workers=False)\n",
    "valid_loader_x2 = DataLoader(valid_ds_x2, batch_size=4, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "test_loader_x2  = DataLoader(test_ds_x2,  batch_size=4, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "\n",
    "train_loader_x4 = DataLoader(train_ds_x4, batch_size=16, shuffle=True,\n",
    "                          num_workers=4, pin_memory=True, drop_last=True)\n",
    "valid_loader_x4 = DataLoader(valid_ds_x4, batch_size=4, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)\n",
    "test_loader_x4  = DataLoader(test_ds_x4,  batch_size=4, shuffle=False,\n",
    "                          num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ff236a-e41e-471a-ab51-5088b027a889",
   "metadata": {},
   "source": [
    "### Архитектура"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19022fdc-bd58-42c7-9a34-94bd20643791",
   "metadata": {},
   "source": [
    "Архитектура была вынесена в отдельный файл (см. `unet2d.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25259f45-d641-40fc-b1c1-dd0d8c36b4da",
   "metadata": {},
   "source": [
    "#### Основные компоненты\n",
    "1. Блок `ConvBNAct` - содержит последовательность: `Conv2D` -> `BatchNorm` (опц.) -> `ReLU` -> `Dropout` (опц.). Базовый конвейер для извлечения признаков при стабильном градиенте (BN) и нелинейности (ReLU).\n",
    "2. Блок `DoubleConv` - повторение `ConvBNAct` два раза. Классическая связка U-Net: после конкатенации со скипом перерабатывает признаки и склеивает информацию из энкодера и декодера.\n",
    "3. `Down`: `MaxPool2D`(2) -> `DoubleConv`. Уменьшает H×W в 2 раза и удваивает число каналов. Это энкодерный шаг: больше контекста, грубее пространственное разрешение.\n",
    "4. `Up`: `Upsample` (два способа, см. `unet2d.py`) -> concat со skip из соответсутвующего энкодера -> `DoubleConv`. Обратное повышение разрешения и убавление количества каналов.\n",
    "5. `OutConv`: `Conv2D`(kernel_size=1) до нужного out_ch (в нашем случае 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9dbf9a-76dc-4efd-a824-7d21ea4c9dcb",
   "metadata": {},
   "source": [
    "#### Конфиг для UNetConfig\n",
    "1. `in_channels` - входные каналы (1 для greyscale);\n",
    "2. `out_channels` - число классов или каналов цели;\n",
    "3. `base_channels` - ширина сети на первом уровне (обычно 32/64);\n",
    "4. `depth` - число понижающих шагов (глубина энкодера);\n",
    "5. `bilinear` — тип апсемплинга (True: `Upsample` + `Conv2D`, False: `TransposedConv2D`);\n",
    "6. `norm` / `dropout` — включение BN и Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4f687-3cb7-4cb6-9a8a-05bfa2aebe5b",
   "metadata": {},
   "source": [
    "#### Пример пайплайна преобразований данных\n",
    "Пусть `base_channels=64` и `depth=4`:\n",
    "\n",
    "**Энкодер** (Down path):\n",
    "1. `inc`: in -> 64 (DoubleConv);\n",
    "2. `down1`: 64 -> 128 (Pool×2, DoubleConv);\n",
    "3. `down2`: 128 -> 256;\n",
    "4. `down3`: 256 -> 512;\n",
    "5. `down4`: 512 -> 1024 <- bottleneck;\n",
    "\n",
    "**Декодер** (Up path):\n",
    "\n",
    "Каждый up-шаг: upsample текущего тензора, concat со skip connection из энкодера того же уровня, прогоняем через DoubleConv, при этом число каналов уменьшается в 2 раза:\n",
    "1. `up1`: (1024 ↑) ⊕ (512) → 512;\n",
    "2. `up2`: (512 ↑) ⊕ (256) → 256;\n",
    "3. `up3`: (256 ↑) ⊕ (128) → 128;\n",
    "4. `up4`: (128 ↑) ⊕ (64) → 64;\n",
    "5. `outc`: 64 → out_channels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a3f761-7726-4fa9-a3f4-e482fe8754d0",
   "metadata": {},
   "source": [
    "#### Форматы данных после преобразований\n",
    "\n",
    "Для примера: вход `x` формы `[B, 1, 256, 256]`, `out_channels=1`.\n",
    "\n",
    "**Энкодер**:\n",
    "\n",
    "1. `x1 = inc(x)` -> `[B, 64, 256, 256]` (сохраняем в skips);\n",
    "2. `x2 = down1(x1)` -> `[B, 128, 128, 128]` (skips);\n",
    "3. `x3 = down2(x2)` -> `[B, 256, 64, 64]` (skips);\n",
    "4. `x4 = down3(x3)` -> `[B, 512, 32, 32]` (skips);\n",
    "5. `x5 = down4(x4)` -> `[B, 1024, 16, 16]` (дно).\n",
    "\n",
    "**Декодер**:\n",
    "1. `u1 = up1(x5, x4)` -> `[B, 512, 32, 32]`;\n",
    "2. `u2 = up2(u1, x3)` -> `[B, 256, 64, 64]`;\n",
    "3. `u3 = up3(u2, x2)` -> `[B, 128, 128, 128]`;\n",
    "4. `u4 = up4(u3, x1)` -> `[B, 64, 256, 256]`;\n",
    "5. `logits = outc(u4)` -> `[B, 1, 256, 256]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28c801-eb4c-4a49-8940-c29fc9f0ca53",
   "metadata": {},
   "source": [
    "#### Инициализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f09de38b-608e-4546-9f46-018b06db25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2859c8d2-15ee-4710-9aa9-f30c5e54eb39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28599361"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unet2d import UNet2D, UNetConfig\n",
    "\n",
    "cfg = UNetConfig(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    base_channels=64,\n",
    "    depth=4,\n",
    "    bilinear=True,\n",
    "    norm=True,\n",
    "    dropout=0.0,\n",
    ")\n",
    "model = UNet2D(cfg)\n",
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86d066e1-9552-4e06-85b8-e4d699a1f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "wandb: WARNING Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Вячеслав\\_netrc\n",
      "wandb: Currently logged in as: vyacheslav-timofeev (vyacheslav-timofeev-tomsk-polytechnic-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"e178f2b1ee4008c4903f56c9a600498f420802cc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a224b-e1ba-4ee4-8e8d-d087a5c049f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(\n",
    "    project=\"unet2d\",\n",
    "    name=\"baseline\",\n",
    "    config={\n",
    "        \"epochs\": 150,\n",
    "        \"batch_size\": 16,\n",
    "        \"lr\": 2e-4,\n",
    "        \"weight_decay\": 1e-4,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"stages\": 4,\n",
    "        \"activation\": \"ReLU\",\n",
    "        \"scheduler\": \"OneCycleLR\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66d9fc7b-109a-4635-b5da-453cfac633d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "484964c5-bcbf-4a1b-9261-1718024493c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "steps_per_epoch = len(train_loader_x2)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=2e-4, epochs=EPOCHS, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4945e1ca-d3f3-4e6c-893d-28749e8b7626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed33051b-ab40-43ec-8ac0-87a4b58cdb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def time_loader(dl, n_batches=5):\n",
    "    t0 = time.time()\n",
    "    for i, batch in enumerate(dl):\n",
    "        if i==0:\n",
    "            print(f\"⏱ first batch: {time.time()-t0:.2f}s\")\n",
    "        if i+1 >= n_batches:\n",
    "            break\n",
    "    print(f\"⏱ {n_batches} batches: {time.time()-t0:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b860361-de26-4f1e-b9c3-c8e37bef942d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱ first batch: 0.20s\n",
      "⏱ 5 batches: 0.61s\n"
     ]
    }
   ],
   "source": [
    "time_loader(train_loader_x2, n_batches=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efdddf63-54c0-4472-9729-8c2247f4964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssim_val(pred, target):\n",
    "    # ожидаем [0,1]; возвращает скаляр в [0,1]\n",
    "    return ssim(pred, target, data_range=1.0, size_average=True)\n",
    "\n",
    "def psnr_val(pred, target, eps=1e-8):\n",
    "    # ожидаем [0,1]\n",
    "    mse = F.mse_loss(pred, target, reduction=\"mean\")\n",
    "    return 10.0 * torch.log10(1.0 / (mse + eps))\n",
    "\n",
    "# ---- Комбинированный лосс: L1 + λ(1-SSIM)\n",
    "def sr_loss(pred, target, lambda_ssim=0.1):\n",
    "    pred_01   = pred.clamp(0, 1)\n",
    "    target_01 = target.clamp(0, 1)\n",
    "    l1 = F.l1_loss(pred_01, target_01)\n",
    "    \n",
    "    with torch.cuda.amp.autocast(enabled=False):  # SSIM более стабилен в float32\n",
    "        ssim_score = ssim_val(pred_01.float(), target_01.float())\n",
    "        \n",
    "    return l1 + lambda_ssim * (1.0 - ssim_score), {'l1': l1.detach(), 'ssim': ssim_score.detach()}\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler, device, epoch, amp=True, lambda_ssim=0.1, step_scheduler_in_batch=True):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=(amp and (device == \"cuda\" or \"cuda\" in str(device))))\n",
    "    total_loss = total_psnr = total_ssim = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for step, (lr_img, hr_img) in enumerate(loader, 1):\n",
    "        lr_img = lr_img.to(device, non_blocking=True)\n",
    "        hr_img = hr_img.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(device_type=\"cuda\", enabled=(amp and (device == \"cuda\" or \"cuda\" in str(device)))):\n",
    "            pred = model(lr_img)\n",
    "            loss, parts = sr_loss(pred, hr_img, lambda_ssim=lambda_ssim)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        if step_scheduler_in_batch and scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # метрики (в float32, [0,1])\n",
    "        with torch.no_grad():\n",
    "            pred_01   = pred.detach().clamp(0, 1)\n",
    "            hr_01     = hr_img.detach().clamp(0, 1)\n",
    "            psnr = psnr_val(pred_01, hr_01)\n",
    "            ssim_score = parts['ssim']\n",
    "\n",
    "        bsz = lr_img.size(0)\n",
    "        total_samples += bsz\n",
    "        total_loss += loss.item() * bsz\n",
    "        total_psnr += psnr.item() * bsz\n",
    "        total_ssim += ssim_score.item() * bsz\n",
    "\n",
    "    epoch_loss = total_loss / total_samples\n",
    "    epoch_psnr = total_psnr / total_samples\n",
    "    epoch_ssim = total_ssim / total_samples\n",
    "    return {'loss': epoch_loss, 'psnr': epoch_psnr, 'ssim': epoch_ssim}\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device, amp=False, lambda_ssim=0.1):\n",
    "    model.eval()\n",
    "    total_loss = total_psnr = total_ssim = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    for lr_img, hr_img in loader:\n",
    "        lr_img = lr_img.to(device, non_blocking=True)\n",
    "        hr_img = hr_img.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(device_type=\"cuda\", enabled=(amp and (device == \"cuda\" or \"cuda\" in str(device)))):\n",
    "            pred = model(lr_img)\n",
    "            loss, parts = sr_loss(pred, hr_img, lambda_ssim=lambda_ssim)\n",
    "\n",
    "        pred_01   = pred.clamp(0, 1)\n",
    "        hr_01     = hr_img.clamp(0, 1)\n",
    "        psnr = psnr_val(pred_01, hr_01)\n",
    "        ssim_score = parts['ssim']\n",
    "\n",
    "        bsz = lr_img.size(0)\n",
    "        total_samples += bsz\n",
    "        total_loss += loss.item() * bsz\n",
    "        total_psnr += psnr.item() * bsz\n",
    "        total_ssim += ssim_score.item() * bsz\n",
    "\n",
    "    val_loss = total_loss / total_samples\n",
    "    val_psnr = total_psnr / total_samples\n",
    "    val_ssim = total_ssim / total_samples\n",
    "    return {'loss': val_loss, 'psnr': val_psnr, 'ssim': val_ssim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "416b89a0-16a6-416f-8fa1-bfb49e1dbde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Ранняя остановка по метрике.\n",
    "    mode='max' — метрика должна расти (PSNR/SSIM), 'min' — падать (loss).\n",
    "    Если улучшение <= min_delta — не считаем за улучшение.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=20, min_delta=0.0, mode='max', restore_best=True):\n",
    "        assert mode in ('max', 'min')\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.restore_best = restore_best\n",
    "\n",
    "        self.best_value = -math.inf if mode == 'max' else math.inf\n",
    "        self.best_state = None\n",
    "        self.best_epoch = 0\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def _is_better(self, value):\n",
    "        if self.mode == 'max':\n",
    "            return value > (self.best_value + self.min_delta)\n",
    "        else:\n",
    "            return value < (self.best_value - self.min_delta)\n",
    "\n",
    "    def step(self, value, model, epoch):\n",
    "        \"\"\"\n",
    "        Возвращает True, если нужно ОСТАНОВИТЬСЯ.\n",
    "        \"\"\"\n",
    "        if self._is_better(value):\n",
    "            self.best_value = value\n",
    "            self.best_epoch = epoch\n",
    "            self.num_bad_epochs = 0\n",
    "            if self.restore_best:\n",
    "                # копируем лучшие веса в память (чтобы не читать файл)\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "            return self.num_bad_epochs > self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fed696-cf34-49c4-bac7-4a2c74215c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Вячеслав\\AppData\\Local\\Temp\\ipykernel_14852\\2035881260.py:23: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(amp and (device == \"cuda\" or \"cuda\" in str(device))))\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "best_psnr = -math.inf\n",
    "best_path = \"best_sr_unet.pt\"\n",
    "\n",
    "STEP_SCHED_IN_BATCH = isinstance(scheduler, torch.optim.lr_scheduler.OneCycleLR)\n",
    "\n",
    "early = EarlyStopping(\n",
    "    patience=15,     # попробуйте 10–30\n",
    "    min_delta=0.05,  # 0.05 dB — игнорируем микро-флуктуации PSNR\n",
    "    mode='max',\n",
    "    restore_best=True\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_stats = train_one_epoch(\n",
    "        model, train_loader_x2, optimizer, scheduler if STEP_SCHED_IN_BATCH else None,\n",
    "        device, epoch=epoch, amp=True, lambda_ssim=0.1, step_scheduler_in_batch=STEP_SCHED_IN_BATCH\n",
    "    )\n",
    "    val_stats = evaluate(model, valid_loader_x2, device, amp=False, lambda_ssim=0.1)\n",
    "\n",
    "    if not STEP_SCHED_IN_BATCH and scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    # сохранение лучшего чекпойнта по PSNR (или поменяйте на SSIM)\n",
    "    if val_stats['psnr'] > best_psnr + 1e-9:\n",
    "        best_psnr = val_stats['psnr']\n",
    "        torch.save(\n",
    "            {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"best_psnr\": best_psnr,\n",
    "                \"best_ssim\": val_stats['ssim'],\n",
    "                \"epoch\": epoch,\n",
    "                \"cfg\": getattr(model, \"cfg\", None),\n",
    "            },\n",
    "            best_path,\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:03d}/{EPOCHS} | \"\n",
    "        f\"train: loss {train_stats['loss']:.4f}, PSNR {train_stats['psnr']:.2f}, SSIM {train_stats['ssim']:.4f} | \"\n",
    "        f\"valid: loss {val_stats['loss']:.4f}, PSNR {val_stats['psnr']:.2f}, SSIM {val_stats['ssim']:.4f} | \"\n",
    "        f\"best PSNR: {best_psnr:.2f}\"\n",
    "    )\n",
    "\n",
    "    # шаг ранней остановки по PSNR\n",
    "    should_stop = early.step(val_stats['psnr'], model, epoch)\n",
    "    if should_stop:\n",
    "        print(f\"Early stopping: no improvement in {early.patience} epochs \"\n",
    "              f\"(best PSNR {early.best_value:.2f} at epoch {early.best_epoch}).\")\n",
    "        break\n",
    "\n",
    "# восстановим лучшие веса из памяти (если не грузите из файла)\n",
    "early.restore(model)\n",
    "print(f\"Best PSNR: {early.best_value:.2f} dB @ epoch {early.best_epoch} (also saved to {best_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cd5427-ace9-466f-b427-54e7f51ddba6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
